
================================================================================
File: api_config.py
Size: 2.71 kB
================================================================================

# Core API Endpoints Configuration
# This file defines which endpoints should be visible in the API documentation
# for the essential teacher workflow

CORE_TEACHER_WORKFLOW_ENDPOINTS = {
    # Authentication (Essential)
    "/v1/auth/signup": ["POST"],
    "/v1/auth/me": ["GET"],
    "/v1/auth/me/profile": ["PUT"],
    
    # Student Management (Essential)
    "/v1/students/upload-csv": ["POST"],
    "/v1/students/": ["GET"],
    "/v1/students/{student_id}": ["GET"],
    
    # Assessment Analysis (Essential) 
    "/v1/learning/analyze-assessment": ["POST"],
    "/v1/learning/student/{student_id}/progress": ["GET"],
    
    # Learning Path Management (Essential)
    "/v1/learning/start-monitoring": ["POST"],
    "/v1/learning/generate-learning-path": ["POST"],
    "/v1/learning/student/{student_id}/learning-paths": ["GET"],
    "/v1/learning/learning-path/{path_id}": ["GET"],
    "/v1/learning/adapt-learning-path/{path_id}": ["POST"],
    
    # Lesson Generation (Essential)
    "/v1/lessons/lessons/create-from-step": ["POST"],
    "/v1/lessons/lessons/{lesson_id}": ["GET"],
    "/v1/lessons/lessons/{lesson_id}/progress": ["POST"],
    
    # Chatbot Support (Essential)
    "/v1/lessons/lessons/{lesson_id}/chat/start": ["POST"],
    "/v1/lessons/lessons/chat/{session_id}/message": ["POST"],
    
    # Analytics & Insights (Essential)
    "/v1/learning/teacher/learning-analytics": ["GET"],
    "/v1/learning/student/{student_id}/learning-insights": ["GET"],
    
    # Health Check (Utility)
    "/": ["GET"],
    "/health": ["GET"]
}

# Optional endpoints for advanced users
ADVANCED_ENDPOINTS = {
    "/v1/learning/monitoring-status": ["GET"],
    "/v1/learning/learning-path/{path_id}/update-progress": ["POST"],
    "/v1/lessons/lessons/student/{student_id}": ["GET"],
    "/v1/lessons/lessons/{lesson_id}/analytics": ["GET"],
    "/v1/lessons/lessons/{lesson_id}/regenerate-slide": ["POST"],
    "/v1/students/stats/summary": ["GET"],
    "/v1/students/{student_id}": ["DELETE", "PUT"]
}

# Document generation and assessment creation (Optional)
CONTENT_CREATION_ENDPOINTS = {
    "/v1/documents/upload": ["POST"],
    "/v1/documents/list": ["GET"],
    "/v1/assessments/configs": ["POST", "GET"],
    "/v1/assessments/configs/{config_id}/generate": ["POST"]
}

# All agent/debug endpoints (Hidden by default)
HIDDEN_ENDPOINTS = {
    # Agent framework endpoints
    "/list-apps",
    "/debug/*",
    "/apps/*",
    "/builder/*",
    "/run",
    "/run_sse",
    "/dev-ui",
    
    # Agent specific
    "/v1/agent/*",
    
    # RAG assessments (advanced feature)
    "/v1/assessments/rag/*",
    
    # Basic assessments (if using enhanced learning paths)
    "/v1/assessments/*"
}


================================================================================
File: app/__init__.py
Size: 0 B
================================================================================



================================================================================
File: app/agents/__init__.py
Size: 0 B
================================================================================



================================================================================
File: app/agents/assessment_generation/__init__.py
Size: 144 B
================================================================================

# FILE: app/agents/assessment_generation/__init__.py

from .agent import assessment_generation_agent

__all__ = ["assessment_generation_agent"]


================================================================================
File: app/agents/assessment_generation/agent.py
Size: 12.03 kB
================================================================================

# FILE: app/agents/assessment_generation/agent.py

import logging
import json
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.vertex import get_vertex_model
from app.services.vertex_rag_service import vertex_rag_service
from app.models.student import AssessmentConfig, Assessment, AssessmentQuestion

logger = logging.getLogger(__name__)

class AssessmentGenerationAgent:
    """Agent for generating MCQ assessments using RAG documents."""
    
    def __init__(self):
        self.model = get_vertex_model()
        
    async def generate_assessment(
        self,
        config: AssessmentConfig,
        teacher_uid: str
    ) -> Assessment:
        """
        Generate a complete MCQ assessment based on configuration.
        
        Args:
            config: Assessment configuration
            teacher_uid: UID of the teacher requesting the assessment
            
        Returns:
            Assessment object with generated questions
        """
        try:
            logger.info(f"Generating assessment for config {config.config_id}")
            
            # Step 1: Retrieve relevant content from RAG
            relevant_content = await self._retrieve_relevant_content(
                teacher_uid=teacher_uid,
                subject=config.subject,
                grade_level=config.target_grade,
                topic=config.topic
            )
            
            if not relevant_content:
                logger.warning(f"No relevant content found for topic: {config.topic}")
                # Generate questions without RAG content (using general knowledge)
                questions = await self._generate_questions_without_rag(config)
            else:
                # Generate questions using RAG content
                questions = await self._generate_questions_with_rag(config, relevant_content)
            
            # Step 2: Create assessment object
            assessment_id = str(uuid.uuid4())
            
            assessment = Assessment(
                assessment_id=assessment_id,
                config_id=config.config_id,
                teacher_uid=teacher_uid,
                title=f"{config.subject} - {config.topic} (Grade {config.target_grade})",
                subject=config.subject,
                grade=config.target_grade,
                difficulty=config.difficulty_level,
                topic=config.topic,
                questions=questions,
                time_limit_minutes=config.time_limit_minutes
            )
            
            logger.info(f"Generated assessment {assessment_id} with {len(questions)} questions")
            return assessment
            
        except Exception as e:
            logger.error(f"Failed to generate assessment: {e}")
            raise
    
    async def _retrieve_relevant_content(
        self,
        teacher_uid: str,
        subject: str,
        grade_level: int,
        topic: str
    ) -> str:
        """Retrieve relevant content from RAG system."""
        try:
            # Create search query for RAG
            search_query = f"""
            Find educational content about {topic} for {subject} at grade {grade_level} level.
            Look for definitions, explanations, examples, and key concepts related to {topic}.
            """
            
            # Search in RAG system with filters
            search_filters = {
                "teacher_uid": teacher_uid,
                "subject": subject,
                "grade_level": grade_level
            }
            
            # Use vertex RAG service to search
            search_results = await vertex_rag_service.search_documents(
                query=search_query,
                filters=search_filters,
                max_results=5
            )
            
            # Combine search results into content
            relevant_content = ""
            for result in search_results:
                relevant_content += f"\n{result.get('content', '')}\n"
            
            return relevant_content.strip()
            
        except Exception as e:
            logger.error(f"Failed to retrieve RAG content: {e}")
            return ""
    
    async def _generate_questions_with_rag(
        self,
        config: AssessmentConfig,
        content: str
    ) -> List[AssessmentQuestion]:
        """Generate questions using RAG content."""
        
        difficulty_descriptions = {
            "easy": "basic understanding and recall",
            "medium": "application and analysis", 
            "hard": "synthesis and evaluation"
        }
        
        prompt = f"""You are an expert educational assessment creator. Generate {config.question_count} multiple choice questions based on the provided educational content.

ASSESSMENT REQUIREMENTS:
- Subject: {config.subject}
- Topic: {config.topic}  
- Grade Level: {config.target_grade}
- Difficulty: {config.difficulty_level} ({difficulty_descriptions.get(config.difficulty_level, "standard")})
- Question Count: {config.question_count}

EDUCATIONAL CONTENT TO BASE QUESTIONS ON:
{content}

INSTRUCTIONS:
1. Create exactly {config.question_count} multiple choice questions
2. Each question should have exactly 4 options (A, B, C, D)
3. Questions should be appropriate for grade {config.target_grade} students
4. Difficulty should be {config.difficulty_level} level
5. Base questions on the provided educational content
6. Include clear explanations for correct answers

OUTPUT FORMAT - Return ONLY a JSON array like this:
[
  {{
    "question_text": "What is the main concept of...?",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "correct_answer": 0,
    "explanation": "The correct answer is A because...",
    "difficulty": "{config.difficulty_level}",
    "topic": "{config.topic}"
  }}
]

Generate exactly {config.question_count} questions following this format."""

        try:
            response = await self.model.generate_content_async(prompt)
            
            # Parse JSON response
            questions_data = self._parse_questions_response(response.text)
            
            # Convert to AssessmentQuestion objects
            questions = []
            for i, q_data in enumerate(questions_data):
                question = AssessmentQuestion(
                    question_id=str(uuid.uuid4()),
                    question_text=q_data.get("question_text", f"Question {i+1}"),
                    options=q_data.get("options", ["A", "B", "C", "D"]),
                    correct_answer=q_data.get("correct_answer", 0),
                    explanation=q_data.get("explanation", "No explanation provided"),
                    difficulty=q_data.get("difficulty", config.difficulty_level),
                    topic=q_data.get("topic", config.topic)
                )
                questions.append(question)
            
            return questions
            
        except Exception as e:
            logger.error(f"Failed to generate questions with RAG: {e}")
            # Fallback to generating without RAG
            return await self._generate_questions_without_rag(config)
    
    async def _generate_questions_without_rag(
        self,
        config: AssessmentConfig
    ) -> List[AssessmentQuestion]:
        """Generate questions without RAG content (fallback)."""
        
        prompt = f"""You are an expert educational assessment creator. Generate {config.question_count} multiple choice questions for the given specifications.

ASSESSMENT REQUIREMENTS:
- Subject: {config.subject}
- Topic: {config.topic}
- Grade Level: {config.target_grade}
- Difficulty: {config.difficulty_level}
- Question Count: {config.question_count}

INSTRUCTIONS:
1. Create grade-appropriate questions for {config.subject} on the topic of {config.topic}
2. Each question should have exactly 4 options
3. Questions should be suitable for grade {config.target_grade} students
4. Difficulty should be {config.difficulty_level} level
5. Cover different aspects of the topic

OUTPUT FORMAT - Return ONLY a JSON array:
[
  {{
    "question_text": "Question text here?",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "correct_answer": 0,
    "explanation": "Explanation for correct answer",
    "difficulty": "{config.difficulty_level}",
    "topic": "{config.topic}"
  }}
]

Generate exactly {config.question_count} questions."""

        try:
            response = await self.model.generate_content_async(prompt)
            
            # Parse JSON response
            questions_data = self._parse_questions_response(response.text)
            
            # Convert to AssessmentQuestion objects
            questions = []
            for i, q_data in enumerate(questions_data):
                question = AssessmentQuestion(
                    question_id=str(uuid.uuid4()),
                    question_text=q_data.get("question_text", f"Sample question {i+1} about {config.topic}"),
                    options=q_data.get("options", ["Option A", "Option B", "Option C", "Option D"]),
                    correct_answer=q_data.get("correct_answer", 0),
                    explanation=q_data.get("explanation", "Explanation not available"),
                    difficulty=q_data.get("difficulty", config.difficulty_level),
                    topic=q_data.get("topic", config.topic)
                )
                questions.append(question)
            
            return questions
            
        except Exception as e:
            logger.error(f"Failed to generate fallback questions: {e}")
            # Create basic sample questions as last resort
            return self._create_sample_questions(config)
    
    def _parse_questions_response(self, response_text: str) -> List[Dict[str, Any]]:
        """Parse AI response and extract questions JSON."""
        try:
            # Clean up response text
            cleaned_text = response_text.strip()
            
            # Remove markdown formatting if present
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]
            
            # Find JSON array in response
            start_idx = cleaned_text.find('[')
            end_idx = cleaned_text.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_text = cleaned_text[start_idx:end_idx]
                questions_data = json.loads(json_text)
                
                if isinstance(questions_data, list):
                    return questions_data
            
            # If parsing fails, try to parse the entire cleaned text
            return json.loads(cleaned_text)
            
        except Exception as e:
            logger.error(f"Failed to parse questions response: {e}")
            logger.debug(f"Response text: {response_text}")
            return []
    
    def _create_sample_questions(self, config: AssessmentConfig) -> List[AssessmentQuestion]:
        """Create basic sample questions as fallback."""
        questions = []
        
        for i in range(min(config.question_count, 5)):  # Max 5 sample questions
            question = AssessmentQuestion(
                question_id=str(uuid.uuid4()),
                question_text=f"Sample {config.subject} question {i+1} about {config.topic}?",
                options=[f"Option A for question {i+1}", f"Option B for question {i+1}", 
                        f"Option C for question {i+1}", f"Option D for question {i+1}"],
                correct_answer=0,  # Always A for samples
                explanation=f"This is a sample question for {config.topic}",
                difficulty=config.difficulty_level,
                topic=config.topic
            )
            questions.append(question)
        
        return questions

# Global instance
assessment_generation_agent = AssessmentGenerationAgent()


================================================================================
File: app/agents/document_analysis/__init__.py
Size: 290 B
================================================================================

# FILE: app/agents/document_analysis/__init__.py

"""Document Analysis Agent Package

This agent analyzes uploaded documents to extract:
- Content type (worksheet, lesson plan, assessment, etc.)
- Learning objectives
- Topics and concepts
- Difficulty level assessment
- Key vocabulary
"""


================================================================================
File: app/agents/document_analysis/agent.py
Size: 6.81 kB
================================================================================

# FILE: app/agents/document_analysis/agent.py

from __future__ import annotations
import logging
from typing import Dict, Any, Optional, List
import PyPDF2
import io
from PIL import Image
import pytesseract

from app.core.vertex import get_vertex_model

logger = logging.getLogger(__name__)

class DocumentAnalysisAgent:
    """Agent for analyzing document content and extracting metadata."""
    
    def __init__(self):
        self.model = get_vertex_model()
        
    async def analyze_document(
        self, 
        file_content: bytes, 
        filename: str, 
        file_type: str,
        subject: str,
        grade_level: int
    ) -> Dict[str, Any]:
        """
        Analyze document content and extract metadata.
        
        Args:
            file_content: The document file content as bytes
            filename: Original filename
            file_type: MIME type of the file
            subject: Subject category
            grade_level: Grade level provided by teacher
            
        Returns:
            Dictionary containing analysis results
        """
        try:
            # Extract text content based on file type
            text_content = await self._extract_text_content(file_content, file_type)
            
            if not text_content or len(text_content.strip()) < 50:
                return {
                    "content_type": "unknown",
                    "topics": [],
                    "learning_objectives": [],
                    "difficulty_level": "unknown",
                    "key_vocabulary": [],
                    "text_preview": text_content[:200] if text_content else "",
                    "analysis_status": "insufficient_content"
                }
            
            # Use AI to analyze the content
            analysis = await self._ai_analyze_content(
                text_content, filename, subject, grade_level
            )
            
            return {
                **analysis,
                "text_preview": text_content[:500],
                "analysis_status": "completed"
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze document {filename}: {e}")
            return {
                "content_type": "unknown",
                "topics": [],
                "learning_objectives": [],
                "difficulty_level": "unknown", 
                "key_vocabulary": [],
                "text_preview": "",
                "analysis_status": "failed",
                "error": str(e)
            }
    
    async def _extract_text_content(self, file_content: bytes, file_type: str) -> str:
        """Extract text content from different file types."""
        try:
            if file_type == "application/pdf":
                return await self._extract_pdf_text(file_content)
            elif file_type == "text/plain":
                return file_content.decode('utf-8')
            elif file_type.startswith("image/"):
                return await self._extract_image_text(file_content)
            else:
                logger.warning(f"Unsupported file type for text extraction: {file_type}")
                return ""
        except Exception as e:
            logger.error(f"Failed to extract text from {file_type}: {e}")
            return ""
    
    async def _extract_pdf_text(self, file_content: bytes) -> str:
        """Extract text from PDF files."""
        try:
            pdf_file = io.BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text_content = ""
            for page in pdf_reader.pages:
                text_content += page.extract_text() + "\n"
            
            return text_content.strip()
        except Exception as e:
            logger.error(f"Failed to extract PDF text: {e}")
            return ""
    
    async def _extract_image_text(self, file_content: bytes) -> str:
        """Extract text from images using OCR."""
        try:
            image = Image.open(io.BytesIO(file_content))
            text = pytesseract.image_to_string(image)
            return text.strip()
        except Exception as e:
            logger.error(f"Failed to extract image text: {e}")
            return ""
    
    async def _ai_analyze_content(
        self, 
        text_content: str, 
        filename: str, 
        subject: str, 
        grade_level: int
    ) -> Dict[str, Any]:
        """Use AI to analyze document content."""
        
        prompt = f"""Analyze this educational document and provide structured information:

DOCUMENT INFO:
- Filename: {filename}
- Subject: {subject}
- Grade Level: {grade_level}

DOCUMENT CONTENT:
{text_content}  

Please analyze and return ONLY a JSON object with these exact fields:
{{
    "content_type": "one of: worksheet, lesson_plan, assessment, quiz, homework, reading_material, activity, project, handout, other",
    "topics": ["list of 3-5 main topics covered"],
    "learning_objectives": ["list of 2-4 learning objectives the document addresses"],
    "difficulty_level": "one of: basic, intermediate, advanced",
    "key_vocabulary": ["list of 5-10 important terms or concepts"],
    "educational_value": "brief description of what students will learn"
}}

Focus on extracting concrete, actionable information that will help with personalized learning recommendations."""

        try:
            response = await self.model.generate_content_async(prompt)
            
            # Parse the JSON response
            import json
            analysis_text = response.text.strip()
            
            # Clean up the response if it has markdown formatting
            if analysis_text.startswith("```json"):
                analysis_text = analysis_text[7:]
            if analysis_text.endswith("```"):
                analysis_text = analysis_text[:-3]
            
            analysis = json.loads(analysis_text.strip())
            
            # Validate required fields
            required_fields = ["content_type", "topics", "learning_objectives", "difficulty_level", "key_vocabulary"]
            for field in required_fields:
                if field not in analysis:
                    analysis[field] = [] if field in ["topics", "learning_objectives", "key_vocabulary"] else "unknown"
            
            return analysis
            
        except Exception as e:
            logger.error(f"Failed to AI analyze content: {e}")
            return {
                "content_type": "unknown",
                "topics": [],
                "learning_objectives": [],
                "difficulty_level": "unknown",
                "key_vocabulary": [],
                "educational_value": "Analysis failed"
            }

# Global instance
document_analysis_agent = DocumentAnalysisAgent()


================================================================================
File: app/agents/learning_path_agent/__init__.py
Size: 108 B
================================================================================

# FILE: app/agents/learning_path_agent/__init__.py

from .agent import root_agent

__all__ = ["root_agent"]


================================================================================
File: app/agents/learning_path_agent/agent.py
Size: 5.96 kB
================================================================================

# FILE: app/agents/learning_path_agent/agent.py

from __future__ import annotations
import logging
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from google.adk.agents import Agent
from app.core.config import settings
from app.core.firebase import db
from app.services.assessment_analysis_service import assessment_analysis_service
from app.services.learning_path_service import learning_path_service
from app.services.enhanced_assessment_service import enhanced_assessment_service
from app.models.learning_models import StudentPerformance, KnowledgeGap
from app.agents.tools.learning_path_tools import (
    monitor_student_assessments,
    analyze_assessment_completion,
    generate_learning_path_automatically,
    track_learning_progress,
    adapt_learning_path_on_new_data,
    get_student_learning_status
)

logger = logging.getLogger(__name__)

root_agent = Agent(
    model=settings.gemini_model_name,
    name="learning_path_agent",
    description="An intelligent agent that monitors student learning journeys and automatically generates personalized learning paths based on assessment performance.",
    instruction="""
    You are an intelligent Learning Path Agent that automatically monitors and manages the entire student learning journey. Your primary responsibility is to observe student assessment activities and proactively generate personalized learning paths without manual intervention.

    **Core Responsibilities:**

    1. **Assessment Monitoring:**
       - Continuously monitor when students complete assessments
       - Detect assessment submission events in real-time
       - Track assessment performance patterns across multiple attempts
       - Identify learning trends and progress indicators

    2. **Automatic Analysis Triggering:**
       - Immediately analyze assessment results when students complete tests
       - Identify knowledge gaps and learning needs automatically
       - Calculate performance metrics and trend analysis
       - Determine if learning intervention is needed

    3. **Intelligent Learning Path Generation:**
       - Automatically generate personalized learning paths based on assessment analysis
       - Tailor learning steps to individual student needs and performance patterns
       - Create adaptive sequences that build on strengths and address weaknesses
       - Set appropriate difficulty levels and learning objectives

    4. **Continuous Learning Adaptation:**
       - Monitor student progress through learning paths
       - Detect when students struggle or excel with learning steps
       - Automatically adapt and modify learning paths based on performance
       - Generate additional assessments when needed to validate progress

    5. **Proactive Intervention:**
       - Identify students who need immediate learning support
       - Automatically create intervention plans for struggling students
       - Generate enrichment paths for advanced students
       - Alert teachers when student needs exceed automated support

    **Operational Workflow:**

    **Phase 1: Assessment Monitoring**
    - Monitor assessment completion events across all students
    - Track time spent, completion rates, and performance patterns
    - Identify students requiring immediate attention

    **Phase 2: Automatic Analysis**
    - Run comprehensive performance analysis for each completed assessment
    - Identify knowledge gaps using AI-enhanced analysis
    - Calculate learning needs and prioritize intervention areas

    **Phase 3: Learning Path Generation**
    - Automatically generate personalized learning paths without waiting for teacher input
    - Create step-by-step learning sequences tailored to identified gaps
    - Set realistic timelines and learning objectives

    **Phase 4: Progress Monitoring**
    - Continuously track student progress through learning paths
    - Monitor engagement levels and completion rates
    - Detect when adaptation is needed

    **Phase 5: Adaptive Optimization**
    - Modify learning paths based on ongoing performance
    - Generate new assessments to validate learning progress
    - Create feedback loops for continuous improvement

    **Decision Making Criteria:**

    **Immediate Learning Path Generation (Score < 70%):**
    - Generate comprehensive learning path addressing all identified gaps
    - Focus on foundational skills and step-by-step progression
    - Include multiple practice opportunities and assessments

    **Targeted Intervention (Score 70-85%):**
    - Create focused learning paths for specific weak areas
    - Maintain current strengths while addressing gaps
    - Include challenging practice to solidify understanding

    **Enrichment Path (Score > 85%):**
    - Generate advanced learning paths with challenging content
    - Focus on application, analysis, and creative problem-solving
    - Prepare for next-level learning objectives

    **Communication Style:**
    - Be proactive and informative when reporting actions taken
    - Provide clear rationale for learning path decisions
    - Explain how generated paths address specific student needs
    - Offer insights about student learning patterns and progress

    **Automation Principles:**
    - Act immediately when assessment data becomes available
    - Generate learning paths without waiting for manual triggers
    - Continuously optimize based on student performance data
    - Maintain detailed logs of all automated actions taken

    You are designed to work autonomously, making intelligent decisions about student learning needs and automatically generating appropriate educational interventions.
    """,
    tools=[
        monitor_student_assessments,
        analyze_assessment_completion,
        generate_learning_path_automatically,
        track_learning_progress,
        adapt_learning_path_on_new_data,
        get_student_learning_status
    ]
)


================================================================================
File: app/agents/lesson_agent/agent.py
Size: 5.96 kB
================================================================================

# FILE: app/agents/lesson_agent/agent.py

from __future__ import annotations
import logging
import json
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime

from google.adk.agents import Agent
from app.core.config import settings
from app.core.firebase import db
from app.models.lesson_models import (
    LessonContent, LessonSlide, ContentElement, LessonChatSession, 
    ChatMessage, SlideType, ContentElementType, InteractiveWidget
)
from app.models.learning_models import LearningStep
from app.agents.tools.lesson_tools import (
    generate_lesson_content,
    get_lesson_content,
    update_lesson_progress,
    start_lesson_chat,
    send_chat_message,
    get_chat_history,
    generate_slide_content,
    create_interactive_element,
    adapt_lesson_difficulty
)

logger = logging.getLogger(__name__)

lesson_agent = Agent(
    model=settings.gemini_model_name,
    name="lesson_agent",
    description="An intelligent agent that generates dynamic lesson content and provides interactive chatbot support for personalized learning experiences.",
    instruction="""
    You are an intelligent Lesson Agent responsible for creating engaging, interactive lesson content and providing real-time educational support through a chatbot interface. Your role is to make learning dynamic, personalized, and effective.

    **Core Responsibilities:**

    1. **Dynamic Lesson Content Generation:**
       - Create engaging slide-based lessons from learning steps
       - Generate diverse content types (explanations, examples, exercises, interactive widgets)
       - Adapt content difficulty based on student performance and needs
       - Include multimedia elements and interactive components
       - Ensure lessons are pedagogically sound and age-appropriate

    2. **Interactive Slide Creation:**
       - Design slides with clear learning objectives
       - Include interactive elements (quizzes, drag-drop, fill-in-blanks)
       - Create practice exercises and real-world examples
       - Add visual aids and diagrams where helpful
       - Ensure progressive difficulty and concept building

    3. **Intelligent Chatbot Support:**
       - Answer student questions about lesson content in real-time
       - Provide explanations, clarifications, and additional examples
       - Offer hints and guidance for interactive exercises
       - Adapt responses to student's grade level and understanding
       - Encourage learning and maintain engagement

    4. **Personalized Learning Experience:**
       - Tailor content to individual student's learning style and pace
       - Adjust explanations based on student's previous performance
       - Provide additional support for struggling concepts
       - Offer enrichment content for advanced learners
       - Track engagement and adapt accordingly

    5. **Progress Monitoring and Adaptation:**
       - Monitor student progress through lesson slides
       - Identify areas where students struggle
       - Automatically adjust content difficulty or provide additional support
       - Generate follow-up activities based on performance
       - Provide teachers with insights into student learning

    **Content Generation Guidelines:**

    **Slide Structure:**
    - Start with clear learning objectives
    - Build concepts progressively from simple to complex
    - Include multiple examples and non-examples
    - Add interactive elements to maintain engagement
    - End with practice and assessment opportunities

    **Interactive Elements:**
    - Multiple choice questions with immediate feedback
    - Drag-and-drop activities for concept matching
    - Fill-in-the-blank exercises for key terms
    - Drawing/annotation tools for visual learners
    - Step-by-step guided practice problems

    **Chatbot Behavior:**
    - Be encouraging and supportive in all interactions
    - Ask clarifying questions to understand student needs
    - Provide step-by-step explanations when needed
    - Use student's name and refer to their progress
    - Offer multiple ways to understand difficult concepts
    - Never give direct answers to assessment questions, but provide guidance

    **Personalization Factors:**
    - Student's grade level and subject proficiency
    - Learning style preferences (visual, auditory, kinesthetic)
    - Previous performance on similar topics
    - Time spent on different types of content
    - Questions asked and areas of confusion
    - Teacher feedback and preferences

    **Quality Standards:**
    - All content must be educationally accurate and current
    - Language appropriate for the target grade level
    - Clear, concise explanations with relevant examples
    - Culturally sensitive and inclusive content
    - Aligned with curriculum standards and learning objectives

    **Response Format for Lesson Generation:**
    When generating lessons, provide:
    - Clear slide structure with learning progression
    - Diverse content types and interactive elements
    - Specific learning objectives for each slide
    - Estimated completion times
    - Success criteria and assessment methods

    **Response Format for Chatbot Interactions:**
    When responding to student questions:
    - Acknowledge the student's question
    - Provide clear, grade-appropriate explanations
    - Include examples or analogies when helpful
    - Ask follow-up questions to check understanding
    - Suggest next steps or related activities

    You are designed to make learning engaging, effective, and enjoyable while providing comprehensive support for both content creation and real-time student assistance.
    """,
    tools=[
        generate_lesson_content,
        get_lesson_content,
        update_lesson_progress,
        start_lesson_chat,
        send_chat_message,
        get_chat_history,
        generate_slide_content,
        create_interactive_element,
        adapt_lesson_difficulty
    ]
)


================================================================================
File: app/agents/orchestrator_agent/__init__.py
Size: 71 B
================================================================================

# FILE: app/agents/orchestrator_agent/__init__.py

from . import agent


================================================================================
File: app/agents/orchestrator_agent/agent.py
Size: 3.62 kB
================================================================================

# FILE: app/agents/orchestrator_agent/agent.py

from __future__ import annotations
from google.adk.agents import Agent
from app.agents.teacher_onboarding_agent.agent import root_agent as teacher_onboarding_agent
from app.agents.learning_path_agent.agent import root_agent as learning_path_agent
from app.agents.lesson_agent.agent import lesson_agent
from app.core.config import settings

root_agent = Agent(
    model=settings.gemini_model_name,
    name="orchestrator_agent",
    description="An intelligent orchestrator that routes requests to the appropriate specialized agents and spins up agents when needed.",
    instruction="""
      You are an intelligent orchestrator agent that routes user requests to the appropriate specialized agents.
      You can dynamically spin up and manage agents based on user needs.
      
      **Available Specialized Agents:**
      
      1. **Teacher Onboarding Agent** - Handles ALL teacher-related tasks:
         - New teacher profile creation and onboarding
         - Existing teacher subject management
         - Educational content assistance
         - Profile updates and maintenance
         - General teaching support
         - All teacher-related inquiries
      
      2. **Learning Path Agent** - Automated student learning management:
         - Monitors student assessment completions automatically
         - Analyzes performance and identifies knowledge gaps
         - Generates personalized learning paths without manual intervention
         - Adapts learning paths based on student progress
         - Provides continuous learning optimization
      
      3. **Lesson Agent** - Dynamic lesson content and interactive learning:
         - Generates engaging slide-based lessons from learning steps
         - Creates interactive content with exercises and quizzes
         - Provides real-time chatbot support for student questions
         - Adapts lesson difficulty based on student performance
         - Manages lesson progress and completion tracking
      
      **Routing Logic:**
      
      **Route to Teacher Onboarding Agent for:**
      - Any teacher-related request
      - Profile creation or management
      - Subject setup or updates
      - Educational content needs
      - Teaching assistance
      - Onboarding and setup tasks
      - General teacher support
      
      **Route to Learning Path Agent for:**
      - Assessment monitoring and analysis
      - Learning path generation requests
      - Student progress tracking
      - Adaptive learning interventions
      - Performance analytics
      - Automated educational workflows
      
      **Route to Lesson Agent for:**
      - Lesson content creation and generation
      - Interactive slide development
      - Student lesson support and chatbot interactions
      - Lesson progress management
      - Content adaptation and regeneration
      - Slide-by-slide learning experiences
      
      **Decision Process:**
      1. Analyze the user's request
      2. Determine if it's teacher-related, learning-path-related, or lesson-related
      3. Route to the appropriate specialized agent
      4. Let the specialized agent handle the complete interaction
      
      **Response Style:**
      - Be helpful and welcoming
      - Quickly identify user needs
      - Seamlessly connect users with the right specialist
      - Explain that you're connecting them with an expert
      
      Route requests to the most appropriate specialized agent based on the content.
    """,
    sub_agents=[
        teacher_onboarding_agent,
        lesson_agent,
    ],
)


================================================================================
File: app/agents/question_generator_agent.py
Size: 13.07 kB
================================================================================

# FILE: app/agents/question_generator_agent.py

import logging
from typing import List, Dict, Any, Optional
import json
import asyncio

try:
    from langchain.agents import AgentType, initialize_agent
    from langchain.tools import Tool
    from langchain.schema import HumanMessage, SystemMessage
    from langchain_google_vertexai import ChatVertexAI
    from langchain.prompts import PromptTemplate
    from langchain.output_parsers import PydanticOutputParser
    LANGCHAIN_AVAILABLE = True
except ImportError:
    # Fallback imports will be handled in initialization
    LANGCHAIN_AVAILABLE = False
    ChatVertexAI = None
    HumanMessage = None

from app.models.rag_models import (
    RAGResult, 
    QuestionGenerationRequest, 
    EnhancedAssessmentQuestion,
    GeneratedQuestionContext
)
from app.models.student import AssessmentQuestion
from app.core.config import settings

logger = logging.getLogger(__name__)

class QuestionGeneratorAgent:
    """AI Agent for generating assessment questions from RAG context."""
    
    def __init__(self):
        self.settings = settings
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """Initialize the LLM for question generation."""
        try:
            if not LANGCHAIN_AVAILABLE:
                logger.warning("LangChain not available, question generation will be limited")
                self.llm = None
                return
                
            # Initialize Vertex AI LLM
            self.llm = ChatVertexAI(
                model_name="gemini-2.5-pro",
                project=self.settings.google_cloud_project or self.settings.firebase_project_id,
                location=self.settings.google_cloud_location,
                temperature=0.3,  # Lower temperature for more consistent questions
                max_output_tokens=2048
            )
            logger.info("Initialized Vertex AI LLM for question generation")
            
        except Exception as e:
            logger.warning(f"Failed to initialize Vertex AI LLM: {str(e)}")
            # Could add fallback to other LLMs here
            self.llm = None
    
    async def generate_questions(
        self,
        request: QuestionGenerationRequest
    ) -> List[EnhancedAssessmentQuestion]:
        """Generate assessment questions from RAG context."""
        
        if not self.llm or not LANGCHAIN_AVAILABLE:
            logger.error("LLM not initialized or LangChain not available, cannot generate questions")
            return []
        
        try:
            # Prepare context from RAG results
            context_text = self._prepare_context(request.context_chunks)
            
            # Generate questions in batches if needed
            all_questions = []
            batch_size = min(3, request.question_count)  # Generate 3 at a time max
            
            for i in range(0, request.question_count, batch_size):
                remaining = min(batch_size, request.question_count - i)
                
                batch_questions = await self._generate_question_batch(
                    context_text=context_text,
                    subject=request.subject,
                    grade_level=request.grade_level,
                    topic=request.topic,
                    difficulty=request.difficulty_level,
                    count=remaining,
                    context_chunks=request.context_chunks
                )
                
                all_questions.extend(batch_questions)
                
                # Small delay between batches to avoid rate limits
                if i + batch_size < request.question_count:
                    await asyncio.sleep(1)
            
            logger.info(f"Generated {len(all_questions)} questions for {request.subject} grade {request.grade_level}")
            return all_questions[:request.question_count]  # Ensure exact count
            
        except Exception as e:
            logger.error(f"Question generation failed: {str(e)}")
            return []
    
    def _prepare_context(self, context_chunks: List[RAGResult]) -> str:
        """Prepare context text from RAG results."""
        
        if not context_chunks:
            return "No specific context available."
        
        context_parts = []
        for i, result in enumerate(context_chunks, 1):
            chunk_text = result.chunk.content.strip()
            source = result.document_metadata.get("filename", "Unknown source")
            
            context_part = f"""
Context {i} (from {source}):
{chunk_text}
            """.strip()
            context_parts.append(context_part)
        
        return "\\n\\n".join(context_parts)
    
    async def _generate_question_batch(
        self,
        context_text: str,
        subject: str,
        grade_level: int,
        topic: str,
        difficulty: str,
        count: int,
        context_chunks: List[RAGResult]
    ) -> List[EnhancedAssessmentQuestion]:
        """Generate a batch of questions."""
        
        try:
            # Create the prompt
            prompt = self._create_question_prompt(
                context_text, subject, grade_level, topic, difficulty, count
            )
            
            # Get response from LLM
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            response_text = response.content
            
            # Parse the response
            questions = self._parse_llm_response(
                response_text, context_chunks, difficulty, topic
            )
            
            return questions
            
        except Exception as e:
            logger.error(f"Batch question generation failed: {str(e)}")
            return []
    
    def _create_question_prompt(
        self,
        context_text: str,
        subject: str,
        grade_level: int,
        topic: str,
        difficulty: str,
        count: int
    ) -> str:
        """Create the prompt for question generation."""
        
        difficulty_guidance = {
            "easy": "Create simple, direct questions that test basic recall and understanding. Use simple vocabulary appropriate for the grade level.",
            "medium": "Create questions that require some analysis or application of concepts. Include one-step problem solving.",
            "hard": "Create challenging questions that require analysis, synthesis, or multi-step reasoning. Test deeper understanding."
        }
        
        bloom_levels = {
            "easy": "Remember, Understand",
            "medium": "Apply, Analyze", 
            "hard": "Evaluate, Create"
        }
        
        prompt = f"""You are an expert educational assessment designer. Create {count} high-quality multiple-choice questions based on the provided context.

REQUIREMENTS:
- Subject: {subject}
- Grade Level: {grade_level}
- Topic: {topic}
- Difficulty: {difficulty}
- Question Type: Multiple choice with 4 options

DIFFICULTY GUIDANCE:
{difficulty_guidance.get(difficulty, difficulty_guidance["medium"])}

BLOOM'S TAXONOMY LEVEL:
Target cognitive levels: {bloom_levels.get(difficulty, bloom_levels["medium"])}

CONTEXT MATERIAL:
{context_text}

FORMATTING REQUIREMENTS:
Return EXACTLY {count} questions in JSON format as follows:

```json
[
  {{
    "question_text": "Clear, specific question based on the context",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "correct_answer": 0,
    "explanation": "Clear explanation of why the answer is correct",
    "bloom_level": "Remember/Understand/Apply/Analyze/Evaluate/Create",
    "learning_objectives": ["What students should learn from this question"]
  }}
]
```

QUALITY STANDARDS:
1. Questions MUST be based on the provided context
2. Use age-appropriate vocabulary for grade {grade_level}
3. Ensure one clearly correct answer
4. Make distractors plausible but clearly wrong
5. Include educational explanations
6. Avoid trick questions or ambiguous wording
7. Test important concepts, not trivial details

Generate {count} question(s) now:"""

        return prompt
    
    def _parse_llm_response(
        self,
        response_text: str,
        context_chunks: List[RAGResult],
        difficulty: str,
        topic: str
    ) -> List[EnhancedAssessmentQuestion]:
        """Parse the LLM response into question objects."""
        
        questions = []
        
        try:
            # Extract JSON from response
            json_start = response_text.find('[')
            json_end = response_text.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No JSON array found in response")
            
            json_str = response_text[json_start:json_end]
            parsed_questions = json.loads(json_str)
            
            # Convert to EnhancedAssessmentQuestion objects
            for i, q_data in enumerate(parsed_questions):
                try:
                    question = EnhancedAssessmentQuestion(
                        question_id=f"gen_{hash(q_data['question_text'])}_{i}",
                        question_text=q_data["question_text"],
                        options=q_data["options"],
                        correct_answer=q_data["correct_answer"],
                        explanation=q_data["explanation"],
                        difficulty=difficulty,
                        topic=topic,
                        context=GeneratedQuestionContext(
                            source_chunks=[chunk.chunk.chunk_id for chunk in context_chunks],
                            confidence_score=0.85,  # Default confidence
                            generation_metadata={
                                "model": "gemini-2.5-pro",
                                "bloom_level": q_data.get("bloom_level", ""),
                                "context_sources": len(context_chunks)
                            }
                        ),
                        bloom_taxonomy_level=q_data.get("bloom_level", ""),
                        learning_objectives=q_data.get("learning_objectives", [])
                    )
                    questions.append(question)
                    
                except Exception as e:
                    logger.warning(f"Failed to parse question {i}: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"Failed to parse LLM response: {str(e)}")
            logger.debug(f"Response text: {response_text[:500]}...")
        
        return questions
    
    def _convert_to_assessment_question(
        self,
        enhanced_question: EnhancedAssessmentQuestion
    ) -> AssessmentQuestion:
        """Convert enhanced question to standard assessment question."""
        
        return AssessmentQuestion(
            question_id=enhanced_question.question_id,
            question_text=enhanced_question.question_text,
            options=enhanced_question.options,
            correct_answer=enhanced_question.correct_answer,
            explanation=enhanced_question.explanation,
            difficulty=enhanced_question.difficulty,
            topic=enhanced_question.topic
        )
    
    async def validate_question_quality(
        self,
        question: EnhancedAssessmentQuestion,
        context_chunks: List[RAGResult]
    ) -> Dict[str, Any]:
        """Validate the quality of a generated question."""
        
        quality_score = 0.0
        issues = []
        
        # Check if question is based on context
        question_lower = question.question_text.lower()
        context_match = False
        
        for chunk in context_chunks:
            if any(word in chunk.chunk.content.lower() for word in question_lower.split() if len(word) > 3):
                context_match = True
                break
        
        if context_match:
            quality_score += 0.3
        else:
            issues.append("Question may not be based on provided context")
        
        # Check answer options
        if len(set(question.options)) == len(question.options):
            quality_score += 0.2
        else:
            issues.append("Duplicate answer options found")
        
        # Check correct answer index
        if 0 <= question.correct_answer < len(question.options):
            quality_score += 0.2
        else:
            issues.append("Invalid correct answer index")
        
        # Check explanation quality
        if len(question.explanation) > 10:
            quality_score += 0.15
        else:
            issues.append("Explanation too short")
        
        # Check question length (not too short or too long)
        if 10 <= len(question.question_text) <= 200:
            quality_score += 0.15
        else:
            issues.append("Question length inappropriate")
        
        return {
            "quality_score": quality_score,
            "issues": issues,
            "is_acceptable": quality_score >= 0.7 and len(issues) <= 1
        }


================================================================================
File: app/agents/question_stub_agent.py
Size: 3.63 kB
================================================================================

# FILE: app/agents/question_stub_agent.py

import logging
from typing import List, Dict, Any, Optional
import uuid

from app.models.rag_models import (
    RAGResult, 
    QuestionGenerationRequest, 
    EnhancedAssessmentQuestion,
    GeneratedQuestionContext
)

logger = logging.getLogger(__name__)

class QuestionStubAgent:
    """Stub question generator for when AI dependencies aren't available."""
    
    def __init__(self):
        self.is_ai_enabled = False
        logger.warning("Using question stub agent - AI generation not available")
    
    async def generate_questions(
        self,
        request: QuestionGenerationRequest
    ) -> List[EnhancedAssessmentQuestion]:
        """Generate simple template questions as fallback."""
        
        logger.info(f"Stub agent: would generate {request.question_count} questions for {request.subject}")
        
        questions = []
        
        # Create simple template questions based on subject
        templates = self._get_templates(request.subject, request.grade_level)
        
        for i in range(min(request.question_count, len(templates))):
            template = templates[i % len(templates)]
            
            question = EnhancedAssessmentQuestion(
                question_id=f"stub_{uuid.uuid4()}_{i}",
                question_text=template["question"],
                options=template["options"],
                correct_answer=template["correct"],
                explanation=template["explanation"],
                difficulty=request.difficulty_level,
                topic=request.topic,
                context=GeneratedQuestionContext(
                    source_chunks=[],
                    confidence_score=0.5,  # Low confidence for stub
                    generation_metadata={
                        "model": "stub",
                        "method": "template",
                        "ai_enabled": False
                    }
                ),
                bloom_taxonomy_level="Remember",
                learning_objectives=[f"Basic {request.topic} understanding"]
            )
            questions.append(question)
        
        return questions
    
    def _get_templates(self, subject: str, grade_level: int) -> List[Dict[str, Any]]:
        """Get simple question templates by subject."""
        
        print("Still using Simple")
        templates = {
            "Mathematics": [
                {
                    "question": "What is 2 + 3?",
                    "options": ["4", "5", "6", "7"],
                    "correct": 1,
                    "explanation": "2 + 3 = 5"
                },
                {
                    "question": "What is 10 - 4?",
                    "options": ["5", "6", "7", "8"],
                    "correct": 1,
                    "explanation": "10 - 4 = 6"
                }
            ],
            "Science": [
                {
                    "question": "What planet is closest to the Sun?",
                    "options": ["Venus", "Mercury", "Earth", "Mars"],
                    "correct": 1,
                    "explanation": "Mercury is the closest planet to the Sun"
                }
            ],
            "English": [
                {
                    "question": "What is a noun?",
                    "options": ["Action word", "Describing word", "Person, place, or thing", "Connecting word"],
                    "correct": 2,
                    "explanation": "A noun is a person, place, or thing"
                }
            ]
        }
        
        return templates.get(subject, templates["Mathematics"])


================================================================================
File: app/agents/teacher_agent/__init__.py
Size: 61 B
================================================================================

# FILE: agents/teacher_agent/__init__.py

from . import agent

================================================================================
File: app/agents/teacher_agent/agent.py
Size: 2.55 kB
================================================================================

# FILE: agents/teacher_agent/agent.py

from __future__ import annotations
from google.adk.agents import Agent
from app.agents.tools.profile_tools import update_teacher_subjects, get_teacher_subjects
from app.core.config import settings

root_agent = Agent(
    model=settings.gemini_model_name,
    name="teacher_agent",
    description="A specialized assistant for existing teachers to manage their profile and educational content.",
    instruction="""
      You are a specialized assistant for EXISTING teachers on the Edvance platform. 
      You help teachers who already have profiles manage their subjects and get teaching assistance.
      
      **Your Primary Functions:**
      
      1. **Subject Management for Existing Users:**
         - Use `get_teacher_subjects` to check their current subjects
         - Use `update_teacher_subjects` to modify their subject list
         - When adding subjects, include their existing subjects plus the new ones
         - When replacing subjects, use only the new list they specify
      
      2. **Teaching Assistance:**
         - Provide general teaching guidance and support
         - Help with educational content questions
         - Assist with teaching strategies and methods
      
      **Key Guidelines:**
      - Always check current subjects first when relevant to the conversation
      - Be helpful with subject management (add, remove, update subjects)
      - Provide educational guidance and teaching support
      - If user seems to be new or mentions onboarding, let them know they should speak with the onboarding specialist
      
      **Examples of requests you handle:**
      - "Add Physics to my subjects"
      - "What are my current subjects?"
      - "Replace my subjects with Math, Science, English"
      - "Remove Chemistry from my subjects"
      - "How can I teach fractions better?"
      - "Help me with lesson planning"
    """,
    tools=[
        get_teacher_subjects,
        update_teacher_subjects,
    ],
      - Always be helpful, professional, and encouraging
      - Provide clear guidance on next steps
      - Celebrate completed milestones
      - If unsure about user status, check onboarding status first
      - Handle both onboarding and ongoing profile management seamlessly
    """,
    tools=[
        # Onboarding tools
        get_onboarding_status,
        create_teacher_profile,
        complete_onboarding_step,
        
        # Profile management tools
        get_teacher_subjects,
        update_teacher_subjects,
    ],
)

================================================================================
File: app/agents/teacher_onboarding_agent/__init__.py
Size: 77 B
================================================================================

# FILE: app/agents/teacher_onboarding_agent/__init__.py

from . import agent


================================================================================
File: app/agents/teacher_onboarding_agent/agent.py
Size: 2.35 kB
================================================================================

# FILE: app/agents/teacher_onboarding_agent/agent.py

from __future__ import annotations
from google.adk.agents import Agent
from app.agents.tools.profile_tools import update_teacher_subjects, get_teacher_subjects
from app.agents.tools.onboarding_tools import (
    create_teacher_profile, 
    get_onboarding_status,
    complete_onboarding_step
)
from app.core.config import settings

root_agent = Agent(
    model=settings.gemini_model_name,
    name="teacher_onboarding_agent",
    description="A comprehensive agent that handles ALL teacher-related tasks, from onboarding new teachers to managing existing teacher profiles.",
    instruction="""
      You are a comprehensive teacher assistant that handles ALL teacher-related tasks.
      You help both new and existing teachers with their needs.
      
      **For New Teachers (Onboarding):**
      1. Check their onboarding status with `get_onboarding_status`
      2. Create their profile with `create_teacher_profile` if needed
      3. Guide them through setup steps
      4. Complete onboarding steps with `complete_onboarding_step`
      
      **For Existing Teachers (Profile Management):**
      1. Use `get_teacher_subjects` to see their current subjects
      2. Use `update_teacher_subjects` to modify their subject list
      3. Help with general teaching questions
      4. Assist with profile updates
      
      **Your Approach:**
      - Always be helpful and welcoming
      - For new users: Start with onboarding status check
      - For existing users: Check their current subjects first
      - When updating subjects: Always show current subjects before updating
      - Guide users through each step clearly
      - Celebrate milestones and completed tasks
      
      **When users ask to add subjects:**
      1. Get their current subjects first
      2. Add the new subject(s) to the existing list
      3. Update with the complete list
      
      **When users ask to set/replace subjects:**
      1. Replace with the new list they specify
      2. Confirm the change
      
      You handle the complete teacher journey from first signup to daily profile management.
    """,
    tools=[
        get_teacher_subjects,
        update_teacher_subjects,
        create_teacher_profile,
        get_onboarding_status,
        complete_onboarding_step,
    ],
)


================================================================================
File: app/agents/tools/learning_path_tools.py
Size: 24.7 kB
================================================================================

# FILE: app/agents/tools/learning_path_tools.py

import logging
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from app.core.firebase import db
from app.services.assessment_analysis_service import assessment_analysis_service
from app.services.learning_path_service import learning_path_service
from app.services.enhanced_assessment_service import enhanced_assessment_service
from app.models.learning_models import StudentPerformance, KnowledgeGap

logger = logging.getLogger(__name__)

async def monitor_student_assessments(teacher_uid: str, continuous: bool = True) -> Dict[str, Any]:
    """
    Monitor student assessment completion and trigger automatic learning path generation.
    
    Args:
        teacher_uid: Teacher's unique identifier
        continuous: Whether to run continuous monitoring
    
    Returns:
        Dict containing monitoring status and actions taken
    """
    try:
        logger.info(f"Starting assessment monitoring for teacher {teacher_uid}")
        
        monitoring_results = {
            "teacher_uid": teacher_uid,
            "monitoring_active": True,
            "assessments_processed": 0,
            "learning_paths_generated": 0,
            "students_helped": [],
            "actions_taken": [],
            "start_time": datetime.utcnow().isoformat()
        }
        
        if continuous:
            # Set up continuous monitoring (would be implemented with Firestore listeners)
            logger.info("Setting up continuous assessment monitoring...")
            monitoring_results["monitoring_type"] = "continuous"
            monitoring_results["status"] = "Continuous monitoring activated - will automatically process new assessments"
        else:
            # Process current pending assessments
            pending_assessments = await _get_pending_assessments(teacher_uid)
            
            for assessment_completion in pending_assessments:
                result = await analyze_assessment_completion(
                    assessment_completion["student_id"],
                    assessment_completion["assessment_id"],
                    assessment_completion["student_answers"],
                    assessment_completion["time_taken_minutes"]
                )
                
                monitoring_results["assessments_processed"] += 1
                monitoring_results["actions_taken"].append(result)
                
                if result.get("learning_path_generated"):
                    monitoring_results["learning_paths_generated"] += 1
                    if assessment_completion["student_id"] not in monitoring_results["students_helped"]:
                        monitoring_results["students_helped"].append(assessment_completion["student_id"])
            
            monitoring_results["status"] = f"Processed {monitoring_results['assessments_processed']} assessments"
        
        return monitoring_results
        
    except Exception as e:
        logger.error(f"Failed to monitor assessments: {str(e)}")
        return {
            "error": f"Assessment monitoring failed: {str(e)}",
            "teacher_uid": teacher_uid,
            "monitoring_active": False
        }

async def analyze_assessment_completion(
    student_id: str,
    assessment_id: str,
    student_answers: List[int],
    time_taken_minutes: int
) -> Dict[str, Any]:
    """
    Automatically analyze a completed assessment and determine next actions.
    
    Args:
        student_id: Student who completed the assessment
        assessment_id: Assessment that was completed
        student_answers: Student's answer choices
        time_taken_minutes: Time taken to complete
    
    Returns:
        Dict containing analysis results and actions taken
    """
    try:
        logger.info(f"Analyzing assessment completion for student {student_id}")
        
        # Get the assessment
        assessment = await enhanced_assessment_service.get_assessment_by_id(assessment_id)
        if not assessment:
            # If assessment not found, create a mock assessment for testing
            logger.warning(f"Assessment {assessment_id} not found, creating mock assessment for testing")
            
            # Create mock performance analysis
            # Calculate score based on student answers (assuming 4 questions)
            total_questions = len(student_answers)
            correct_answers = sum(student_answers)  # Assuming 1 = correct, 0 = incorrect
            score_percentage = (correct_answers / total_questions) * 100 if total_questions > 0 else 0
            
            analysis_result = {
                "student_id": student_id,
                "assessment_id": assessment_id,
                "performance_score": score_percentage,
                "analysis_completed": True,
                "learning_path_generated": False,
                "intervention_type": None,
                "actions_taken": []
            }
            
            # Determine intervention type based on performance
            if score_percentage < 70:
                analysis_result["intervention_type"] = "comprehensive_support"
                # Automatically generate learning path
                path_result = await generate_learning_path_automatically(
                    student_id, "test_teacher_uid", "Mathematics", 5, focused=False, enrichment=False
                )
                analysis_result["learning_path_generated"] = path_result.get("success", False)
                analysis_result["learning_path_id"] = path_result.get("path_id")
                analysis_result["actions_taken"].append("Generated comprehensive learning path")
                
            elif score_percentage < 85:
                analysis_result["intervention_type"] = "targeted_improvement"
                # Generate focused learning path
                path_result = await generate_learning_path_automatically(
                    student_id, "test_teacher_uid", "Mathematics", 5, focused=True, enrichment=False
                )
                analysis_result["learning_path_generated"] = path_result.get("success", False)
                analysis_result["learning_path_id"] = path_result.get("path_id")
                analysis_result["actions_taken"].append("Generated targeted learning path")
                
            else:
                analysis_result["intervention_type"] = "enrichment"
                # Generate advanced learning path
                path_result = await generate_learning_path_automatically(
                    student_id, "test_teacher_uid", "Mathematics", 6, focused=False, enrichment=True
                )
                analysis_result["learning_path_generated"] = path_result.get("success", False)
                analysis_result["learning_path_id"] = path_result.get("path_id")
                analysis_result["actions_taken"].append("Generated enrichment learning path")
            
            # Log the analysis
            await _log_analysis_action(analysis_result)
            
            logger.info(f"Completed mock analysis for student {student_id}: {analysis_result['intervention_type']}")
            return analysis_result
        
        # Perform comprehensive analysis with real assessment
        performance = await assessment_analysis_service.analyze_assessment_performance(
            student_id=student_id,
            assessment=assessment,
            student_answers=student_answers,
            time_taken_minutes=time_taken_minutes
        )
        
        analysis_result = {
            "student_id": student_id,
            "assessment_id": assessment_id,
            "performance_score": performance.score_percentage,
            "analysis_completed": True,
            "learning_path_generated": False,
            "intervention_type": None,
            "actions_taken": []
        }
        
        # Determine intervention type based on performance
        if performance.score_percentage < 70:
            analysis_result["intervention_type"] = "comprehensive_support"
            # Automatically generate learning path
            path_result = await generate_learning_path_automatically(
                student_id, assessment.teacher_uid, assessment.subject, assessment.grade
            )
            analysis_result["learning_path_generated"] = path_result.get("success", False)
            analysis_result["learning_path_id"] = path_result.get("path_id")
            analysis_result["actions_taken"].append("Generated comprehensive learning path")
            
        elif performance.score_percentage < 85:
            analysis_result["intervention_type"] = "targeted_improvement"
            # Generate focused learning path
            path_result = await generate_learning_path_automatically(
                student_id, assessment.teacher_uid, assessment.subject, assessment.grade, focused=True
            )
            analysis_result["learning_path_generated"] = path_result.get("success", False)
            analysis_result["learning_path_id"] = path_result.get("path_id")
            analysis_result["actions_taken"].append("Generated targeted learning path")
            
        else:
            analysis_result["intervention_type"] = "enrichment"
            # Generate advanced learning path
            path_result = await generate_learning_path_automatically(
                student_id, assessment.teacher_uid, assessment.subject, assessment.grade + 1, enrichment=True
            )
            analysis_result["learning_path_generated"] = path_result.get("success", False)
            analysis_result["learning_path_id"] = path_result.get("path_id")
            analysis_result["actions_taken"].append("Generated enrichment learning path")
        
        # Log the analysis
        await _log_analysis_action(analysis_result)
        
        logger.info(f"Completed analysis for student {student_id}: {analysis_result['intervention_type']}")
        return analysis_result
        
    except Exception as e:
        logger.error(f"Failed to analyze assessment completion: {str(e)}")
        return {
            "error": f"Analysis failed: {str(e)}",
            "student_id": student_id,
            "assessment_id": assessment_id
        }

async def generate_learning_path_automatically(
    student_id: str,
    teacher_uid: str,
    subject: str,
    grade: int,
    focused: bool = False,
    enrichment: bool = False
) -> Dict[str, Any]:
    """
    Automatically generate a learning path based on recent assessment performance.
    
    Args:
        student_id: Student to generate path for
        teacher_uid: Teacher creating the path
        subject: Subject area
        grade: Target grade level
        focused: Whether to create a focused path for specific gaps
        enrichment: Whether to create an enrichment path for advanced students
    
    Returns:
        Dict containing generation results
    """
    try:
        logger.info(f"Auto-generating learning path for student {student_id}")
        
        # Get student's recent performance and knowledge gaps
        progress_summary = await assessment_analysis_service.get_student_progress_summary(student_id)
        
        # For now, create mock knowledge gaps - in production this would come from actual analysis
        knowledge_gaps = []  # This would be populated from real analysis
        student_performances = []  # This would come from recent assessments
        
        # Set learning goals based on path type
        if enrichment:
            learning_goals = [
                f"Advanced {subject} problem-solving",
                "Higher-order thinking skills",
                "Creative application of concepts"
            ]
        elif focused:
            learning_goals = [
                f"Strengthen specific {subject} concepts",
                "Fill identified knowledge gaps",
                "Build confidence in weak areas"
            ]
        else:
            learning_goals = [
                f"Master fundamental {subject} concepts",
                "Build strong foundation",
                "Develop problem-solving skills"
            ]
        
        # Generate the learning path
        learning_path = await learning_path_service.generate_personalized_learning_path(
            student_id=student_id,
            teacher_uid=teacher_uid,
            knowledge_gaps=knowledge_gaps,
            student_performances=student_performances,
            target_subject=subject,
            target_grade=grade,
            learning_goals=learning_goals
        )
        
        generation_result = {
            "success": True,
            "path_id": learning_path.path_id,
            "student_id": student_id,
            "teacher_uid": teacher_uid,
            "path_type": "enrichment" if enrichment else "focused" if focused else "comprehensive",
            "total_steps": len(learning_path.steps),
            "estimated_duration_hours": learning_path.total_estimated_duration_minutes / 60,
            "learning_goals": learning_path.learning_goals,
            "generated_at": datetime.utcnow().isoformat()
        }
        
        # Schedule progress monitoring
        await _schedule_progress_monitoring(learning_path.path_id, student_id)
        
        logger.info(f"Generated learning path {learning_path.path_id} with {len(learning_path.steps)} steps")
        return generation_result
        
    except Exception as e:
        logger.error(f"Failed to generate learning path automatically: {str(e)}")
        return {
            "success": False,
            "error": f"Path generation failed: {str(e)}",
            "student_id": student_id
        }

async def track_learning_progress(student_id: str, path_id: str) -> Dict[str, Any]:
    """
    Track and analyze student progress through a learning path.
    
    Args:
        student_id: Student to track
        path_id: Learning path to monitor
    
    Returns:
        Dict containing progress analysis and recommendations
    """
    try:
        logger.info(f"Tracking learning progress for student {student_id} on path {path_id}")
        
        # Get current learning path
        learning_path = await learning_path_service.get_learning_path(path_id)
        if not learning_path:
            return {"error": "Learning path not found", "path_id": path_id}
        
        # Analyze progress patterns
        completed_steps = [step for step in learning_path.steps if step.is_completed]
        in_progress_steps = [step for step in learning_path.steps if not step.is_completed]
        
        progress_analysis = {
            "student_id": student_id,
            "path_id": path_id,
            "completion_percentage": learning_path.completion_percentage,
            "total_steps": len(learning_path.steps),
            "completed_steps": len(completed_steps),
            "remaining_steps": len(in_progress_steps),
            "estimated_remaining_time": sum(step.estimated_duration_minutes for step in in_progress_steps),
            "progress_trend": "steady",  # Would be calculated from completion patterns
            "performance_scores": [step.performance_score for step in completed_steps if step.performance_score],
            "recommendations": []
        }
        
        # Generate recommendations based on progress
        if progress_analysis["completion_percentage"] < 30 and len(completed_steps) > 2:
            # Student may be struggling
            progress_analysis["recommendations"].append("Consider additional support or simplified steps")
            progress_analysis["progress_trend"] = "concerning"
            
        elif progress_analysis["completion_percentage"] > 80:
            # Student is doing well
            progress_analysis["recommendations"].append("Consider advanced content or acceleration")
            progress_analysis["progress_trend"] = "excellent"
            
        # Check for adaptation needs
        avg_performance = sum(progress_analysis["performance_scores"]) / len(progress_analysis["performance_scores"]) if progress_analysis["performance_scores"] else 0
        
        if avg_performance < 70 and len(progress_analysis["performance_scores"]) >= 3:
            progress_analysis["recommendations"].append("Learning path may need adaptation")
            adaptation_result = await adapt_learning_path_on_new_data(path_id, student_id)
            progress_analysis["adaptation_triggered"] = adaptation_result.get("success", False)
        
        return progress_analysis
        
    except Exception as e:
        logger.error(f"Failed to track learning progress: {str(e)}")
        return {
            "error": f"Progress tracking failed: {str(e)}",
            "student_id": student_id,
            "path_id": path_id
        }

async def adapt_learning_path_on_new_data(path_id: str, student_id: str) -> Dict[str, Any]:
    """
    Automatically adapt a learning path based on student performance data.
    
    Args:
        path_id: Learning path to adapt
        student_id: Student the path is for
    
    Returns:
        Dict containing adaptation results
    """
    try:
        logger.info(f"Adapting learning path {path_id} for student {student_id}")
        
        # Get current path and recent performance
        learning_path = await learning_path_service.get_learning_path(path_id)
        if not learning_path:
            return {"error": "Learning path not found", "path_id": path_id}
        
        # Analyze need for adaptation
        completed_steps = [step for step in learning_path.steps if step.is_completed]
        performance_scores = [step.performance_score for step in completed_steps if step.performance_score]
        
        adaptation_needed = False
        adaptation_type = None
        
        if performance_scores:
            avg_performance = sum(performance_scores) / len(performance_scores)
            
            if avg_performance < 60:
                adaptation_needed = True
                adaptation_type = "simplify_and_reinforce"
            elif avg_performance > 90 and len(performance_scores) >= 3:
                adaptation_needed = True
                adaptation_type = "accelerate_and_challenge"
        
        adaptation_result = {
            "path_id": path_id,
            "student_id": student_id,
            "adaptation_needed": adaptation_needed,
            "adaptation_type": adaptation_type,
            "success": False,
            "changes_made": []
        }
        
        if adaptation_needed:
            # For now, simulate adaptation - in production this would modify the actual path
            if adaptation_type == "simplify_and_reinforce":
                adaptation_result["changes_made"] = [
                    "Added prerequisite review steps",
                    "Simplified current difficulty level",
                    "Increased practice opportunities"
                ]
            elif adaptation_type == "accelerate_and_challenge":
                adaptation_result["changes_made"] = [
                    "Skipped redundant steps",
                    "Increased difficulty level",
                    "Added advanced challenges"
                ]
            
            adaptation_result["success"] = True
            
            # Record adaptation in path history
            adaptation_record = {
                "timestamp": datetime.utcnow().isoformat(),
                "trigger": "automatic_performance_analysis",
                "type": adaptation_type,
                "changes": adaptation_result["changes_made"]
            }
            
            learning_path.adaptation_history.append(adaptation_record)
            await learning_path_service._save_learning_path(learning_path)
        
        logger.info(f"Adaptation analysis complete for path {path_id}: {adaptation_result}")
        return adaptation_result
        
    except Exception as e:
        logger.error(f"Failed to adapt learning path: {str(e)}")
        return {
            "error": f"Adaptation failed: {str(e)}",
            "path_id": path_id,
            "success": False
        }

async def get_student_learning_status(student_id: str) -> Dict[str, Any]:
    """
    Get comprehensive learning status for a student.
    
    Args:
        student_id: Student to get status for
    
    Returns:
        Dict containing complete learning status
    """
    try:
        logger.info(f"Getting learning status for student {student_id}")
        
        # Get student's learning paths
        learning_paths = await learning_path_service.get_student_learning_paths(student_id)
        
        # Get recent progress summary
        progress_summary = await assessment_analysis_service.get_student_progress_summary(student_id)
        
        # Analyze current status
        status = {
            "student_id": student_id,
            "total_learning_paths": len(learning_paths),
            "active_paths": len([p for p in learning_paths if not p.completed_at and p.started_at]),
            "completed_paths": len([p for p in learning_paths if p.completed_at]),
            "overall_progress": 0,
            "current_focus_areas": [],
            "recent_achievements": [],
            "recommended_actions": [],
            "next_assessments_due": [],
            "learning_velocity": "steady"
        }
        
        if learning_paths:
            # Calculate overall progress
            total_completion = sum(path.completion_percentage for path in learning_paths)
            status["overall_progress"] = total_completion / len(learning_paths)
            
            # Identify current focus areas
            active_paths = [p for p in learning_paths if not p.completed_at and p.started_at]
            for path in active_paths:
                current_step = path.current_step
                if current_step < len(path.steps):
                    step = path.steps[current_step]
                    status["current_focus_areas"].append(f"{step.topic} - {step.title}")
            
            # Recent achievements
            recently_completed = [p for p in learning_paths if p.completed_at and 
                                (datetime.utcnow() - p.completed_at).days <= 7]
            status["recent_achievements"] = [f"Completed {p.title}" for p in recently_completed]
        
        # Generate recommendations
        if status["active_paths"] == 0 and status["completed_paths"] > 0:
            status["recommended_actions"].append("Ready for new assessment to generate next learning path")
        elif status["overall_progress"] < 50:
            status["recommended_actions"].append("Continue with current learning path")
        elif status["overall_progress"] > 80:
            status["recommended_actions"].append("Consider advanced challenges")
        
        return status
        
    except Exception as e:
        logger.error(f"Failed to get student learning status: {str(e)}")
        return {
            "error": f"Status retrieval failed: {str(e)}",
            "student_id": student_id
        }

# Helper functions

async def _get_pending_assessments(teacher_uid: str) -> List[Dict[str, Any]]:
    """Get assessments that have been completed but not yet processed for learning path generation."""
    # This would query Firestore for recently completed assessments
    # For now, return empty list
    return []

async def _log_analysis_action(analysis_result: Dict[str, Any]) -> None:
    """Log analysis actions for monitoring and debugging."""
    try:
        log_entry = {
            "timestamp": datetime.utcnow(),
            "action_type": "assessment_analysis",
            "student_id": analysis_result["student_id"],
            "assessment_id": analysis_result["assessment_id"],
            "performance_score": analysis_result["performance_score"],
            "intervention_type": analysis_result["intervention_type"],
            "learning_path_generated": analysis_result["learning_path_generated"]
        }
        
        # Save to Firestore analytics collection
        db.collection("learning_analytics").add(log_entry)
        
    except Exception as e:
        logger.warning(f"Failed to log analysis action: {str(e)}")

async def _schedule_progress_monitoring(path_id: str, student_id: str) -> None:
    """Schedule periodic progress monitoring for a learning path."""
    # This would set up scheduled tasks to monitor progress
    # For now, just log the intent
    logger.info(f"Scheduled progress monitoring for path {path_id}, student {student_id}")


================================================================================
File: app/agents/tools/lesson_tools.py
Size: 93.96 kB
================================================================================

# FILE: app/agents/tools/lesson_tools.py

import logging
import uuid
import json
import re
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from app.core.firebase import db
from app.core.vertex import get_vertex_model
from firebase_admin import firestore
from app.models.lesson_models import (
    LessonContent, LessonSlide, ContentElement, LessonChatSession,
    ChatMessage, SlideType, ContentElementType, InteractiveWidget,
    LessonProgress
)
from app.models.learning_models import LearningStep, DifficultyLevel
from app.services.enhanced_assessment_service import enhanced_assessment_service

logger = logging.getLogger(__name__)


def serialize_for_firestore(obj):
    """Helper function to serialize objects for Firestore storage"""
    if hasattr(obj, 'dict'):
        data = obj.dict()
    elif isinstance(obj, dict):
        data = obj
    else:
        return obj
    
    # Convert datetime objects to strings
    for key, value in data.items():
        if isinstance(value, datetime):
            data[key] = value.isoformat()
        elif hasattr(value, '__dict__') and hasattr(value, 'timestamp'):
            # Handle DatetimeWithNanoseconds
            data[key] = value.isoformat() if hasattr(value, 'isoformat') else str(value)
    
    return data

async def generate_lesson_content_ultra_fast(
    learning_step_id: str,
    student_id: str,
    teacher_uid: str,
    customizations: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate lesson content with ultra-fast approach - minimal AI calls, optimized prompts.
    
    Args:
        learning_step_id: ID of the learning step to create lesson for
        student_id: Student this lesson is for
        teacher_uid: Teacher who owns the learning path
        customizations: Optional customizations for the lesson
    
    Returns:
        Dict containing lesson generation results
    """
    try:
        start_time = datetime.utcnow()
        
        # Minimal data gathering - only get essentials
        logger.info(f"Starting ultra-fast lesson generation")
        
        # Get basic learning step info (could be from cache in production)
        learning_step = await _get_learning_step(learning_step_id)
        if not learning_step:
            return {"success": False, "error": "Learning step not found"}
        
        # Basic student context - minimal data
        student_context = {
            "student_id": student_id,
            "grade_level": 5,  # Default for speed
            "learning_style": "mixed"
        }
        
        gather_time = (datetime.utcnow() - start_time).total_seconds()
        logger.info(f"Minimal data gathering completed in {gather_time:.2f} seconds")
        
        # Ultra-optimized AI generation with concise prompt
        generation_start = datetime.utcnow()
        
        lesson_content = await _generate_lesson_ultra_fast(
            learning_step, student_context
        )
        
        generation_time = (datetime.utcnow() - generation_start).total_seconds()
        logger.info(f"Ultra-fast generation completed in {generation_time:.2f} seconds")
        
        # Quick save
        save_start = datetime.utcnow()
        lesson_id = await _save_lesson_content(lesson_content)
        save_time = (datetime.utcnow() - save_start).total_seconds()
        
        total_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Ultra-fast lesson {lesson_id} generated in {total_time:.2f}s")
        
        return {
            "success": True,
            "lesson_id": lesson_id,
            "title": lesson_content.title,
            "total_slides": len(lesson_content.slides),
            "estimated_duration_minutes": sum(slide.estimated_duration_minutes for slide in lesson_content.slides),
            "learning_objectives": lesson_content.learning_objectives,
            "generated_at": datetime.utcnow().isoformat(),
            "generation_metrics": {
                "total_time_seconds": total_time,
                "data_gathering_seconds": gather_time,
                "ai_generation_seconds": generation_time,
                "save_operations_seconds": save_time,
                "slides_generated": len(lesson_content.slides),
                "optimization_approach": "ultra_fast",
                "speed_optimizations": ["minimal_data_gathering", "concise_prompt", "template_based_fallback"]
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to generate ultra-fast lesson content: {str(e)}")
        return {"success": False, "error": str(e)}

async def generate_lesson_content_optimized(
    learning_step_id: str,
    student_id: str,
    teacher_uid: str,
    customizations: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive lesson content with optimized single AI call approach.
    
    Args:
        learning_step_id: ID of the learning step to create lesson for
        student_id: Student this lesson is for
        teacher_uid: Teacher who owns the learning path
        customizations: Optional customizations for the lesson
    
    Returns:
        Dict containing lesson generation results
    """
    try:
        start_time = datetime.utcnow()
        
        # Parallel data gathering phase (optimized)
        logger.info(f"Starting optimized data gathering for lesson generation")
        
        tasks = [
            _get_learning_step(learning_step_id),
            _get_student_context(student_id, teacher_uid),
            _get_relevant_teacher_content(teacher_uid, None, None)  # We'll filter this later
        ]
        
        learning_step, student_context, initial_teacher_content = await asyncio.gather(*tasks)
        
        if not learning_step:
            return {"success": False, "error": "Learning step not found"}
        
        # Get the properly filtered teacher content
        teacher_content = await _get_relevant_teacher_content(
            teacher_uid, learning_step.topic, learning_step.subject
        )
        
        gather_time = (datetime.utcnow() - start_time).total_seconds()
        logger.info(f"Data gathering completed in {gather_time:.2f} seconds")
        
        # Single comprehensive AI generation call
        generation_start = datetime.utcnow()
        
        lesson_content = await _generate_complete_lesson_optimized(
            learning_step, student_context, teacher_content, customizations
        )
        
        generation_time = (datetime.utcnow() - generation_start).total_seconds()
        logger.info(f"Complete lesson generation in {generation_time:.2f} seconds")
        
        # Parallel save operations
        save_start = datetime.utcnow()
        
        save_task = _save_lesson_content(lesson_content)
        progress_task = _initialize_lesson_progress(lesson_content.lesson_id, student_id)
        
        lesson_id, _ = await asyncio.gather(save_task, progress_task)
        
        save_time = (datetime.utcnow() - save_start).total_seconds()
        total_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Optimized lesson {lesson_id} generated in {total_time:.2f}s (gather: {gather_time:.2f}s, generation: {generation_time:.2f}s, save: {save_time:.2f}s)")
        
        return {
            "success": True,
            "lesson_id": lesson_id,
            "title": lesson_content.title,
            "total_slides": len(lesson_content.slides),
            "estimated_duration_minutes": sum(slide.estimated_duration_minutes for slide in lesson_content.slides),
            "learning_objectives": lesson_content.learning_objectives,
            "generated_at": datetime.utcnow().isoformat(),
            "generation_metrics": {
                "total_time_seconds": total_time,
                "data_gathering_seconds": gather_time,
                "ai_generation_seconds": generation_time,
                "save_operations_seconds": save_time,
                "slides_generated": len(lesson_content.slides),
                "optimization_approach": "single_ai_call",
                "api_calls_saved": len(lesson_content.slides) + 1  # Structure + individual slides
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to generate optimized lesson content: {str(e)}")
        return {"success": False, "error": str(e)}

async def generate_lesson_content(
    learning_step_id: str,
    student_id: str,
    teacher_uid: str,
    customizations: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive lesson content using the ultra-fast optimized approach.
    This is the production-ready method achieving ~27 second generation times.
    
    Args:
        learning_step_id: ID of the learning step to create lesson for
        student_id: Student this lesson is for
        teacher_uid: Teacher who owns the learning path
        customizations: Optional customizations for the lesson
    
    Returns:
        Dict containing lesson generation results
    """
    # Use ultra-fast approach for production performance
    return await _generate_lesson_ultra_fast(
        learning_step_id, student_id, teacher_uid, customizations
    )

async def generate_lesson_content_legacy(
    learning_step_id: str,
    student_id: str,
    teacher_uid: str,
    customizations: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive lesson content from a learning step with parallelized processing.
    Legacy method kept for fallback scenarios.
    
    Args:
        learning_step_id: ID of the learning step to create lesson for
        student_id: Student this lesson is for
        teacher_uid: Teacher who owns the learning path
        customizations: Optional customizations for the lesson
    
    Returns:
        Dict containing lesson generation results
    """
    try:
        start_time = datetime.utcnow()
        
        # Parallel data gathering phase
        logger.info(f"Starting parallel data gathering for lesson generation")
        
        tasks = [
            _get_learning_step(learning_step_id),
            _get_student_context(student_id, teacher_uid),
            _get_relevant_teacher_content(teacher_uid, None, None)  # We'll filter this later
        ]
        
        learning_step, student_context, initial_teacher_content = await asyncio.gather(*tasks)
        
        if not learning_step:
            return {"success": False, "error": "Learning step not found"}
        
        # Now get the properly filtered teacher content
        teacher_content = await _get_relevant_teacher_content(
            teacher_uid, learning_step.topic, learning_step.subject
        )
        
        gather_time = (datetime.utcnow() - start_time).total_seconds()
        logger.info(f"Data gathering completed in {gather_time:.2f} seconds")
        
        # Parallel lesson generation phase
        generation_start = datetime.utcnow()
        
        # Generate lesson structure and content in parallel
        lesson_structure_task = _generate_lesson_structure_with_ai(
            learning_step, student_context, customizations
        )
        
        content_outline_task = _generate_content_outline(
            learning_step, teacher_content
        )
        
        lesson_structure, content_outline = await asyncio.gather(
            lesson_structure_task, content_outline_task
        )
        
        structure_time = (datetime.utcnow() - generation_start).total_seconds()
        logger.info(f"Lesson structure generation completed in {structure_time:.2f} seconds")
        
        # Parallel slide content generation
        slides_start = datetime.utcnow()
        
        slide_tasks = []
        for i, slide_outline in enumerate(lesson_structure.get("slides", [])):
            task = _generate_individual_slide_content(
                slide_outline,
                content_outline,
                student_context,
                i + 1
            )
            slide_tasks.append(task)
        
        # Process slides in batches to avoid overwhelming the API
        batch_size = 3
        slide_batches = [slide_tasks[i:i + batch_size] for i in range(0, len(slide_tasks), batch_size)]
        
        all_slides = []
        for batch in slide_batches:
            batch_results = await asyncio.gather(*batch)
            all_slides.extend(batch_results)
        
        slides_time = (datetime.utcnow() - slides_start).total_seconds()
        logger.info(f"Slide content generation completed in {slides_time:.2f} seconds")
        
        # Build final lesson content
        lesson_content = await _build_lesson_content(
            lesson_structure, all_slides, learning_step, student_context
        )
        
        # Parallel save operations
        save_start = datetime.utcnow()
        
        save_task = _save_lesson_content(lesson_content)
        progress_task = _initialize_lesson_progress(lesson_content.lesson_id, student_id)
        
        lesson_id, _ = await asyncio.gather(save_task, progress_task)
        
        save_time = (datetime.utcnow() - save_start).total_seconds()
        total_time = (datetime.utcnow() - start_time).total_seconds()
        
        logger.info(f"Lesson {lesson_id} generated in {total_time:.2f}s (gather: {gather_time:.2f}s, structure: {structure_time:.2f}s, slides: {slides_time:.2f}s, save: {save_time:.2f}s)")
        
        return {
            "success": True,
            "lesson_id": lesson_id,
            "title": lesson_content.title,
            "total_slides": len(lesson_content.slides),
            "estimated_duration_minutes": sum(slide.estimated_duration_minutes for slide in lesson_content.slides),
            "learning_objectives": lesson_content.learning_objectives,
            "generated_at": datetime.utcnow().isoformat(),
            "generation_metrics": {
                "total_time_seconds": total_time,
                "data_gathering_seconds": gather_time,
                "structure_generation_seconds": structure_time,
                "slide_generation_seconds": slides_time,
                "save_operations_seconds": save_time,
                "slides_generated": len(all_slides),
                "parallel_optimization": True
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to generate lesson content: {str(e)}")
        return {"success": False, "error": str(e)}

async def generate_lesson_content_legacy(
    learning_step_id: str,
    student_id: str,
    teacher_uid: str,
    customizations: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive lesson content from a learning step.
    
    Args:
        learning_step_id: ID of the learning step to create lesson for
        student_id: Student this lesson is for
        teacher_uid: Teacher who owns the learning path
        customizations: Optional customizations for the lesson
    
    Returns:
        Dict containing lesson generation results
    """
    try:
        # Get the learning step
        learning_step = await _get_learning_step(learning_step_id)
        if not learning_step:
            return {"success": False, "error": "Learning step not found"}
        
        # Get student context for personalization
        student_context = await _get_student_context(student_id, teacher_uid)
        
        # Get teacher's content for RAG
        teacher_content = await _get_relevant_teacher_content(
            teacher_uid, learning_step.topic, learning_step.subject
        )
        
        # Generate lesson using AI
        lesson_content = await _generate_lesson_with_ai(
            learning_step, student_context, teacher_content, customizations
        )
        
        # Save lesson to database
        lesson_id = await _save_lesson_content(lesson_content)
        
        # Initialize progress tracking
        await _initialize_lesson_progress(lesson_id, student_id)
        
        logger.info(f"Generated lesson {lesson_id} for step {learning_step_id}")
        
        return {
            "success": True,
            "lesson_id": lesson_id,
            "title": lesson_content.title,
            "total_slides": len(lesson_content.slides),
            "estimated_duration_minutes": sum(slide.estimated_duration_minutes for slide in lesson_content.slides),
            "learning_objectives": lesson_content.learning_objectives,
            "generated_at": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to generate lesson content: {str(e)}")
        return {"success": False, "error": str(e)}

async def get_lesson_content(
    lesson_id: str,
    student_id: str,
    include_progress: bool = True
) -> Dict[str, Any]:
    """
    Retrieve lesson content with optional progress information.
    
    Args:
        lesson_id: ID of the lesson to retrieve
        student_id: Student requesting the lesson
        include_progress: Whether to include progress information
    
    Returns:
        Dict containing lesson content and progress
    """
    try:
        # Get lesson from database
        lesson_doc = db.collection("lessons").document(lesson_id).get()
        
        if not lesson_doc.exists:
            return {"success": False, "error": "Lesson not found"}
        
        lesson_data = lesson_doc.to_dict()
        
        # Verify student access
        if lesson_data.get("student_id") != student_id:
            return {"success": False, "error": "Access denied"}
        
        result = {
            "success": True,
            "lesson": lesson_data
        }
        
        # Add progress information if requested
        if include_progress:
            progress = await _get_lesson_progress(lesson_id, student_id)
            result["progress"] = progress
        
        return result
        
    except Exception as e:
        logger.error(f"Failed to get lesson content: {str(e)}")
        return {"success": False, "error": str(e)}

async def update_lesson_progress(
    lesson_id: str,
    student_id: str,
    slide_id: str,
    progress_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Update student progress on a lesson slide.
    
    Args:
        lesson_id: ID of the lesson
        student_id: Student ID
        slide_id: ID of the slide being updated
        progress_data: Progress information to update
    
    Returns:
        Dict containing update results
    """
    try:
        # Get current progress
        progress_doc = db.collection("lesson_progress").where(
            filter=firestore.FieldFilter("lesson_id", "==", lesson_id)
        ).where(
            filter=firestore.FieldFilter("student_id", "==", student_id)
        ).limit(1).get()
        
        if not progress_doc:
            return {"success": False, "error": "Progress record not found"}
        
        progress_ref = progress_doc[0].reference
        current_progress = progress_doc[0].to_dict()
        
        # Update slide progress
        slide_progress = current_progress.get("slide_progress", [])
        
        # Find and update the specific slide
        updated = False
        for slide_data in slide_progress:
            if slide_data.get("slide_id") == slide_id:
                slide_data.update(progress_data)
                slide_data["last_updated"] = datetime.utcnow()
                updated = True
                break
        
        if not updated:
            # Add new slide progress
            slide_progress.append({
                "slide_id": slide_id,
                "started_at": datetime.utcnow(),
                "last_updated": datetime.utcnow(),
                **progress_data
            })
        
        # Calculate overall progress
        completed_slides = sum(1 for slide in slide_progress if slide.get("is_completed", False))
        total_slides = max(current_progress.get("slides_total", 1), 1)  # Ensure at least 1 to avoid division by zero
        completion_percentage = (completed_slides / total_slides) * 100
        
        # Update progress document
        progress_ref.update({
            "slide_progress": slide_progress,
            "slides_completed": completed_slides,
            "completion_percentage": completion_percentage,
            "last_updated": datetime.utcnow(),
            "time_spent_minutes": current_progress.get("time_spent_minutes", 0) + progress_data.get("time_spent_minutes", 0)
        })
        
        # Check if lesson is completed
        if completion_percentage >= 100:
            progress_ref.update({
                "completed_at": datetime.utcnow()
            })
        
        logger.info(f"Updated lesson progress for student {student_id}, lesson {lesson_id}")
        
        return {
            "success": True,
            "slides_completed": completed_slides,
            "completion_percentage": completion_percentage,
            "lesson_completed": completion_percentage >= 100
        }
        
    except Exception as e:
        logger.error(f"Failed to update lesson progress: {str(e)}")
        return {"success": False, "error": str(e)}

async def start_lesson_chat(
    lesson_id: str,
    student_id: str,
    initial_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Start a new chatbot session for a lesson.
    
    Args:
        lesson_id: ID of the lesson
        student_id: Student starting the chat
        initial_message: Optional initial message from student
    
    Returns:
        Dict containing chat session information
    """
    try:
        # Create new chat session
        session_id = str(uuid.uuid4())
        
        chat_session = LessonChatSession(
            session_id=session_id,
            lesson_id=lesson_id,
            student_id=student_id,
            messages=[],
            is_active=True
        )
        
        # Add welcome message
        welcome_message = ChatMessage(
            message_id=str(uuid.uuid4()),
            lesson_id=lesson_id,
            sender="agent",
            message="Hello! I'm your lesson assistant. I'm here to help you understand the concepts and answer any questions you have. How can I help you today?",
            message_type="text"
        )
        
        chat_session.messages.append(welcome_message)
        chat_session.total_messages = 1
        chat_session.agent_responses = 1
        
        # Add initial student message if provided
        if initial_message:
            student_message = ChatMessage(
                message_id=str(uuid.uuid4()),
                lesson_id=lesson_id,
                sender="student",
                message=initial_message,
                message_type="text"
            )
            chat_session.messages.append(student_message)
            chat_session.total_messages += 1
            chat_session.student_questions += 1
        
        # Save to database
        db.collection("lesson_chats").document(session_id).set(chat_session.dict())
        
        logger.info(f"Started lesson chat session {session_id} for lesson {lesson_id}")
        
        return {
            "success": True,
            "session_id": session_id,
            "messages": [msg.dict() for msg in chat_session.messages]
        }
        
    except Exception as e:
        logger.error(f"Failed to start lesson chat: {str(e)}")
        return {"success": False, "error": str(e)}

async def send_chat_message(
    session_id: str,
    student_id: str,
    message: str,
    current_slide_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Send a message in a lesson chat session and get AI response.
    
    Args:
        session_id: Chat session ID
        student_id: Student sending the message
        message: Student's message
        current_slide_id: Current slide student is viewing
    
    Returns:
        Dict containing chat response
    """
    try:
        # Get chat session
        chat_doc = db.collection("lesson_chats").document(session_id).get()
        
        if not chat_doc.exists:
            return {"success": False, "error": "Chat session not found"}
        
        chat_data = chat_doc.to_dict()
        
        # Verify student access
        if chat_data.get("student_id") != student_id:
            return {"success": False, "error": "Access denied"}
        
        # Get lesson context
        lesson_context = await _get_lesson_context_for_chat(
            chat_data["lesson_id"], current_slide_id
        )
        
        # Generate AI response
        ai_response = await _generate_chat_response(
            message, lesson_context, chat_data["messages"]
        )
        
        # Create message objects
        student_message = ChatMessage(
            message_id=str(uuid.uuid4()),
            lesson_id=chat_data["lesson_id"],
            sender="student",
            message=message,
            current_slide_id=current_slide_id
        )
        
        agent_message = ChatMessage(
            message_id=str(uuid.uuid4()),
            lesson_id=chat_data["lesson_id"],
            sender="agent",
            message=ai_response["message"],
            confidence_score=ai_response.get("confidence_score"),
            sources=ai_response.get("sources", []),
            suggested_actions=ai_response.get("suggested_actions", [])
        )
        
        # Update chat session
        chat_ref = db.collection("lesson_chats").document(session_id)
        chat_ref.update({
            "messages": firestore.ArrayUnion([
                serialize_for_firestore(student_message), 
                serialize_for_firestore(agent_message)
            ]),
            "total_messages": firestore.Increment(2),
            "student_questions": firestore.Increment(1),
            "agent_responses": firestore.Increment(1),
            "last_activity": datetime.utcnow()
        })
        
        logger.info(f"Processed chat message in session {session_id}")
        
        return {
            "success": True,
            "agent_response": agent_message.dict(),
            "suggested_actions": ai_response.get("suggested_actions", [])
        }
        
    except Exception as e:
        logger.error(f"Failed to send chat message: {str(e)}")
        return {"success": False, "error": str(e)}

async def get_chat_history(
    session_id: str,
    student_id: str,
    limit: Optional[int] = None
) -> Dict[str, Any]:
    """
    Get chat history for a lesson session.
    
    Args:
        session_id: Chat session ID
        student_id: Student requesting the history
        limit: Optional limit on number of messages
    
    Returns:
        Dict containing chat history
    """
    try:
        # Get chat session
        chat_doc = db.collection("lesson_chats").document(session_id).get()
        
        if not chat_doc.exists:
            return {"success": False, "error": "Chat session not found"}
        
        chat_data = chat_doc.to_dict()
        
        # Verify student access
        if chat_data.get("student_id") != student_id:
            return {"success": False, "error": "Access denied"}
        
        messages = chat_data.get("messages", [])
        
        # Apply limit if specified
        if limit:
            messages = messages[-limit:]
        
        return {
            "success": True,
            "session_id": session_id,
            "messages": messages,
            "total_messages": len(messages),
            "session_active": chat_data.get("is_active", False)
        }
        
    except Exception as e:
        logger.error(f"Failed to get chat history: {str(e)}")
        return {"success": False, "error": str(e)}

async def generate_slide_content(
    slide_type: str,
    topic: str,
    learning_objective: str,
    grade_level: int,
    student_context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Generate content for a specific slide type.
    
    Args:
        slide_type: Type of slide to generate
        topic: Topic for the slide
        learning_objective: Learning objective for the slide
        grade_level: Target grade level
        student_context: Optional student context for personalization
    
    Returns:
        Dict containing generated slide content
    """
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        prompt = f"""Generate content for a {slide_type} slide about {topic} for grade {grade_level} students.

Learning Objective: {learning_objective}

Student Context: {json.dumps(student_context) if student_context else 'General audience'}

Create engaging, age-appropriate content that includes:
1. Clear title and subtitle
2. Main content elements (text, examples, activities)
3. Interactive elements where appropriate
4. Visual descriptions for diagrams or images needed

Return the response in this JSON format:
{{
  "title": "Slide title",
  "subtitle": "Optional subtitle",
  "content_elements": [
    {{
      "element_type": "text|image|interactive_widget|exercise",
      "title": "Element title",
      "content": "Element content or description",
      "position": 1,
      "styling": {{}},
      "interactive_widget": {{
        "widget_type": "multiple_choice|drag_drop|fill_blank",
        "title": "Widget title",
        "instructions": "Instructions for student",
        "content": {{}},
        "correct_answer": {{}},
        "hints": []
      }}
    }}
  ],
  "estimated_duration_minutes": 5,
  "completion_criteria": {{}}
}}"""

        response = await model.generate_content_async(prompt)
        
        # Parse JSON response
        try:
            slide_data = json.loads(response.text)
        except json.JSONDecodeError:
            # Fallback if JSON parsing fails
            slide_data = {
                "title": f"{topic} - {slide_type.title()}",
                "content_elements": [{
                    "element_type": "text",
                    "title": "Content",
                    "content": response.text,
                    "position": 1
                }],
                "estimated_duration_minutes": 5
            }
        
        logger.info(f"Generated {slide_type} slide content for {topic}")
        
        return {
            "success": True,
            "slide_content": slide_data
        }
        
    except Exception as e:
        logger.error(f"Failed to generate slide content: {str(e)}")
        return {"success": False, "error": str(e)}

async def create_interactive_element(
    element_type: str,
    topic: str,
    difficulty_level: str,
    learning_objective: str
) -> Dict[str, Any]:
    """
    Create an interactive element for a lesson slide.
    
    Args:
        element_type: Type of interactive element
        topic: Topic for the element
        difficulty_level: Difficulty level
        learning_objective: Learning objective
    
    Returns:
        Dict containing interactive element data
    """
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        prompt = f"""Create an interactive {element_type} element about {topic} at {difficulty_level} difficulty level.

Learning Objective: {learning_objective}

Generate appropriate content based on the element type:
- multiple_choice: Question with 4 options, 1 correct
- drag_drop: Items to drag and drop zones to match
- fill_blank: Text with blanks to fill in
- matching: Items to match with their pairs
- ordering: Items to put in correct sequence

Return JSON format:
{{
  "widget_type": "{element_type}",
  "title": "Interactive element title",
  "instructions": "Clear instructions for students",
  "content": {{
    // Element-specific content structure
  }},
  "correct_answer": {{
    // Correct answer data
  }},
  "hints": ["hint1", "hint2"],
  "points": 1,
  "feedback": {{
    "correct": "Positive feedback for correct answer",
    "incorrect": "Helpful feedback for incorrect answer"
  }}
}}"""

        response = await model.generate_content_async(prompt)
        
        try:
            element_data = json.loads(response.text)
        except json.JSONDecodeError:
            # Fallback
            element_data = {
                "widget_type": element_type,
                "title": f"{topic} Practice",
                "instructions": "Complete this activity",
                "content": {"question": response.text},
                "correct_answer": {},
                "hints": [],
                "points": 1
            }
        
        logger.info(f"Created {element_type} interactive element for {topic}")
        
        return {
            "success": True,
            "interactive_element": element_data
        }
        
    except Exception as e:
        logger.error(f"Failed to create interactive element: {str(e)}")
        return {"success": False, "error": str(e)}

async def adapt_lesson_difficulty(
    lesson_id: str,
    student_id: str,
    performance_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Adapt lesson difficulty based on student performance.
    
    Args:
        lesson_id: ID of the lesson to adapt
        student_id: Student ID
        performance_data: Recent performance data
    
    Returns:
        Dict containing adaptation results
    """
    try:
        # Get current lesson and progress
        lesson_doc = db.collection("lessons").document(lesson_id).get()
        if not lesson_doc.exists:
            return {"success": False, "error": "Lesson not found"}
        
        lesson_data = lesson_doc.to_dict()
        progress_data = await _get_lesson_progress(lesson_id, student_id)
        
        # Analyze performance and determine adaptations needed
        adaptations = await _analyze_performance_for_adaptation(
            performance_data, progress_data, lesson_data
        )
        
        if not adaptations["needs_adaptation"]:
            return {
                "success": True,
                "adapted": False,
                "message": "No adaptation needed based on current performance"
            }
        
        # Apply adaptations to lesson content
        adapted_content = await _apply_lesson_adaptations(
            lesson_data, adaptations["recommendations"]
        )
        
        # Update lesson in database
        db.collection("lessons").document(lesson_id).update(adapted_content)
        
        # Log adaptation
        adaptation_log = {
            "lesson_id": lesson_id,
            "student_id": student_id,
            "adaptation_type": adaptations["type"],
            "recommendations": adaptations["recommendations"],
            "applied_at": datetime.utcnow()
        }
        
        db.collection("lesson_adaptations").add(adaptation_log)
        
        logger.info(f"Adapted lesson {lesson_id} for student {student_id}")
        
        return {
            "success": True,
            "adapted": True,
            "adaptation_type": adaptations["type"],
            "changes_made": adaptations["recommendations"],
            "message": f"Lesson adapted to {adaptations['type']} difficulty level"
        }
        
    except Exception as e:
        logger.error(f"Failed to adapt lesson difficulty: {str(e)}")
        return {"success": False, "error": str(e)}

# Helper functions

async def _get_learning_step(step_id: str) -> Optional[LearningStep]:
    """Get learning step from database."""
    try:
        # This would get the learning step from the learning paths
        # For now, return a mock step
        return LearningStep(
            step_id=step_id,
            step_number=1,
            title="Sample Step",
            description="Sample learning step",
            subject="Mathematics",
            topic="Addition",
            difficulty_level=DifficultyLevel.EASY,
            learning_objective="understand",
            content_type="practice",
            estimated_duration_minutes=30
        )
    except Exception as e:
        logger.error(f"Failed to get learning step: {str(e)}")
        return None

async def _get_student_context(student_id: str, teacher_uid: str) -> Dict[str, Any]:
    """Get student context for personalization."""
    try:
        # Get student performance history, preferences, etc.
        # For now, return basic context
        return {
            "student_id": student_id,
            "grade_level": 5,
            "learning_style": "visual",
            "performance_level": "average",
            "areas_of_strength": ["basic_math"],
            "areas_needing_support": ["word_problems"]
        }
    except Exception as e:
        logger.error(f"Failed to get student context: {str(e)}")
        return {}

async def _get_relevant_teacher_content(teacher_uid: str, topic: str, subject: str) -> List[str]:
    """Get relevant teacher content using RAG."""
    try:
        # Use enhanced assessment service to search teacher content
        search_results = await enhanced_assessment_service.search_teacher_content(
            teacher_uid=teacher_uid,
            search_query=f"{topic} {subject}",
            subject_filter=subject
        )
        
        return [result.get("content_preview", "") for result in search_results.get("results", [])]
    except Exception as e:
        logger.error(f"Failed to get teacher content: {str(e)}")
        return []

async def _generate_lesson_with_ai(
    learning_step: LearningStep,
    student_context: Dict[str, Any],
    teacher_content: List[str],
    customizations: Optional[Dict[str, Any]]
) -> LessonContent:
    """Generate lesson content using AI."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        # Build context for AI generation
        context = f"""
Learning Step: {learning_step.title}
Topic: {learning_step.topic}
Subject: {learning_step.subject}
Difficulty: {learning_step.difficulty_level}
Objective: {learning_step.learning_objective}
Duration: {learning_step.estimated_duration_minutes} minutes

Student Context: {json.dumps(student_context)}

Teacher Content Available: {json.dumps(teacher_content[:3]) if teacher_content else 'None'}

Customizations: {json.dumps(customizations) if customizations else 'None'}
"""

        prompt = f"""Create a comprehensive lesson with multiple slides for the following learning step:

{context}

Generate a lesson with 5-8 slides that includes:
1. Introduction slide with learning objectives
2. Concept explanation slides with examples
3. Interactive practice slides
4. Assessment/check understanding slides
5. Summary/reflection slide

Each slide should be engaging and appropriate for the student's level. Include interactive elements where beneficial.

Return JSON format:
{{
  "title": "Lesson title",
  "description": "What this lesson covers",
  "learning_objectives": ["objective1", "objective2"],
  "slides": [
    {{
      "slide_number": 1,
      "slide_type": "introduction|concept_explanation|example|practice|assessment|summary",
      "title": "Slide title",
      "subtitle": "Optional subtitle",
      "learning_objective": "What students learn from this slide",
      "estimated_duration_minutes": 5,
      "content_elements": [
        {{
          "element_type": "text|image|interactive_widget|exercise",
          "title": "Element title",
          "content": "Element content",
          "position": 1
        }}
      ],
      "is_interactive": false,
      "completion_criteria": {{}}
    }}
  ]
}}"""

        response = await model.generate_content_async(prompt)
        
        try:
            lesson_data = json.loads(response.text)
        except json.JSONDecodeError:
            # Fallback lesson structure
            lesson_data = {
                "title": learning_step.title,
                "description": learning_step.description,
                "learning_objectives": [f"Learn about {learning_step.topic}"],
                "slides": [{
                    "slide_number": 1,
                    "slide_type": "concept_explanation",
                    "title": learning_step.title,
                    "learning_objective": f"Understand {learning_step.topic}",
                    "estimated_duration_minutes": learning_step.estimated_duration_minutes,
                    "content_elements": [{
                        "element_type": "text",
                        "title": "Content",
                        "content": response.text,
                        "position": 1
                    }],
                    "is_interactive": False
                }]
            }
        
        # Create LessonContent object
        lesson_id = str(uuid.uuid4())
        
        # Convert slides to LessonSlide objects
        slides = []
        for slide_data in lesson_data["slides"]:
            slide_id = str(uuid.uuid4())
            
            # Convert content elements
            content_elements = []
            for elem_data in slide_data.get("content_elements", []):
                element = ContentElement(
                    element_id=str(uuid.uuid4()),
                    element_type=ContentElementType(elem_data.get("element_type", "text")),
                    title=elem_data.get("title"),
                    content=elem_data.get("content"),
                    position=elem_data.get("position", 1)
                )
                content_elements.append(element)
            
            slide = LessonSlide(
                slide_id=slide_id,
                slide_number=slide_data["slide_number"],
                slide_type=SlideType(slide_data.get("slide_type", "concept_explanation")),
                title=slide_data["title"],
                subtitle=slide_data.get("subtitle"),
                content_elements=content_elements,
                learning_objective=slide_data["learning_objective"],
                estimated_duration_minutes=slide_data.get("estimated_duration_minutes", 5),
                is_interactive=slide_data.get("is_interactive", False),
                completion_criteria=slide_data.get("completion_criteria", {})
            )
            slides.append(slide)
        
        lesson_content = LessonContent(
            lesson_id=lesson_id,
            learning_step_id=learning_step.step_id,
            student_id=student_context["student_id"],
            teacher_uid=student_context.get("teacher_uid", ""),
            title=lesson_data["title"],
            description=lesson_data["description"],
            subject=learning_step.subject,
            topic=learning_step.topic,
            grade_level=student_context.get("grade_level", 5),
            slides=slides,
            total_slides=len(slides),
            learning_objectives=lesson_data["learning_objectives"]
        )
        
        return lesson_content
        
    except Exception as e:
        logger.error(f"Failed to generate lesson with AI: {str(e)}")
        raise e

async def _save_lesson_content(lesson_content: LessonContent) -> str:
    """Save lesson content to database."""
    try:
        lesson_ref = db.collection("lessons").document(lesson_content.lesson_id)
        lesson_ref.set(lesson_content.dict())
        return lesson_content.lesson_id
    except Exception as e:
        logger.error(f"Failed to save lesson content: {str(e)}")
        raise e

async def _initialize_lesson_progress(lesson_id: str, student_id: str) -> None:
    """Initialize progress tracking for a lesson."""
    try:
        progress = LessonProgress(
            progress_id=str(uuid.uuid4()),
            lesson_id=lesson_id,
            student_id=student_id,
            slides_total=0,  # Will be updated when lesson is loaded
            slide_progress=[]
        )
        
        db.collection("lesson_progress").document(progress.progress_id).set(progress.dict())
    except Exception as e:
        logger.error(f"Failed to initialize lesson progress: {str(e)}")
        raise e

async def _get_lesson_progress(lesson_id: str, student_id: str) -> Optional[Dict[str, Any]]:
    """Get lesson progress for a student."""
    try:
        progress_docs = db.collection("lesson_progress").where(
            filter=firestore.FieldFilter("lesson_id", "==", lesson_id)
        ).where(
            filter=firestore.FieldFilter("student_id", "==", student_id)
        ).limit(1).get()
        
        if progress_docs:
            return progress_docs[0].to_dict()
        return None
    except Exception as e:
        logger.error(f"Failed to get lesson progress: {str(e)}")
        return None

async def _get_lesson_context_for_chat(lesson_id: str, current_slide_id: Optional[str]) -> Dict[str, Any]:
    """Get lesson context for chatbot responses."""
    try:
        lesson_doc = db.collection("lessons").document(lesson_id).get()
        
        if not lesson_doc.exists:
            return {}
        
        lesson_data = lesson_doc.to_dict()
        
        context = {
            "lesson_title": lesson_data.get("title"),
            "topic": lesson_data.get("topic"),
            "subject": lesson_data.get("subject"),
            "learning_objectives": lesson_data.get("learning_objectives", []),
            "key_concepts": lesson_data.get("key_concepts", [])
        }
        
        # Add current slide context if provided
        if current_slide_id:
            slides = lesson_data.get("slides", [])
            current_slide = next((s for s in slides if s.get("slide_id") == current_slide_id), None)
            if current_slide:
                context["current_slide"] = {
                    "title": current_slide.get("title"),
                    "learning_objective": current_slide.get("learning_objective"),
                    "slide_type": current_slide.get("slide_type")
                }
        
        return context
    except Exception as e:
        logger.error(f"Failed to get lesson context for chat: {str(e)}")
        return {}

async def _generate_lesson_structure_with_ai(
    learning_step: LearningStep,
    student_context: Dict[str, Any],
    customizations: Optional[Dict[str, Any]]
) -> Dict[str, Any]:
    """Generate the high-level lesson structure quickly."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        context = f"""
Learning Step: {learning_step.title}
Topic: {learning_step.topic}
Subject: {learning_step.subject}
Difficulty: {learning_step.difficulty_level}
Duration: {learning_step.estimated_duration_minutes} minutes
Student Level: {student_context.get('grade_level', 5)}
Learning Style: {student_context.get('learning_style', 'mixed')}
"""

        prompt = f"""Create a lesson structure outline for:

{context}

Generate a lean lesson structure with 4-6 slides. Focus on slide types and learning flow.

Return JSON:
{{
  "title": "Lesson title",
  "description": "Brief description",
  "learning_objectives": ["obj1", "obj2"],
  "slides": [
    {{
      "slide_number": 1,
      "slide_type": "introduction|concept_explanation|example|practice|assessment|summary",
      "title": "Slide title",
      "learning_objective": "What students learn",
      "estimated_duration_minutes": 5,
      "is_interactive": false
    }}
  ]
}}"""

        response = await model.generate_content_async(prompt)
        return _parse_ai_response(response.text)
        
    except Exception as e:
        logger.error(f"Failed to generate lesson structure: {str(e)}")
        return {
            "title": learning_step.title,
            "description": learning_step.description,
            "learning_objectives": [f"Learn about {learning_step.topic}"],
            "slides": [{
                "slide_number": 1,
                "slide_type": "concept_explanation",
                "title": learning_step.title,
                "learning_objective": f"Understand {learning_step.topic}",
                "estimated_duration_minutes": learning_step.estimated_duration_minutes,
                "is_interactive": False
            }]
        }

async def _generate_content_outline(
    learning_step: LearningStep,
    teacher_content: List[str]
) -> Dict[str, Any]:
    """Generate content outline and key points."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        prompt = f"""Create a content outline for teaching: {learning_step.topic}

Subject: {learning_step.subject}
Objective: {learning_step.learning_objective}
Teacher Resources: {teacher_content[:2] if teacher_content else 'None'}

Generate key points, examples, and explanations to use in lesson slides.

Return JSON:
{{
  "key_concepts": ["concept1", "concept2"],
  "examples": ["example1", "example2"],
  "explanations": {{"concept1": "explanation"}},
  "practice_ideas": ["idea1", "idea2"],
  "assessment_questions": ["question1", "question2"]
}}"""

        response = await model.generate_content_async(prompt)
        return _parse_ai_response(response.text)
        
    except Exception as e:
        logger.error(f"Failed to generate content outline: {str(e)}")
        return {
            "key_concepts": [learning_step.topic],
            "examples": ["Basic example"],
            "explanations": {learning_step.topic: "Basic explanation"},
            "practice_ideas": ["Practice activity"],
            "assessment_questions": ["Understanding check"]
        }

async def _generate_individual_slide_content(
    slide_outline: Dict[str, Any],
    content_outline: Dict[str, Any],
    student_context: Dict[str, Any],
    slide_number: int
) -> LessonSlide:
    """Generate detailed content for an individual slide."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        slide_type = slide_outline.get("slide_type", "concept_explanation")
        
        prompt = f"""Create detailed content for slide {slide_number}:

Type: {slide_type}
Title: {slide_outline.get('title', 'Untitled')}
Objective: {slide_outline.get('learning_objective', 'Learn')}
Student Level: Grade {student_context.get('grade_level', 5)}

Available Content: {json.dumps(content_outline)}

Generate 2-4 content elements for this slide. Make it engaging and appropriate.

Return JSON:
{{
  "title": "Final slide title",
  "subtitle": "Optional subtitle",
  "content_elements": [
    {{
      "element_type": "text|image|interactive_widget|exercise",
      "title": "Element title",
      "content": "Detailed content",
      "position": 1
    }}
  ],
  "completion_criteria": {{}}
}}"""

        response = await model.generate_content_async(prompt)
        slide_data = _parse_ai_response(response.text)
        
        # Build content elements
        content_elements = []
        for i, elem_data in enumerate(slide_data.get("content_elements", [])):
            element = ContentElement(
                element_id=str(uuid.uuid4()),
                element_type=ContentElementType(elem_data.get("element_type", "text")),
                title=elem_data.get("title", f"Element {i+1}"),
                content=elem_data.get("content", "Content"),
                position=elem_data.get("position", i+1)
            )
            content_elements.append(element)
        
        # Create slide
        slide = LessonSlide(
            slide_id=str(uuid.uuid4()),
            slide_number=slide_number,
            slide_type=SlideType(slide_type),
            title=slide_data.get("title", slide_outline.get("title", "Untitled")),
            subtitle=slide_data.get("subtitle"),
            content_elements=content_elements,
            learning_objective=slide_outline.get("learning_objective", "Learn"),
            estimated_duration_minutes=slide_outline.get("estimated_duration_minutes", 5),
            is_interactive=slide_outline.get("is_interactive", False),
            completion_criteria=slide_data.get("completion_criteria", {})
        )
        
        return slide
        
    except Exception as e:
        logger.error(f"Failed to generate slide {slide_number} content: {str(e)}")
        # Return a basic slide as fallback
        return LessonSlide(
            slide_id=str(uuid.uuid4()),
            slide_number=slide_number,
            slide_type=SlideType("concept_explanation"),
            title=slide_outline.get("title", "Content Slide"),
            content_elements=[ContentElement(
                element_id=str(uuid.uuid4()),
                element_type=ContentElementType("text"),
                title="Content",
                content="Lesson content will be displayed here.",
                position=1
            )],
            learning_objective=slide_outline.get("learning_objective", "Learn"),
            estimated_duration_minutes=5,
            is_interactive=False
        )

async def _build_lesson_content(
    lesson_structure: Dict[str, Any],
    slides: List[LessonSlide],
    learning_step: LearningStep,
    student_context: Dict[str, Any]
) -> LessonContent:
    """Build the final lesson content object."""
    try:
        lesson_id = str(uuid.uuid4())
        
        lesson_content = LessonContent(
            lesson_id=lesson_id,
            learning_step_id=learning_step.step_id,
            student_id=student_context["student_id"],
            teacher_uid=student_context.get("teacher_uid", ""),
            title=lesson_structure.get("title", learning_step.title),
            description=lesson_structure.get("description", learning_step.description),
            subject=learning_step.subject,
            topic=learning_step.topic,
            grade_level=student_context.get("grade_level", 5),
            slides=slides,
            total_slides=len(slides),
            learning_objectives=lesson_structure.get("learning_objectives", [f"Learn about {learning_step.topic}"])
        )
        
        return lesson_content
        
    except Exception as e:
        logger.error(f"Failed to build lesson content: {str(e)}")
        raise e

async def _generate_lesson_ultra_fast(
    learning_step: LearningStep,
    student_context: Dict[str, Any]
) -> LessonContent:
    """Generate lesson content with ultra-fast approach."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        # Extremely concise prompt for speed
        prompt = f"""Create a {learning_step.topic} lesson for grade {student_context.get('grade_level', 5)}. 
Topic: {learning_step.topic}
Subject: {learning_step.subject}

Generate 4 slides: intro, explanation, practice, summary. Be concise.

JSON:
{{
  "title": "{learning_step.topic} Lesson",
  "description": "Learn {learning_step.topic}",
  "learning_objectives": ["Understand {learning_step.topic}", "Apply concepts"],
  "slides": [
    {{"slide_number": 1, "slide_type": "introduction", "title": "Introduction to {learning_step.topic}", "learning_objective": "Get started", "estimated_duration_minutes": 5, "content_elements": [{{"element_type": "text", "title": "Welcome", "content": "Today we learn {learning_step.topic}", "position": 1}}], "is_interactive": false}},
    {{"slide_number": 2, "slide_type": "concept_explanation", "title": "Understanding {learning_step.topic}", "learning_objective": "Learn concepts", "estimated_duration_minutes": 10, "content_elements": [{{"element_type": "text", "title": "Key Ideas", "content": "{learning_step.topic} explained", "position": 1}}], "is_interactive": false}},
    {{"slide_number": 3, "slide_type": "practice", "title": "Practice", "learning_objective": "Apply knowledge", "estimated_duration_minutes": 8, "content_elements": [{{"element_type": "text", "title": "Try This", "content": "Practice problems", "position": 1}}], "is_interactive": true}},
    {{"slide_number": 4, "slide_type": "summary", "title": "Summary", "learning_objective": "Review", "estimated_duration_minutes": 5, "content_elements": [{{"element_type": "text", "title": "Recap", "content": "What we learned", "position": 1}}], "is_interactive": false}}
  ]
}}

Expand content but keep structure exactly as shown."""

        response = await model.generate_content_async(prompt)
        
        # Quick parsing with immediate fallback
        lesson_data = _parse_ai_response(response.text)
        
        # If parsing fails or insufficient data, use fast template
        if not lesson_data.get("slides") or len(lesson_data.get("slides", [])) < 4:
            lesson_data = _get_fast_template_lesson(learning_step)
        
        # Quick conversion to objects
        lesson_id = str(uuid.uuid4())
        slides = []
        
        for slide_data in lesson_data["slides"]:
            content_elements = []
            for elem_data in slide_data.get("content_elements", []):
                element = ContentElement(
                    element_id=str(uuid.uuid4()),
                    element_type=ContentElementType(elem_data.get("element_type", "text")),
                    title=elem_data.get("title", "Content"),
                    content=elem_data.get("content") or "Educational content here",
                    position=elem_data.get("position", 1)
                )
                content_elements.append(element)
            
            slide = LessonSlide(
                slide_id=str(uuid.uuid4()),
                slide_number=slide_data["slide_number"],
                slide_type=SlideType(slide_data.get("slide_type", "concept_explanation")),
                title=slide_data["title"],
                content_elements=content_elements,
                learning_objective=slide_data["learning_objective"],
                estimated_duration_minutes=slide_data.get("estimated_duration_minutes", 5),
                is_interactive=slide_data.get("is_interactive", False)
            )
            slides.append(slide)
        
        lesson_content = LessonContent(
            lesson_id=lesson_id,
            learning_step_id=learning_step.step_id,
            student_id=student_context["student_id"],
            teacher_uid="",
            title=lesson_data.get("title", f"{learning_step.topic} Lesson"),
            description=lesson_data.get("description", f"Learn about {learning_step.topic}"),
            subject=learning_step.subject,
            topic=learning_step.topic,
            grade_level=student_context.get("grade_level", 5),
            slides=slides,
            total_slides=len(slides),
            learning_objectives=lesson_data.get("learning_objectives", [f"Learn {learning_step.topic}"])
        )
        
        return lesson_content
        
    except Exception as e:
        logger.error(f"Failed to generate ultra-fast lesson: {str(e)}")
        # Emergency fallback
        return _create_emergency_lesson(learning_step, student_context)

def _get_fast_template_lesson(learning_step: LearningStep) -> Dict[str, Any]:
    """Get a fast template lesson structure."""
    return {
        "title": f"Quick {learning_step.topic} Lesson",
        "description": f"Essential {learning_step.topic} concepts",
        "learning_objectives": [f"Understand {learning_step.topic}", "Apply basic concepts"],
        "slides": [
            {
                "slide_number": 1,
                "slide_type": "introduction",
                "title": f"Welcome to {learning_step.topic}",
                "learning_objective": "Get oriented",
                "estimated_duration_minutes": 5,
                "content_elements": [{
                    "element_type": "text",
                    "title": "Today's Topic",
                    "content": f"We're learning about {learning_step.topic} today. This is an important {learning_step.subject} concept.",
                    "position": 1
                }],
                "is_interactive": False
            },
            {
                "slide_number": 2,
                "slide_type": "concept_explanation",
                "title": f"What is {learning_step.topic}?",
                "learning_objective": "Understand the basics",
                "estimated_duration_minutes": 10,
                "content_elements": [{
                    "element_type": "text",
                    "title": "Definition",
                    "content": f"{learning_step.topic} is a fundamental concept in {learning_step.subject}. Let's explore what it means and why it's useful.",
                    "position": 1
                }],
                "is_interactive": False
            },
            {
                "slide_number": 3,
                "slide_type": "practice",
                "title": "Try It Out",
                "learning_objective": "Practice what you learned",
                "estimated_duration_minutes": 8,
                "content_elements": [{
                    "element_type": "text",
                    "title": "Practice Time",
                    "content": f"Now let's practice working with {learning_step.topic}. Start with simple examples and build up your confidence.",
                    "position": 1
                }],
                "is_interactive": True
            },
            {
                "slide_number": 4,
                "slide_type": "summary",
                "title": "What We Learned",
                "learning_objective": "Review and reflect",
                "estimated_duration_minutes": 5,
                "content_elements": [{
                    "element_type": "text",
                    "title": "Key Points",
                    "content": f"Great job learning about {learning_step.topic}! Remember the key concepts and keep practicing.",
                    "position": 1
                }],
                "is_interactive": False
            }
        ]
    }

def _create_emergency_lesson(learning_step: LearningStep, student_context: Dict[str, Any]) -> LessonContent:
    """Create emergency lesson when everything else fails."""
    lesson_id = str(uuid.uuid4())
    
    slide = LessonSlide(
        slide_id=str(uuid.uuid4()),
        slide_number=1,
        slide_type=SlideType("concept_explanation"),
        title=f"{learning_step.topic} Overview",
        content_elements=[ContentElement(
            element_id=str(uuid.uuid4()),
            element_type=ContentElementType("text"),
            title="Lesson Content",
            content=f"This lesson covers {learning_step.topic} fundamentals.",
            position=1
        )],
        learning_objective=f"Learn about {learning_step.topic}",
        estimated_duration_minutes=10,
        is_interactive=False
    )
    
    return LessonContent(
        lesson_id=lesson_id,
        learning_step_id=learning_step.step_id,
        student_id=student_context["student_id"],
        teacher_uid="",
        title=f"{learning_step.topic} Lesson",
        description=f"Learn {learning_step.topic}",
        subject=learning_step.subject,
        topic=learning_step.topic,
        grade_level=student_context.get("grade_level", 5),
        slides=[slide],
        total_slides=1,
        learning_objectives=[f"Understand {learning_step.topic}"]
    )

async def _generate_complete_lesson_optimized(
    learning_step: LearningStep,
    student_context: Dict[str, Any],
    teacher_content: List[str],
    customizations: Optional[Dict[str, Any]]
) -> LessonContent:
    """Generate complete lesson content in a single optimized AI call."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        # Build comprehensive context for single AI generation
        context = f"""
Learning Step: {learning_step.title}
Topic: {learning_step.topic}
Subject: {learning_step.subject}
Difficulty: {learning_step.difficulty_level}
Objective: {learning_step.learning_objective}
Duration: {learning_step.estimated_duration_minutes} minutes

Student Context:
- Grade Level: {student_context.get('grade_level', 5)}
- Learning Style: {student_context.get('learning_style', 'visual')}
- Performance Level: {student_context.get('performance_level', 'average')}
- Strengths: {student_context.get('areas_of_strength', [])}
- Support Areas: {student_context.get('areas_needing_support', [])}

Teacher Resources: {teacher_content[:2] if teacher_content else 'None available'}

Customizations: {json.dumps(customizations) if customizations else 'Standard approach'}
"""

        prompt = f"""Create a comprehensive, engaging lesson with detailed content for each slide. This is a single request to generate everything needed.

{context}

Generate a complete lesson with 4-6 slides that includes:
1. Introduction slide with clear learning objectives
2. Concept explanation slides with examples and visual descriptions
3. Interactive practice elements
4. Assessment/understanding checks
5. Summary with key takeaways

Make each slide rich with content, examples, and activities. Include specific interactive elements where beneficial.

Return comprehensive JSON format:
{{
  "title": "Engaging lesson title",
  "description": "What this lesson covers and why it matters",
  "learning_objectives": ["clear objective 1", "clear objective 2", "clear objective 3"],
  "slides": [
    {{
      "slide_number": 1,
      "slide_type": "introduction|concept_explanation|example|practice|assessment|summary",
      "title": "Compelling slide title",
      "subtitle": "Helpful subtitle",
      "learning_objective": "Specific objective for this slide",
      "estimated_duration_minutes": 5,
      "content_elements": [
        {{
          "element_type": "text|image|interactive_widget|exercise",
          "title": "Element title",
          "content": "Rich, detailed content with examples",
          "position": 1,
          "styling": {{"emphasis": true, "visual_cues": ["highlight_key_terms"]}},
          "interactive_widget": {{
            "widget_type": "multiple_choice|drag_drop|fill_blank|matching",
            "title": "Interactive activity title",
            "instructions": "Clear student instructions",
            "content": {{
              "question": "Engaging question",
              "options": ["option1", "option2", "option3", "option4"],
              "correct_answer": 0,
              "explanation": "Why this is correct"
            }},
            "hints": ["helpful hint 1", "helpful hint 2"],
            "points": 2,
            "feedback": {{
              "correct": "Excellent! You understood...",
              "incorrect": "Not quite. Remember that..."
            }}
          }}
        }}
      ],
      "is_interactive": true,
      "completion_criteria": {{
        "required_interactions": 1,
        "minimum_score": 70
      }},
      "visual_elements": [
        {{
          "type": "diagram|illustration|chart",
          "description": "Detailed description of visual element",
          "purpose": "Why this visual helps learning"
        }}
      ]
    }}
  ],
  "assessment_strategy": {{
    "formative_checks": ["slide 2 quiz", "slide 4 activity"],
    "summative_assessment": "final understanding check",
    "success_criteria": "student can demonstrate..."
  }},
  "differentiation": {{
    "for_struggling": ["additional hints", "simplified examples"],
    "for_advanced": ["extension questions", "deeper analysis"],
    "for_different_learning_styles": ["visual aids", "hands-on activities"]
  }}
}}

Make this lesson engaging, comprehensive, and ready to use immediately. Focus on quality over quantity - each element should be well-developed."""

        response = await model.generate_content_async(prompt)
        
        # Parse the comprehensive response
        lesson_data = _parse_ai_response(response.text)
        
        # Ensure we have a valid lesson structure
        if not lesson_data.get("slides") or len(lesson_data.get("slides", [])) == 0:
            # Comprehensive fallback lesson structure that matches legacy output
            lesson_data = {
                "title": f"Complete Guide to {learning_step.topic}",
                "description": f"A comprehensive lesson covering {learning_step.topic} concepts, examples, and practice activities.",
                "learning_objectives": [
                    f"Understand the fundamentals of {learning_step.topic}",
                    f"Apply {learning_step.topic} concepts to solve problems",
                    f"Demonstrate mastery through practice activities"
                ],
                "slides": [
                    {
                        "slide_number": 1,
                        "slide_type": "introduction",
                        "title": f"Welcome to {learning_step.topic}",
                        "subtitle": f"Building strong {learning_step.subject} foundations",
                        "learning_objective": f"Understand the importance and applications of {learning_step.topic}",
                        "estimated_duration_minutes": 5,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "Learning Goals",
                                "content": f"Today we'll explore {learning_step.topic}, a fundamental concept in {learning_step.subject}. By the end of this lesson, you'll be able to understand key concepts, work with examples, and apply your knowledge to solve problems.",
                                "position": 1
                            },
                            {
                                "element_type": "text",
                                "title": "Why This Matters",
                                "content": f"{learning_step.topic} is essential for building strong mathematical reasoning skills and will help you in many areas of study and daily life.",
                                "position": 2
                            }
                        ],
                        "is_interactive": False
                    },
                    {
                        "slide_number": 2,
                        "slide_type": "concept_explanation",
                        "title": f"Understanding {learning_step.topic}",
                        "subtitle": "Core concepts and definitions",
                        "learning_objective": f"Learn the fundamental concepts of {learning_step.topic}",
                        "estimated_duration_minutes": 10,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "What is it?",
                                "content": f"{learning_step.topic} is a mathematical concept that helps us understand relationships and solve problems. Let's break it down into simple, easy-to-understand parts.",
                                "position": 1
                            },
                            {
                                "element_type": "text",
                                "title": "Key Components",
                                "content": f"The main parts of {learning_step.topic} include basic definitions, important properties, and common patterns you'll see repeatedly.",
                                "position": 2
                            }
                        ],
                        "is_interactive": False
                    },
                    {
                        "slide_number": 3,
                        "slide_type": "example",
                        "title": f"{learning_step.topic} in Action",
                        "subtitle": "Real-world examples and step-by-step solutions",
                        "learning_objective": f"See how {learning_step.topic} works through concrete examples",
                        "estimated_duration_minutes": 8,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "Example 1",
                                "content": f"Let's work through a basic {learning_step.topic} problem step by step. We'll start with a simple example and show each step clearly.",
                                "position": 1
                            },
                            {
                                "element_type": "text",
                                "title": "Step-by-Step Process",
                                "content": "Step 1: Identify what we know. Step 2: Determine what we need to find. Step 3: Apply the appropriate method. Step 4: Check our answer.",
                                "position": 2
                            }
                        ],
                        "is_interactive": False
                    },
                    {
                        "slide_number": 4,
                        "slide_type": "practice",
                        "title": "Practice Activities",
                        "subtitle": "Try it yourself!",
                        "learning_objective": f"Practice applying {learning_step.topic} concepts",
                        "estimated_duration_minutes": 12,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "Practice Problems",
                                "content": f"Now it's your turn! Try these {learning_step.topic} problems. Start with the easier ones and work your way up.",
                                "position": 1
                            },
                            {
                                "element_type": "interactive_widget",
                                "title": "Quick Check",
                                "content": "Test your understanding",
                                "position": 2,
                                "interactive_widget": {
                                    "widget_type": "multiple_choice",
                                    "title": f"{learning_step.topic} Understanding Check",
                                    "instructions": "Choose the best answer",
                                    "content": {
                                        "question": f"Which of the following best describes {learning_step.topic}?",
                                        "options": [
                                            "A mathematical tool for problem solving",
                                            "A way to understand relationships",
                                            "An important concept in mathematics",
                                            "All of the above"
                                        ],
                                        "correct_answer": 3,
                                        "explanation": f"{learning_step.topic} encompasses all these aspects - it's a versatile mathematical concept."
                                    },
                                    "hints": ["Think about what makes this concept useful"],
                                    "points": 2,
                                    "feedback": {
                                        "correct": "Excellent! You understand the broad applications of this concept.",
                                        "incorrect": "Not quite. Consider how this concept can be used in different ways."
                                    }
                                }
                            }
                        ],
                        "is_interactive": True
                    },
                    {
                        "slide_number": 5,
                        "slide_type": "assessment",
                        "title": "Check Your Understanding",
                        "subtitle": "Assessment and review",
                        "learning_objective": "Demonstrate understanding and identify areas for review",
                        "estimated_duration_minutes": 10,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "Self-Assessment",
                                "content": f"Let's check how well you understand {learning_step.topic}. Think about what you've learned and be honest about areas where you might need more practice.",
                                "position": 1
                            },
                            {
                                "element_type": "interactive_widget",
                                "title": "Knowledge Check",
                                "content": "Final assessment",
                                "position": 2,
                                "interactive_widget": {
                                    "widget_type": "fill_blank",
                                    "title": f"Complete the {learning_step.topic} Statement",
                                    "instructions": "Fill in the blanks with the correct terms",
                                    "content": {
                                        "text": f"When working with {learning_step.topic}, it's important to remember that ____ helps us understand the relationship, and ____ helps us solve problems effectively.",
                                        "blanks": ["careful analysis", "systematic approaches"],
                                        "options": [
                                            ["careful analysis", "systematic approaches"],
                                            ["quick guessing", "random methods"],
                                            ["memorization", "repetition"]
                                        ]
                                    },
                                    "hints": ["Think about good mathematical practices"],
                                    "points": 3
                                }
                            }
                        ],
                        "is_interactive": True
                    },
                    {
                        "slide_number": 6,
                        "slide_type": "summary",
                        "title": "Lesson Summary",
                        "subtitle": "Key takeaways and next steps",
                        "learning_objective": "Review key concepts and plan next steps",
                        "estimated_duration_minutes": 5,
                        "content_elements": [
                            {
                                "element_type": "text",
                                "title": "What We Learned",
                                "content": f"Congratulations! You've successfully learned about {learning_step.topic}. You now understand the key concepts, have seen examples, and practiced applying your knowledge.",
                                "position": 1
                            },
                            {
                                "element_type": "text",
                                "title": "Key Takeaways",
                                "content": f"Remember: {learning_step.topic} is a powerful tool that becomes easier with practice. The more you work with these concepts, the more confident you'll become.",
                                "position": 2
                            },
                            {
                                "element_type": "text",
                                "title": "Next Steps",
                                "content": "Continue practicing with similar problems, and don't hesitate to review this lesson if you need to refresh your understanding.",
                                "position": 3
                            }
                        ],
                        "is_interactive": False
                    }
                ],
                "assessment_strategy": {
                    "formative_checks": ["slide 4 multiple choice", "slide 5 fill blanks"],
                    "summative_assessment": "overall lesson understanding",
                    "success_criteria": "student demonstrates understanding of key concepts and can apply them"
                },
                "differentiation": {
                    "for_struggling": ["review basic concepts", "additional guided examples"],
                    "for_advanced": ["extension problems", "real-world applications"],
                    "for_different_learning_styles": ["visual examples", "hands-on activities", "step-by-step text"]
                }
            }
        
        # Create LessonContent object
        lesson_id = str(uuid.uuid4())
        
        # Convert slides to LessonSlide objects with enhanced content
        slides = []
        for slide_data in lesson_data["slides"]:
            slide_id = str(uuid.uuid4())
            
            # Convert content elements with interactive widgets
            content_elements = []
            for elem_data in slide_data.get("content_elements", []):
                # Handle interactive widgets
                interactive_widget = None
                if elem_data.get("interactive_widget"):
                    widget_data = elem_data["interactive_widget"]
                    interactive_widget = InteractiveWidget(
                        widget_id=str(uuid.uuid4()),
                        widget_type=widget_data.get("widget_type", "multiple_choice"),
                        title=widget_data.get("title", "Interactive Activity"),
                        instructions=widget_data.get("instructions", "Complete this activity"),
                        content=widget_data.get("content", {}),
                        correct_answer=widget_data.get("correct_answer"),
                        hints=widget_data.get("hints", []),
                        points=widget_data.get("points", 1),
                        feedback=widget_data.get("feedback", {})
                    )
                
                element = ContentElement(
                    element_id=str(uuid.uuid4()),
                    element_type=ContentElementType(elem_data.get("element_type", "text")),
                    title=elem_data.get("title", "Content"),
                    content=elem_data.get("content") or "Content will be displayed here",  # Ensure content is never None
                    position=elem_data.get("position", 1),
                    styling=elem_data.get("styling", {}),
                    interactive_widget=interactive_widget
                )
                content_elements.append(element)
            
            slide = LessonSlide(
                slide_id=slide_id,
                slide_number=slide_data["slide_number"],
                slide_type=SlideType(slide_data.get("slide_type", "concept_explanation")),
                title=slide_data["title"],
                subtitle=slide_data.get("subtitle"),
                content_elements=content_elements,
                learning_objective=slide_data["learning_objective"],
                estimated_duration_minutes=slide_data.get("estimated_duration_minutes", 5),
                is_interactive=slide_data.get("is_interactive", False),
                completion_criteria=slide_data.get("completion_criteria", {}),
                visual_elements=slide_data.get("visual_elements", [])
            )
            slides.append(slide)
        
        lesson_content = LessonContent(
            lesson_id=lesson_id,
            learning_step_id=learning_step.step_id,
            student_id=student_context["student_id"],
            teacher_uid=student_context.get("teacher_uid", ""),
            title=lesson_data["title"],
            description=lesson_data["description"],
            subject=learning_step.subject,
            topic=learning_step.topic,
            grade_level=student_context.get("grade_level", 5),
            slides=slides,
            total_slides=len(slides),
            learning_objectives=lesson_data["learning_objectives"],
            assessment_strategy=lesson_data.get("assessment_strategy", {}),
            differentiation=lesson_data.get("differentiation", {})
        )
        
        return lesson_content
        
    except Exception as e:
        logger.error(f"Failed to generate optimized lesson: {str(e)}")
        raise e

def _parse_ai_response(response_text: str) -> Dict[str, Any]:
    """
    Parse AI response with multiple fallback mechanisms.
    
    Args:
        response_text: Raw response text from AI model
        
    Returns:
        Parsed response data dictionary
    """
    try:
        # Method 1: Direct JSON parsing
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            pass
        
        # Method 2: Extract JSON from code blocks
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Method 3: Extract JSON from text (find first complete JSON object)
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        # Method 4: Extract message from quotes if JSON parsing fails
        message_match = re.search(r'"message":\s*"([^"]*)"', response_text)
        if message_match:
            return {
                "message": message_match.group(1),
                "confidence_score": 0.7,
                "sources": ["lesson_content"],
                "suggested_actions": []
            }
        
        # Method 5: Use entire text as message (final fallback)
        # Clean the text by removing JSON formatting artifacts
        clean_text = response_text.replace('```json', '').replace('```', '').strip()
        
        # If it looks like it starts with JSON structure, extract just the message content
        if clean_text.startswith('{'):
            # Try to extract just the message content from malformed JSON
            lines = clean_text.split('\n')
            message_lines = []
            in_message = False
            
            for line in lines:
                if '"message":' in line:
                    in_message = True
                    # Extract the message start
                    message_start = line.split('"message":', 1)[1].strip()
                    if message_start.startswith('"'):
                        message_start = message_start[1:]  # Remove opening quote
                    message_lines.append(message_start)
                elif in_message:
                    if line.strip().endswith('",') or line.strip().endswith('"'):
                        # End of message
                        line_text = line.replace('",', '').replace('"', '').strip()
                        if line_text:
                            message_lines.append(line_text)
                        break
                    else:
                        message_lines.append(line)
            
            if message_lines:
                message_text = '\n'.join(message_lines).strip()
                return {
                    "message": message_text,
                    "confidence_score": 0.6,
                    "sources": ["lesson_content"],
                    "suggested_actions": []
                }
        
        # Final fallback - use cleaned text
        return {
            "message": clean_text,
            "confidence_score": 0.5,
            "sources": ["lesson_content"],
            "suggested_actions": []
        }
        
    except Exception as e:
        logger.error(f"Failed to parse AI response: {str(e)}")
        return {
            "message": "I'm here to help with your lesson questions. Could you please rephrase your question?",
            "confidence_score": 0.3,
            "sources": [],
            "suggested_actions": ["Try asking a more specific question about the lesson topic"]
        }

async def _generate_chat_response(
    student_message: str,
    lesson_context: Dict[str, Any],
    chat_history: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Generate AI response for chatbot."""
    try:
        model = get_vertex_model("gemini-2.5-pro")
        
        # Serialize chat history for JSON
        serialized_history = []
        recent_history = chat_history[-5:] if len(chat_history) > 5 else chat_history
        for msg in recent_history:
            serialized_msg = serialize_for_firestore(msg)
            serialized_history.append({
                "sender": serialized_msg.get("sender"),
                "message": serialized_msg.get("message"),
                "timestamp": serialized_msg.get("timestamp")
            })

        # Build context for AI
        context = f"""
Lesson Context:
- Title: {lesson_context.get('lesson_title', 'Unknown')}
- Topic: {lesson_context.get('topic', 'Unknown')}
- Subject: {lesson_context.get('subject', 'Unknown')}
- Learning Objectives: {lesson_context.get('learning_objectives', [])}

Current Slide: {lesson_context.get('current_slide', {}).get('title', 'Unknown')}

Recent Chat History:
{json.dumps(serialized_history)}

Student Message: {student_message}
"""

        prompt = f"""You are a helpful AI tutor assisting a student with their lesson. Based on the context and the student's question, provide a helpful, encouraging, and educational response.

{context}

Guidelines:
- Be encouraging and supportive
- Provide clear explanations appropriate for the grade level
- Use examples and analogies when helpful
- Ask follow-up questions to check understanding
- Don't give direct answers to assessment questions, but provide guidance
- If the question is off-topic, gently redirect to the lesson content

Respond in JSON format:
{{
  "message": "Your helpful response to the student",
  "confidence_score": 0.95,
  "sources": ["lesson_content"],
  "suggested_actions": ["Optional suggestions for next steps"]
}}"""

        response = await model.generate_content_async(prompt)
        
        # Enhanced JSON parsing with multiple fallback mechanisms
        response_data = _parse_ai_response(response.text)
        
        return response_data
        
    except Exception as e:
        logger.error(f"Failed to generate chat response: {str(e)}")
        return {
            "message": "I'm sorry, I'm having trouble understanding your question right now. Could you try asking in a different way?",
            "confidence_score": 0.5,
            "sources": [],
            "suggested_actions": []
        }

async def _analyze_performance_for_adaptation(
    performance_data: Dict[str, Any],
    progress_data: Optional[Dict[str, Any]],
    lesson_data: Dict[str, Any]
) -> Dict[str, Any]:
    """Analyze performance to determine if lesson adaptation is needed."""
    try:
        # Simple adaptation logic - can be enhanced with ML models
        
        # Check completion rate
        completion_rate = progress_data.get("completion_percentage", 0) if progress_data else 0
        
        # Check time spent vs expected
        time_spent = progress_data.get("time_spent_minutes", 0) if progress_data else 0
        expected_time = sum(slide.get("estimated_duration_minutes", 5) for slide in lesson_data.get("slides", []))
        
        # Check interaction success rate
        correct_responses = progress_data.get("correct_responses", 0) if progress_data else 0
        total_responses = progress_data.get("total_responses", 1) if progress_data else 1
        success_rate = correct_responses / total_responses if total_responses > 0 else 0
        
        needs_adaptation = False
        adaptation_type = "maintain"
        recommendations = []
        
        # Determine if adaptation is needed
        if success_rate < 0.5 and completion_rate < 50:
            needs_adaptation = True
            adaptation_type = "simplify"
            recommendations = [
                "Reduce content complexity",
                "Add more examples",
                "Break down concepts into smaller steps",
                "Add more hints and guidance"
            ]
        elif success_rate > 0.9 and time_spent < expected_time * 0.7:
            needs_adaptation = True
            adaptation_type = "enhance"
            recommendations = [
                "Add more challenging content",
                "Include advanced examples",
                "Add extension activities",
                "Reduce repetitive content"
            ]
        
        return {
            "needs_adaptation": needs_adaptation,
            "type": adaptation_type,
            "recommendations": recommendations,
            "analysis": {
                "completion_rate": completion_rate,
                "success_rate": success_rate,
                "time_efficiency": time_spent / expected_time if expected_time > 0 else 1
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to analyze performance for adaptation: {str(e)}")
        return {"needs_adaptation": False, "type": "maintain", "recommendations": []}

async def _apply_lesson_adaptations(
    lesson_data: Dict[str, Any],
    recommendations: List[str]
) -> Dict[str, Any]:
    """Apply adaptations to lesson content."""
    try:
        # This would modify the lesson content based on recommendations
        # For now, just add an adaptation note
        
        adapted_data = lesson_data.copy()
        adapted_data["adaptation_history"] = adapted_data.get("adaptation_history", [])
        adapted_data["adaptation_history"].append({
            "applied_at": datetime.utcnow(),
            "recommendations": recommendations,
            "type": "automatic_adaptation"
        })
        
        return adapted_data
        
    except Exception as e:
        logger.error(f"Failed to apply lesson adaptations: {str(e)}")
        return lesson_data


================================================================================
File: app/agents/tools/onboarding_tools.py
Size: 6.74 kB
================================================================================

# FILE: app/agents/tools/onboarding_tools.py

from __future__ import annotations
from app.core.firebase import db
from typing import Dict, Any, List
import logging
from contextvars import ContextVar
from datetime import datetime

logger = logging.getLogger(__name__)

# Context variable to store the current user's UID
current_user_uid: ContextVar[str] = ContextVar('current_user_uid')

def create_teacher_profile(name: str, email: str, subjects: List[str]) -> str:
    """
    Creates a new teacher profile in Firestore with onboarding tracking.
    The user UID is automatically obtained from the authenticated session context.

    Args:
        name: Teacher's full name
        email: Teacher's email address
        subjects: Initial list of subjects the teacher handles

    Returns:
        A string confirming the successful profile creation.
    """
    try:
        # Get the user UID from context
        user_uid = current_user_uid.get()
        if not user_uid:
            return "Error: User authentication required to create profile."
        
        logger.info(f"Creating teacher profile for user {user_uid}")
        
        profile_data = {
            "name": name,
            "email": email,
            "subjects": subjects,
            "role": "teacher",
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
            "onboarding": {
                "status": "in_progress",
                "steps_completed": ["profile_created"],
                "current_step": "subjects_setup",
                "started_at": datetime.utcnow()
            }
        }
        
        user_ref = db.collection("users").document(user_uid)
        user_ref.set(profile_data)
        
        return f"Successfully created teacher profile for {name}. Welcome to Edvance! Next step: Set up your subjects."
        
    except LookupError:
        return "Error: User authentication context not found."
    except Exception as e:
        logger.error(f"Failed to create teacher profile: {e}")
        return f"An error occurred while creating your profile: {e}"

def get_onboarding_status() -> str:
    """
    Retrieves the current onboarding status for the authenticated teacher.
    
    Returns:
        A string with the onboarding status and next steps.
    """
    try:
        # Get the user UID from context
        user_uid = current_user_uid.get()
        if not user_uid:
            return "Error: User authentication required to check onboarding status."
        
        logger.info(f"Getting onboarding status for user {user_uid}")
        user_ref = db.collection("users").document(user_uid)
        user_doc = user_ref.get()
        
        if not user_doc.exists:
            return "No profile found. Let's start by creating your teacher profile!"
        
        user_data = user_doc.to_dict()
        onboarding = user_data.get("onboarding", {})
        
        status = onboarding.get("status", "not_started")
        steps_completed = onboarding.get("steps_completed", [])
        current_step = onboarding.get("current_step", "profile_creation")
        
        if status == "completed":
            return "🎉 Your onboarding is complete! You're all set to use Edvance."
        
        completed_count = len(steps_completed)
        total_steps = 3  # profile_created, subjects_setup, onboarding_complete
        
        progress_msg = f"Onboarding Progress: {completed_count}/{total_steps} steps completed.\n"
        progress_msg += f"Current step: {current_step}\n"
        progress_msg += f"Completed steps: {', '.join(steps_completed) if steps_completed else 'None'}"
        
        return progress_msg
        
    except LookupError:
        return "Error: User authentication context not found."
    except Exception as e:
        logger.error(f"Failed to get onboarding status: {e}")
        return f"An error occurred while checking onboarding status: {e}"

def complete_onboarding_step(step_name: str) -> str:
    """
    Marks an onboarding step as completed and updates the current step.
    
    Args:
        step_name: The name of the step to mark as completed
        
    Returns:
        A string confirming the step completion and next actions.
    """
    try:
        # Get the user UID from context
        user_uid = current_user_uid.get()
        if not user_uid:
            return "Error: User authentication required to complete onboarding step."
        
        logger.info(f"Completing onboarding step '{step_name}' for user {user_uid}")
        user_ref = db.collection("users").document(user_uid)
        user_doc = user_ref.get()
        
        if not user_doc.exists:
            return "Error: User profile not found. Please create your profile first."
        
        user_data = user_doc.to_dict()
        onboarding = user_data.get("onboarding", {})
        steps_completed = onboarding.get("steps_completed", [])
        
        # Add step if not already completed
        if step_name not in steps_completed:
            steps_completed.append(step_name)
        
        # Determine next step
        next_step_map = {
            "profile_created": "subjects_setup",
            "subjects_setup": "onboarding_complete",
            "onboarding_complete": "completed"
        }
        
        next_step = next_step_map.get(step_name, "completed")
        status = "completed" if next_step == "completed" else "in_progress"
        
        # Update onboarding data
        updated_onboarding = {
            "status": status,
            "steps_completed": steps_completed,
            "current_step": next_step,
            "updated_at": datetime.utcnow()
        }
        
        if "started_at" not in onboarding:
            updated_onboarding["started_at"] = datetime.utcnow()
        else:
            updated_onboarding["started_at"] = onboarding["started_at"]
            
        if status == "completed":
            updated_onboarding["completed_at"] = datetime.utcnow()
        
        # Update the document
        user_ref.update({
            "onboarding": updated_onboarding,
            "updated_at": datetime.utcnow()
        })
        
        if status == "completed":
            return f"🎉 Congratulations! You've completed the '{step_name}' step. Your onboarding is now complete! Welcome to Edvance!"
        else:
            return f"✅ Step '{step_name}' completed successfully! Next step: {next_step}"
        
    except LookupError:
        return "Error: User authentication context not found."
    except Exception as e:
        logger.error(f"Failed to complete onboarding step: {e}")
        return f"An error occurred while completing the onboarding step: {e}"


================================================================================
File: app/agents/tools/profile_tools.py
Size: 3.67 kB
================================================================================

# FILE: app/agents/tools/profile_tools.py

from __future__ import annotations
from app.core.firebase import db
from typing import List
import logging
from contextvars import ContextVar

logger = logging.getLogger(__name__)

# Context variable to store the current user's UID
current_user_uid: ContextVar[str] = ContextVar('current_user_uid')

def get_teacher_subjects() -> str:
    """
    Retrieves the current list of subjects for the authenticated teacher from Firestore.
    The user UID is automatically obtained from the authenticated session context.

    Returns:
        A string with the current subjects or an error message.
    """
    try:
        # Get the user UID from context
        user_uid = current_user_uid.get()
        if not user_uid:
            return "Error: User authentication required to get subjects."
        
        logger.info(f"Getting subjects for user {user_uid}")
        user_ref = db.collection("users").document(user_uid)
        user_doc = user_ref.get()
        
        if user_doc.exists:
            user_data = user_doc.to_dict()
            subjects = user_data.get("subjects", [])
            if subjects:
                return f"Your current subjects are: {', '.join(subjects)}"
            else:
                return "You don't have any subjects set yet."
        else:
            return "User profile not found. Please create your profile first."
            
    except LookupError:
        return "Error: User authentication context not found."
    except Exception as e:
        logger.error(f"Failed to get subjects for user: {e}")
        return f"An error occurred while retrieving subjects: {e}"

def update_teacher_subjects(subjects: List[str]) -> str:
    """
    Updates the list of subjects for the authenticated teacher in the Firestore database.
    The user UID is automatically obtained from the authenticated session context.

    Args:
        subjects: The new list of subjects for the teacher.

    Returns:
        A string confirming the successful update.
    """
    try:
        # Get the user UID from context
        user_uid = current_user_uid.get()
        if not user_uid:
            return "Error: User authentication required to update subjects."
        
        logger.info(f"Updating subjects for user {user_uid} to {subjects}")
        user_ref = db.collection("users").document(user_uid)
        user_ref.update({"subjects": subjects})
        return f"Successfully updated subjects to: {', '.join(subjects)}."
    except LookupError:
        return "Error: User authentication context not found."
    except Exception as e:
        logger.error(f"Failed to update subjects for user: {e}")
        return f"An error occurred: {e}"

# Keep the original function for backwards compatibility if needed
def update_teacher_subjects_with_uid(user_uid: str, subjects: List[str]) -> str:
    """
    Updates the list of subjects for a given teacher in the Firestore database.

    Args:
        user_uid: The unique ID of the teacher to update.
        subjects: The new list of subjects for the teacher.

    Returns:
        A string confirming the successful update.
    """
    if not user_uid:
        return "Error: User ID is required to update subjects."
    
    try:
        logger.info(f"Updating subjects for user {user_uid} to {subjects}")
        user_ref = db.collection("users").document(user_uid)
        user_ref.update({"subjects": subjects})
        return f"Successfully updated subjects to: {', '.join(subjects)}."
    except Exception as e:
        logger.error(f"Failed to update subjects for user {user_uid}: {e}")
        return f"An error occurred: {e}"

================================================================================
File: app/agents/vertex_question_agent.py
Size: 12.57 kB
================================================================================

# FILE: app/agents/vertex_question_agent.py

import logging
from typing import List, Dict, Any, Optional
import json
import asyncio
import uuid

from vertexai.generative_models import GenerativeModel
from app.core.vertex import get_vertex_model
from app.models.rag_models import (
    RAGResult, 
    QuestionGenerationRequest, 
    EnhancedAssessmentQuestion,
    GeneratedQuestionContext
)

logger = logging.getLogger(__name__)

class VertexQuestionAgent:
    """AI Agent for generating assessment questions using Vertex AI Gemini."""
    
    def __init__(self):
        self.model_name = "gemini-2.5-pro"
        self.model = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the Vertex AI Gemini model."""
        try:
            self.model = get_vertex_model(self.model_name)
            logger.info(f"Initialized Vertex AI question agent with {self.model_name}")
            
        except Exception as e:
            logger.error(f"Failed to initialize Vertex AI model: {str(e)}")
            self.model = None
    
    async def generate_questions(
        self,
        request: QuestionGenerationRequest
    ) -> List[EnhancedAssessmentQuestion]:
        """Generate assessment questions from RAG context using Gemini."""
        
        if not self.model:
            logger.error("Vertex AI model not initialized, cannot generate questions")
            return []
        
        try:
            logger.info(f"Generating {request.question_count} questions for {request.subject} grade {request.grade_level}")
            
            # Prepare context from RAG results
            context_text = self._prepare_context(request.context_chunks)
            
            # Generate questions in batches
            all_questions = []
            batch_size = min(3, request.question_count)  # Generate 3 at a time max
            
            for i in range(0, request.question_count, batch_size):
                remaining = min(batch_size, request.question_count - i)
                
                batch_questions = await self._generate_question_batch(
                    context_text=context_text,
                    subject=request.subject,
                    grade_level=request.grade_level,
                    topic=request.topic,
                    difficulty=request.difficulty_level,
                    count=remaining,
                    context_chunks=request.context_chunks
                )
                
                all_questions.extend(batch_questions)
                
                # Small delay between batches
                if i + batch_size < request.question_count:
                    await asyncio.sleep(1)
            
            logger.info(f"Generated {len(all_questions)} questions successfully")
            return all_questions[:request.question_count]  # Ensure exact count
            
        except Exception as e:
            logger.error(f"Question generation failed: {str(e)}")
            return []
    
    def _prepare_context(self, context_chunks: List[RAGResult]) -> str:
        """Prepare context text from RAG results."""
        
        if not context_chunks:
            return "No specific context available."
        
        context_parts = []
        for i, result in enumerate(context_chunks, 1):
            chunk_text = result.chunk.content.strip()
            source = result.document_metadata.get("filename", "Unknown source")
            similarity = result.similarity_score
            
            context_part = f"""
Context {i} (from {source}, similarity: {similarity:.2f}):
{chunk_text}
            """.strip()
            context_parts.append(context_part)
        
        return "\\n\\n".join(context_parts)
    
    async def _generate_question_batch(
        self,
        context_text: str,
        subject: str,
        grade_level: int,
        topic: str,
        difficulty: str,
        count: int,
        context_chunks: List[RAGResult]
    ) -> List[EnhancedAssessmentQuestion]:
        """Generate a batch of questions using Gemini."""
        
        try:
            # Create the prompt
            prompt = self._create_question_prompt(
                context_text, subject, grade_level, topic, difficulty, count
            )
            
            # Generate response using Vertex AI Gemini
            response = self.model.generate_content(prompt)
            response_text = response.text
            
            # Parse the response
            questions = self._parse_gemini_response(
                response_text, context_chunks, difficulty, topic
            )
            
            return questions
            
        except Exception as e:
            logger.error(f"Batch question generation failed: {str(e)}")
            return []
    
    def _create_question_prompt(
        self,
        context_text: str,
        subject: str,
        grade_level: int,
        topic: str,
        difficulty: str,
        count: int
    ) -> str:
        """Create the prompt for Gemini question generation."""
        
        difficulty_guidance = {
            "easy": "Create simple, direct questions that test basic recall and understanding. Use simple vocabulary appropriate for the grade level.",
            "medium": "Create questions that require some analysis or application of concepts. Include one-step problem solving.",
            "hard": "Create challenging questions that require analysis, synthesis, or multi-step reasoning. Test deeper understanding."
        }
        
        bloom_levels = {
            "easy": "Remember, Understand",
            "medium": "Apply, Analyze", 
            "hard": "Evaluate, Create"
        }
        
        # Check if we have meaningful context
        has_context = context_text and context_text != "No specific context available."
        
        if has_context:
            context_instruction = f"""CONTEXT MATERIAL (USE THIS AS YOUR SOURCE):
{context_text}

CRITICAL REQUIREMENTS:
1. Questions MUST be based on the provided context material
2. Reference specific concepts, examples, or information from the context
3. Use age-appropriate vocabulary for grade {grade_level}
4. Ensure one clearly correct answer
5. Make distractors plausible but clearly wrong based on the context
6. Include educational explanations that reference the source material"""
        else:
            context_instruction = f"""CONTENT GENERATION:
Since no specific context material is provided, generate questions based on standard curriculum content for {subject} at grade {grade_level} level, focusing on the topic: {topic}.

CRITICAL REQUIREMENTS:
1. Questions should align with standard curriculum for {subject} grade {grade_level}
2. Focus specifically on the topic: {topic}
3. Use age-appropriate vocabulary for grade {grade_level}
4. Ensure one clearly correct answer
5. Make distractors plausible but clearly wrong
6. Include educational explanations that help students learn"""
        
        prompt = f"""You are an expert educational assessment designer. Create {count} high-quality multiple-choice questions.

REQUIREMENTS:
- Subject: {subject}
- Grade Level: {grade_level}
- Topic: {topic}
- Difficulty: {difficulty}
- Question Type: Multiple choice with 4 options

DIFFICULTY GUIDANCE:
{difficulty_guidance.get(difficulty, difficulty_guidance["medium"])}

BLOOM'S TAXONOMY TARGET:
{bloom_levels.get(difficulty, bloom_levels["medium"])}

{context_instruction}

OUTPUT FORMAT:
Return EXACTLY {count} questions in this JSON format:

```json
[
  {{
    "question_text": "Educational question appropriate for the specified grade and topic",
    "options": ["Option A", "Option B", "Option C", "Option D"],
    "correct_answer": 0,
    "explanation": "Clear explanation that helps students understand the concept",
    "bloom_level": "Remember/Understand/Apply/Analyze/Evaluate/Create",
    "learning_objectives": ["What students learn from this question"],
    "context_reference": "Source reference or curriculum standard if applicable"
  }}
]
```

Generate {count} high-quality educational question(s) now:"""

        return prompt
    
    def _parse_gemini_response(
        self,
        response_text: str,
        context_chunks: List[RAGResult],
        difficulty: str,
        topic: str
    ) -> List[EnhancedAssessmentQuestion]:
        """Parse the Gemini response into question objects."""
        
        questions = []
        
        try:
            # Extract JSON from response
            json_start = response_text.find('[')
            json_end = response_text.rfind(']') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No JSON array found in response")
            
            json_str = response_text[json_start:json_end]
            parsed_questions = json.loads(json_str)
            
            # Convert to EnhancedAssessmentQuestion objects
            for i, q_data in enumerate(parsed_questions):
                try:
                    question = EnhancedAssessmentQuestion(
                        question_id=f"vertex_{uuid.uuid4()}_{i}",
                        question_text=q_data["question_text"],
                        options=q_data["options"],
                        correct_answer=q_data["correct_answer"],
                        explanation=q_data["explanation"],
                        difficulty=difficulty,
                        topic=topic,
                        context=GeneratedQuestionContext(
                            source_chunks=[chunk.chunk.chunk_id for chunk in context_chunks],
                            confidence_score=0.90,  # High confidence for Vertex AI
                            generation_metadata={
                                "model": self.model_name,
                                "method": "vertex_ai_rag",
                                "context_sources": len(context_chunks),
                                "context_reference": q_data.get("context_reference", "")
                            }
                        ),
                        bloom_taxonomy_level=q_data.get("bloom_level", ""),
                        learning_objectives=q_data.get("learning_objectives", [])
                    )
                    questions.append(question)
                    
                except Exception as e:
                    logger.warning(f"Failed to parse question {i}: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"Failed to parse Gemini response: {str(e)}")
            logger.debug(f"Response text: {response_text[:500]}...")
        
        return questions
    
    async def validate_question_quality(
        self,
        question: EnhancedAssessmentQuestion,
        context_chunks: List[RAGResult]
    ) -> Dict[str, Any]:
        """Validate the quality of a generated question."""
        
        quality_score = 0.0
        issues = []
        
        # Check if question references context
        question_lower = question.question_text.lower()
        context_match = False
        
        for chunk in context_chunks:
            chunk_words = set(chunk.chunk.content.lower().split())
            question_words = set(question_lower.split())
            
            # Check for word overlap
            overlap = len(chunk_words.intersection(question_words))
            if overlap >= 3:  # At least 3 words in common
                context_match = True
                break
        
        if context_match:
            quality_score += 0.4
        else:
            issues.append("Question may not be sufficiently based on provided context")
        
        # Check answer options uniqueness
        if len(set(question.options)) == len(question.options):
            quality_score += 0.2
        else:
            issues.append("Duplicate answer options found")
        
        # Check correct answer index
        if 0 <= question.correct_answer < len(question.options):
            quality_score += 0.2
        else:
            issues.append("Invalid correct answer index")
        
        # Check explanation quality
        if len(question.explanation) > 20:
            quality_score += 0.2
        else:
            issues.append("Explanation too short")
        
        return {
            "quality_score": quality_score,
            "issues": issues,
            "is_acceptable": quality_score >= 0.8 and len(issues) <= 1,
            "context_based": context_match
        }


================================================================================
File: app/api/__init__.py
Size: 0 B
================================================================================



================================================================================
File: app/api/v1/__init__.py
Size: 0 B
================================================================================



================================================================================
File: app/api/v1/agent.py
Size: 2.26 kB
================================================================================

# FILE: app/api/v1/agent.py

from fastapi import APIRouter, Depends, HTTPException, status
from typing import Dict, Any
import logging

from app.models.requests import AgentPrompt, AgentResponse
from app.core.auth import get_current_user
from app.services.agent_service import agent_service

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/invoke", response_model=AgentResponse, tags=["Agent Interaction"])
async def invoke_agent(
    request: AgentPrompt,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> AgentResponse:
    """
    Invoke the comprehensive teacher agent with a user's prompt.
    This intelligent agent handles:
    - New user onboarding and profile creation
    - Existing user subject management and updates  
    - General teaching assistance and guidance
    
    The agent automatically detects if you're a new or existing user and provides
    appropriate assistance.
    
    Args:
        request: The agent prompt request
        current_user: The authenticated user information
        
    Returns:
        The agent's response with appropriate guidance
        
    Raises:
        HTTPException: If agent invocation fails
    """
    user_uid = current_user["uid"]
    
    try:
        logger.info(f"Invoking agent for user {user_uid} with prompt: {request.prompt[:50]}...")
        
        response_text = await agent_service.invoke_agent(
            user_uid=user_uid,
            prompt=request.prompt
        )
        
        return AgentResponse(
            response=response_text,
            session_id=f"session_{user_uid}"
        )
        
    except Exception as e:
        logger.error(f"Agent invocation failed for user {user_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred while invoking the agent: {str(e)}"
        )

@router.get("/health", response_model=Dict[str, str], tags=["Agent Interaction"])
async def agent_health() -> Dict[str, str]:
    """
    Check the health status of the agent service.
    
    Returns:
        Health status information
    """
    return {
        "status": "healthy",
        "message": "Agent service is operational"
    }


================================================================================
File: app/api/v1/assess.py
Size: 0 B
================================================================================



================================================================================
File: app/api/v1/assessments.py
Size: 13.46 kB
================================================================================

# FILE: app/api/v1/assessments.py

from fastapi import APIRouter, Depends, HTTPException, status, Query, Body, Path
from typing import Dict, Any, List, Optional
import logging

from app.models.student import AssessmentConfig, Assessment
from app.core.auth import get_current_user
from app.services.assessment_service import assessment_service

logger = logging.getLogger(__name__)

router = APIRouter()

# ====================================================================
# Assessment Configuration Endpoints
# ====================================================================

@router.post("/configs", response_model=AssessmentConfig, tags=["Assessment Configuration"])
async def create_assessment_config(
    name: str = Body(..., description="Name for the assessment configuration"),
    subject: str = Body(..., description="Subject for the assessment"),
    target_grade: int = Body(..., ge=1, le=12, description="Target grade level"),
    difficulty_level: str = Body(..., pattern="^(easy|medium|hard)$", description="Difficulty level"),
    topic: str = Body(..., description="Specific topic to assess"),
    question_count: int = Body(10, ge=5, le=20, description="Number of questions"),
    time_limit_minutes: int = Body(30, ge=10, le=120, description="Time limit in minutes"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> AssessmentConfig:
    """
    Create a new assessment configuration.
    
    This configuration will be used to generate assessments with specific parameters.
    Teachers can create multiple configurations for different topics and reuse them.
    
    Args:
        name: Descriptive name for the configuration
        subject: Subject category (should match teacher's subjects)
        target_grade: Grade level for the assessment (1-12)
        difficulty_level: Difficulty (easy, medium, hard)
        topic: Specific topic to assess
        question_count: Number of MCQ questions (5-20)
        time_limit_minutes: Time limit for students (10-120 minutes)
        current_user: Authenticated teacher
        
    Returns:
        AssessmentConfig object with generated ID
        
    Raises:
        HTTPException: If creation fails
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Creating assessment config for teacher {teacher_uid}: {subject} - {topic}")
        
        config = await assessment_service.create_assessment_config(
            teacher_uid=teacher_uid,
            name=name,
            subject=subject,
            target_grade=target_grade,
            difficulty_level=difficulty_level,
            topic=topic,
            question_count=question_count,
            time_limit_minutes=time_limit_minutes
        )
        
        logger.info(f"Created assessment config {config.config_id}")
        return config
        
    except Exception as e:
        logger.error(f"Assessment config creation failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create assessment configuration: {str(e)}"
        )

@router.get("/configs", response_model=List[AssessmentConfig], tags=["Assessment Configuration"])
async def get_my_assessment_configs(
    subject: Optional[str] = Query(None, description="Filter by subject"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[AssessmentConfig]:
    """
    Get all assessment configurations created by the current teacher.
    
    Args:
        subject: Optional filter by subject
        current_user: Authenticated teacher
        
    Returns:
        List of AssessmentConfig objects
    """
    teacher_uid = current_user["uid"]
    
    try:
        configs = await assessment_service.get_teacher_assessment_configs(
            teacher_uid=teacher_uid,
            subject_filter=subject
        )
        
        logger.info(f"Retrieved {len(configs)} assessment configs for teacher {teacher_uid}")
        return configs
        
    except Exception as e:
        logger.error(f"Failed to retrieve assessment configs: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment configurations: {str(e)}"
        )

@router.get("/configs/{config_id}", response_model=AssessmentConfig, tags=["Assessment Configuration"])
async def get_assessment_config(
    config_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> AssessmentConfig:
    """Get a specific assessment configuration."""
    teacher_uid = current_user["uid"]
    
    try:
        config = await assessment_service.get_assessment_config(config_id)
        
        # Verify ownership
        if config.teacher_uid != teacher_uid:
            raise HTTPException(status_code=403, detail="Access denied")
        
        return config
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get assessment config: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment configuration: {str(e)}"
        )

@router.put("/configs/{config_id}", response_model=AssessmentConfig, tags=["Assessment Configuration"])
async def update_assessment_config(
    config_id: str,
    name: Optional[str] = Body(None),
    subject: Optional[str] = Body(None),
    target_grade: Optional[int] = Body(None, ge=1, le=12),
    difficulty_level: Optional[str] = Body(None, pattern="^(easy|medium|hard)$"),
    topic: Optional[str] = Body(None),
    question_count: Optional[int] = Body(None, ge=5, le=20),
    time_limit_minutes: Optional[int] = Body(None, ge=10, le=120),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> AssessmentConfig:
    """Update an assessment configuration."""
    teacher_uid = current_user["uid"]
    
    try:
        update_fields = {
            "name": name,
            "subject": subject,
            "target_grade": target_grade,
            "difficulty_level": difficulty_level,
            "topic": topic,
            "question_count": question_count,
            "time_limit_minutes": time_limit_minutes
        }
        
        config = await assessment_service.update_assessment_config(
            config_id=config_id,
            teacher_uid=teacher_uid,
            **update_fields
        )
        
        logger.info(f"Updated assessment config {config_id}")
        return config
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update assessment config: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update assessment configuration: {str(e)}"
        )

@router.delete("/configs/{config_id}", tags=["Assessment Configuration"])
async def deactivate_assessment_config(
    config_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, str]:
    """Deactivate an assessment configuration."""
    teacher_uid = current_user["uid"]
    
    try:
        await assessment_service.deactivate_assessment_config(config_id, teacher_uid)
        
        return {"message": f"Assessment configuration {config_id} deactivated successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to deactivate assessment config: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to deactivate assessment configuration: {str(e)}"
        )

# ====================================================================
# Assessment Generation Endpoints
# ====================================================================

@router.post("/generate/{config_id}", response_model=Assessment, tags=["Assessment Generation"])
async def generate_assessment(
    config_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Assessment:
    """
    Generate a new MCQ assessment from a configuration.
    
    This will use AI to create questions based on:
    - Uploaded documents (RAG system)
    - Configuration parameters (difficulty, grade, topic)
    - Subject-specific knowledge
    
    Args:
        config_id: ID of the assessment configuration to use
        current_user: Authenticated teacher
        
    Returns:
        Assessment object with generated questions
        
    Raises:
        HTTPException: If generation fails or config not found
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Generating assessment from config {config_id} for teacher {teacher_uid}")
        
        assessment = await assessment_service.generate_assessment_from_config(
            config_id=config_id,
            teacher_uid=teacher_uid
        )
        
        logger.info(f"Generated assessment {assessment.assessment_id} with {len(assessment.questions)} questions")
        return assessment
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Assessment generation failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate assessment: {str(e)}"
        )

@router.get("/", response_model=List[Assessment], tags=["Assessment Management"])
async def get_my_assessments(
    subject: Optional[str] = Query(None, description="Filter by subject"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[Assessment]:
    """
    Get all assessments created by the current teacher.
    
    Args:
        subject: Optional filter by subject
        current_user: Authenticated teacher
        
    Returns:
        List of Assessment objects
    """
    teacher_uid = current_user["uid"]
    
    try:
        assessments = await assessment_service.get_teacher_assessments(
            teacher_uid=teacher_uid,
            subject_filter=subject
        )
        
        logger.info(f"Retrieved {len(assessments)} assessments for teacher {teacher_uid}")
        return assessments
        
    except Exception as e:
        logger.error(f"Failed to retrieve assessments: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessments: {str(e)}"
        )

@router.get("/{assessment_id}", response_model=Assessment, tags=["Assessment Management"])
async def get_assessment(
    assessment_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Assessment:
    """Get a specific assessment with all questions."""
    teacher_uid = current_user["uid"]
    
    try:
        assessment = await assessment_service.get_assessment(assessment_id)
        
        # Verify ownership
        if assessment.teacher_uid != teacher_uid:
            raise HTTPException(status_code=403, detail="Access denied")
        
        return assessment
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get assessment: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment: {str(e)}"
        )

@router.delete("/{assessment_id}", tags=["Assessment Management"])
async def deactivate_assessment(
    assessment_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, str]:
    """Deactivate an assessment (soft delete)."""
    teacher_uid = current_user["uid"]
    
    try:
        await assessment_service.deactivate_assessment(assessment_id, teacher_uid)
        
        return {"message": f"Assessment {assessment_id} deactivated successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to deactivate assessment: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to deactivate assessment: {str(e)}"
        )

# ====================================================================
# Helper Endpoints
# ====================================================================

@router.get("/topics/{subject}/{grade}", response_model=List[str], tags=["Assessment Helpers"])
async def get_available_topics(
    subject: str,
    grade: int = Path(..., ge=1, le=12, description="Grade level"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[str]:
    """
    Get available topics for a subject and grade based on uploaded documents.
    
    Args:
        subject: Subject name
        grade: Grade level (1-12)
        current_user: Authenticated teacher
        
    Returns:
        List of available topic names
    """
    teacher_uid = current_user["uid"]
    
    try:
        topics = await assessment_service.get_available_topics_for_subject(
            teacher_uid=teacher_uid,
            subject=subject,
            grade_level=grade
        )
        
        return topics
        
    except Exception as e:
        logger.error(f"Failed to get available topics: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve topics: {str(e)}"
        )


================================================================================
File: app/api/v1/auth.py
Size: 4.29 kB
================================================================================

# FILE: app/api/v1/auth.py

from fastapi import APIRouter, HTTPException, status, Depends
# Make sure to import UserProfileUpdate
from app.models import UserCreate, UserInDB, UserProfileUpdate
from app.core.firebase import firebase_auth, db
from firebase_admin.auth import EmailAlreadyExistsError, UserNotFoundError
from app.core.auth import get_current_user
from datetime import datetime

router = APIRouter()

@router.post("/signup", response_model=UserInDB, status_code=status.HTTP_201_CREATED)
def create_user(user_in: UserCreate):
    """
    Create a new user in Firebase Authentication and a corresponding user document in Firestore.
    """
    try:
        user_record = firebase_auth.create_user(
            email=user_in.email,
            password=user_in.password
        )
        new_user_data = {
            "uid": user_record.uid,
            "email": user_record.email,
            "created_at": datetime.utcnow(),
            "subjects": [],
            "role": user_in.role or "student",
            "first_name": user_in.first_name or None,
            "last_name": user_in.last_name or None
        }
        db.collection("users").document(user_record.uid).set(new_user_data)
        return new_user_data
    except EmailAlreadyExistsError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="The email address is already in use by another account."
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred: {str(e)}"
        )

@router.get("/me", response_model=UserInDB)
def get_user_profile(current_user: dict = Depends(get_current_user)):
    """
    Retrieve the profile of the currently authenticated user from Firestore.
    """
    user_uid = current_user["uid"]
    try:
        user_doc = db.collection("users").document(user_uid).get()
        if user_doc.exists:
            user_data = user_doc.to_dict()
            # Ensure 'role' is always present in the response (default to 'student' if missing)
            if "role" not in user_data:
                user_data["role"] = "student"
            return user_data
        else:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found in database.")
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred while fetching user profile: {str(e)}"
        )

@router.put("/me/profile", response_model=UserInDB) # <-- ADD THIS NEW ENDPOINT
def update_user_profile(profile_data: UserProfileUpdate, current_user: dict = Depends(get_current_user)):
    """
    Update the profile of the currently authenticated user (e.g., their subjects).
    """
    user_uid = current_user["uid"]
    try:
        user_ref = db.collection("users").document(user_uid)
        
        # Using .update() will create the fields if they don't exist or overwrite them if they do.
        user_ref.update(profile_data.model_dump())
        
        # Retrieve the updated document to return it
        updated_doc = user_ref.get()
        if not updated_doc.exists:
             raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found after update.")
        
        return updated_doc.to_dict()

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred while updating user profile: {str(e)}"
        )


@router.post("/logout", status_code=status.HTTP_200_OK)
def logout_user(current_user: dict = Depends(get_current_user)):
    """
    Logs out the user by revoking their refresh tokens.
    """
    user_uid = current_user["uid"]
    try:
        firebase_auth.revoke_refresh_tokens(user_uid)
        return {"message": f"Successfully logged out user {user_uid}"}
    except UserNotFoundError:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="User not found.")
    except Exception as e:
         raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An error occurred during logout: {str(e)}"
        )

================================================================================
File: app/api/v1/content.py
Size: 0 B
================================================================================



================================================================================
File: app/api/v1/documents.py
Size: 9.43 kB
================================================================================

# FILE: app/api/v1/documents.py

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form, Query
from typing import Dict, Any, List, Optional, Union
import logging

from app.models.requests import DocumentUploadResponse, DocumentIndexingStatus, DocumentMetadata, ZipUploadResponse
from app.core.auth import get_current_user
from app.services.document_service import document_service

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/upload", tags=["Document Management"])
async def upload_document(
    file: UploadFile = File(..., description="Document file to upload"),
    subject: str = Form(..., description="Subject category for the document"),
    grade_level: int = Form(..., description="Grade level (1-12) for the document"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Union[DocumentUploadResponse, ZipUploadResponse]:
    """
    Upload a document for RAG indexing.
    
    Supported file types:
    - PDF documents
    - Images (JPEG, PNG, TIFF)  
    - Text files
    - Word documents (.doc, .docx)
    - ZIP archives containing supported files
    
    The document will be:
    1. Uploaded to Firebase Storage
    2. Indexed in Vertex AI RAG engine (background process)
    3. Made searchable for content generation
    
    For ZIP files:
    - All supported files within the ZIP will be extracted and processed
    - Unsupported files and system files are automatically skipped
    - Detailed extraction report is returned
    
    Args:
        file: The document file to upload (or ZIP archive)
        subject: Subject category (must match teacher's subjects)
        grade_level: Grade level from 1-12 for the document content
        current_user: The authenticated user information
        
    Returns:
        DocumentUploadResponse for single files, ZipUploadResponse for ZIP files
        
    Raises:
        HTTPException: If upload fails or file type not supported
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Uploading document for teacher {teacher_uid}, subject: {subject}, grade: {grade_level}")
        
        # TODO: Validate that subject is in teacher's subject list
        # This can be added later when we integrate with teacher profiles
        
        result = await document_service.upload_document(
            file=file,
            subject=subject,
            grade_level=grade_level,
            teacher_uid=teacher_uid
        )
        
        # Log success differently for ZIP vs single files
        if hasattr(result, 'zip_filename'):
            logger.info(f"Successfully processed ZIP {result.zip_filename}: {result.files_processed} files")
        else:
            logger.info(f"Successfully uploaded document {result.document_id}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document upload failed for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to upload document: {str(e)}"
        )

@router.get("/status/{document_id}", response_model=DocumentIndexingStatus, tags=["Document Management"])
async def get_indexing_status(
    document_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> DocumentIndexingStatus:
    """
    Get the indexing status of a specific document.
    
    Status values:
    - `pending`: Document uploaded, waiting to be processed
    - `processing`: Currently being indexed in Vertex AI
    - `completed`: Successfully indexed and searchable
    - `failed`: Indexing failed (check error_message)
    
    Args:
        document_id: The unique document identifier
        current_user: The authenticated user information
        
    Returns:
        Current indexing status and progress
        
    Raises:
        HTTPException: If document not found or access denied
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Getting indexing status for document {document_id}")
        
        # TODO: Add permission check to ensure teacher owns the document
        
        status = await document_service.get_indexing_status(document_id)
        return status
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get indexing status for document {document_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get indexing status: {str(e)}"
        )

@router.get("/list", response_model=List[DocumentMetadata], tags=["Document Management"])
async def list_documents(
    subject: Optional[str] = None,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[DocumentMetadata]:
    """
    List all documents uploaded by the authenticated teacher.
    
    Args:
        subject: Optional filter by subject
        current_user: The authenticated user information
        
    Returns:
        List of document metadata sorted by upload date (newest first)
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Listing documents for teacher {teacher_uid}, subject filter: {subject}")
        
        documents = await document_service.list_teacher_documents(
            teacher_uid=teacher_uid,
            subject_filter=subject
        )
        
        return documents
        
    except Exception as e:
        logger.error(f"Failed to list documents for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list documents: {str(e)}"
        )

@router.delete("/{document_id}", tags=["Document Management"])
async def delete_document(
    document_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, str]:
    """
    Delete a document and remove it from indexing.
    
    This will:
    1. Remove the document from Firebase Storage
    2. Remove it from Vertex AI Search index
    3. Delete metadata from Firestore
    
    Args:
        document_id: The unique document identifier
        current_user: The authenticated user information
        
    Returns:
        Deletion confirmation
        
    Raises:
        HTTPException: If document not found or access denied
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Deleting document {document_id} for teacher {teacher_uid}")
        
        # TODO: Implement document deletion
        # This should include:
        # 1. Check teacher owns the document
        # 2. Delete from Firebase Storage
        # 3. Remove from Vertex AI Search
        # 4. Delete Firestore metadata
        
        return {
            "message": f"Document {document_id} deletion initiated",
            "status": "pending"
        }
        
    except Exception as e:
        logger.error(f"Failed to delete document {document_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete document: {str(e)}"
        )

@router.get("/health", tags=["Document Management"])
async def document_service_health() -> Dict[str, str]:
    """
    Check the health status of the document service.
    
    Returns:
        Health status information
    """
    return {
        "status": "healthy",
        "message": "Document service is operational",
        "vertex_ai_status": "connected"
    }

@router.get("/organized", tags=["Document Management"])
async def list_documents_organized(
    subject: Optional[str] = Query(None, description="Filter by subject"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    List teacher's documents organized by individual uploads and ZIP extractions.
    
    Args:
        subject: Optional subject filter
        current_user: Current authenticated user (from dependency injection)
    
    Returns:
        Dictionary with organized document lists
    """
    try:
        teacher_uid = current_user["uid"]
        
        logger.info(f"Listing organized documents for teacher {teacher_uid}")
        
        # Get organized document listing
        organized_docs = await document_service.list_documents_with_zip_info(
            teacher_uid=teacher_uid,
            subject_filter=subject
        )
        
        return {
            "teacher_uid": teacher_uid,
            "individual_documents": [doc.dict() for doc in organized_docs['individual']],
            "zip_extractions": {
                zip_name: [doc.dict() for doc in docs] 
                for zip_name, docs in organized_docs['zip_extractions'].items()
            },
            "total_individual": len(organized_docs['individual']),
            "total_zip_groups": len(organized_docs['zip_extractions']),
            "total_extracted_files": sum(len(docs) for docs in organized_docs['zip_extractions'].values())
        }
        
    except Exception as e:
        logger.error(f"Failed to list organized documents for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list organized documents: {str(e)}"
        )


================================================================================
File: app/api/v1/lessons.py
Size: 20.07 kB
================================================================================

# FILE: app/api/v1/lessons.py

from fastapi import APIRouter, Depends, HTTPException, Body
from typing import Dict, Any, Optional, List
import logging

from app.core.auth import get_current_user
from app.services.lesson_service import lesson_service

router = APIRouter(prefix="/lessons", tags=["lessons"])
logger = logging.getLogger(__name__)

# ====================================================================
# LESSON CREATION AND MANAGEMENT ENDPOINTS
# ====================================================================

@router.post("/create-from-step", response_model=Dict[str, Any])
async def create_lesson_from_learning_step(
    lesson_request: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Create a new interactive lesson from a learning step.
    
    Expected request body:
    {
        "learning_step_id": "step_id",
        "student_id": "student_id",
        "customizations": {
            "difficulty_adjustment": "easier|normal|harder",
            "focus_areas": ["area1", "area2"],
            "learning_style": "visual|auditory|kinesthetic",
            "include_interactive": true,
            "slide_count_preference": "short|medium|long"
        }
    }
    """
    teacher_uid = current_user["uid"]
    
    try:
        learning_step_id = lesson_request.get("learning_step_id")
        student_id = lesson_request.get("student_id")
        customizations = lesson_request.get("customizations", {})
        
        if not learning_step_id or not student_id:
            raise HTTPException(
                status_code=400,
                detail="learning_step_id and student_id are required"
            )
        
        # Create lesson using the service
        result = await lesson_service.create_lesson_from_step(
            learning_step_id=learning_step_id,
            student_id=student_id,
            teacher_uid=teacher_uid,
            customizations=customizations
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=f"Failed to create lesson: {result.get('error', 'Unknown error')}"
            )
        
        logger.info(f"Created lesson {result['lesson_id']} for student {student_id}")
        
        return {
            "lesson_created": True,
            "lesson_id": result["lesson_id"],
            "lesson_details": {
                "title": result["lesson"]["title"],
                "total_slides": result["creation_details"]["total_slides"],
                "estimated_duration_minutes": result["creation_details"]["estimated_duration_minutes"],
                "learning_objectives": result["creation_details"]["learning_objectives"]
            },
            "progress": result["progress"],
            "next_actions": [
                "Student can start the lesson",
                "Teacher can monitor progress",
                "Chatbot is available for student questions"
            ]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create lesson from step: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Lesson creation failed: {str(e)}"
        )

@router.get("/{lesson_id}", response_model=Dict[str, Any])
async def get_lesson_content(
    lesson_id: str,
    include_chat: bool = False,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Get lesson content and progress for a student.
    """
    try:
        # For students, they can only access their own lessons
        # For teachers, they can access lessons for their students
        user_role = current_user.get("role", "student")
        
        if user_role == "student":
            student_id = current_user["uid"]
        else:
            # Teacher access - would need to verify lesson ownership
            # For now, get student_id from query params or lesson data
            student_id = current_user.get("viewing_student_id", current_user["uid"])
        
        result = await lesson_service.get_student_lesson(
            lesson_id=lesson_id,
            student_id=student_id,
            include_chat_history=include_chat
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=404,
                detail=result.get("error", "Lesson not found")
            )
        
        return {
            "lesson": result["lesson"],
            "progress": result["progress"],
            "chat_sessions": result.get("chat_sessions", []) if include_chat else [],
            "access_role": user_role
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get lesson content: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve lesson: {str(e)}"
        )

@router.get("/student/{student_id}", response_model=Dict[str, Any])
async def get_student_lessons(
    student_id: str,
    include_progress: bool = True,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Get all lessons for a specific student.
    """
    teacher_uid = current_user["uid"]
    
    try:
        result = await lesson_service.get_student_lessons(
            student_id=student_id,
            teacher_uid=teacher_uid,
            include_progress=include_progress
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to retrieve lessons")
            )
        
        return {
            "student_id": student_id,
            "total_lessons": result["total_lessons"],
            "lessons": result["lessons"],
            "summary": {
                "completed_lessons": sum(1 for lesson in result["lessons"] 
                                       if lesson.get("progress", {}).get("completion_percentage", 0) >= 100),
                "in_progress_lessons": sum(1 for lesson in result["lessons"] 
                                         if 0 < lesson.get("progress", {}).get("completion_percentage", 0) < 100),
                "not_started_lessons": sum(1 for lesson in result["lessons"] 
                                         if lesson.get("progress", {}).get("completion_percentage", 0) == 0)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get student lessons: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to retrieve student lessons: {str(e)}"
        )

# ====================================================================
# LESSON PROGRESS AND INTERACTION ENDPOINTS
# ====================================================================

@router.post("/{lesson_id}/progress", response_model=Dict[str, Any])
async def update_lesson_progress(
    lesson_id: str,
    progress_update: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Update student progress on a lesson slide.
    
    Expected request body:
    {
        "slide_id": "slide_id",
        "is_completed": true,
        "time_spent_minutes": 5,
        "responses": {
            "interactive_element_1": "answer",
            "quiz_response": "option_b"
        },
        "difficulty_feedback": "too_easy|just_right|too_hard",
        "understanding_level": 1-5
    }
    """
    student_id = current_user["uid"]
    
    try:
        slide_id = progress_update.get("slide_id")
        
        if not slide_id:
            raise HTTPException(
                status_code=400,
                detail="slide_id is required"
            )
        
        # Prepare progress data
        progress_data = {
            "is_completed": progress_update.get("is_completed", False),
            "time_spent_minutes": progress_update.get("time_spent_minutes", 0),
            "student_responses": progress_update.get("responses", {}),
            "difficulty_feedback": progress_update.get("difficulty_feedback"),
            "understanding_level": progress_update.get("understanding_level"),
            "completed_at": progress_update.get("completed_at"),
            "last_updated": progress_update.get("last_updated")
        }
        
        result = await lesson_service.update_slide_progress(
            lesson_id=lesson_id,
            student_id=student_id,
            slide_id=slide_id,
            progress_data=progress_data
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to update progress")
            )
        
        response = {
            "progress_updated": True,
            "slides_completed": result["slides_completed"],
            "completion_percentage": result["completion_percentage"],
            "lesson_completed": result["lesson_completed"]
        }
        
        # Add congratulations message if lesson completed
        if result["lesson_completed"]:
            response["congratulations"] = {
                "message": "🎉 Congratulations! You've completed this lesson!",
                "achievements": [
                    f"Completed {result['slides_completed']} slides",
                    "Gained new knowledge and skills",
                    "Ready for the next learning challenge"
                ]
            }
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update lesson progress: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Progress update failed: {str(e)}"
        )

# ====================================================================
# LESSON CHATBOT ENDPOINTS
# ====================================================================

@router.post("/{lesson_id}/chat/start", response_model=Dict[str, Any])
async def start_lesson_chatbot(
    lesson_id: str,
    chat_request: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Start a new chatbot session for a lesson.
    
    Expected request body:
    {
        "initial_message": "I need help with fractions"  // Optional
    }
    """
    student_id = current_user["uid"]
    
    try:
        initial_message = chat_request.get("initial_message")
        
        result = await lesson_service.start_lesson_chatbot(
            lesson_id=lesson_id,
            student_id=student_id,
            initial_message=initial_message
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to start chat session")
            )
        
        return {
            "chat_started": True,
            "session_id": result["session_id"],
            "messages": result["messages"],
            "chatbot_features": [
                "Ask questions about lesson content",
                "Get explanations and examples",
                "Receive hints for exercises",
                "Clarify difficult concepts"
            ]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to start lesson chatbot: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chat session start failed: {str(e)}"
        )

@router.post("/chat/{session_id}/message", response_model=Dict[str, Any])
async def send_chat_message(
    session_id: str,
    message_request: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Send a message to the lesson chatbot.
    
    Expected request body:
    {
        "message": "Can you explain this concept again?",
        "current_slide_id": "slide_123"  // Optional, for context
    }
    """
    student_id = current_user["uid"]
    
    try:
        message = message_request.get("message")
        current_slide_id = message_request.get("current_slide_id")
        
        if not message:
            raise HTTPException(
                status_code=400,
                detail="message is required"
            )
        
        result = await lesson_service.send_chatbot_message(
            session_id=session_id,
            student_id=student_id,
            message=message,
            current_slide_id=current_slide_id
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to send message")
            )
        
        return {
            "message_sent": True,
            "agent_response": result["agent_response"],
            "suggested_actions": result.get("suggested_actions", []),
            "conversation_active": True
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to send chat message: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Message sending failed: {str(e)}"
        )

# ====================================================================
# LESSON ANALYTICS AND MANAGEMENT ENDPOINTS
# ====================================================================

@router.get("/{lesson_id}/analytics", response_model=Dict[str, Any])
async def get_lesson_analytics(
    lesson_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Get comprehensive analytics for a lesson (teacher access).
    """
    teacher_uid = current_user["uid"]
    
    try:
        result = await lesson_service.get_lesson_analytics(
            lesson_id=lesson_id,
            teacher_uid=teacher_uid
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=404,
                detail=result.get("error", "Lesson not found or access denied")
            )
        
        return {
            "lesson_id": lesson_id,
            "analytics": result["lesson_analytics"],
            "insights": {
                "performance_summary": _generate_performance_insights(result["lesson_analytics"]),
                "recommendations": _generate_lesson_recommendations(result["lesson_analytics"])
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get lesson analytics: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analytics retrieval failed: {str(e)}"
        )

@router.post("/{lesson_id}/regenerate-slide", response_model=Dict[str, Any])
async def regenerate_lesson_slide(
    lesson_id: str,
    regeneration_request: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Regenerate a specific slide in a lesson based on feedback.
    
    Expected request body:
    {
        "slide_id": "slide_123",
        "student_id": "student_456",
        "reason": "too_difficult|too_easy|confusing|needs_more_examples",
        "specific_feedback": "Student struggled with the math problems"
    }
    """
    try:
        slide_id = regeneration_request.get("slide_id")
        student_id = regeneration_request.get("student_id")
        reason = regeneration_request.get("reason")
        specific_feedback = regeneration_request.get("specific_feedback", "")
        
        if not slide_id or not student_id or not reason:
            raise HTTPException(
                status_code=400,
                detail="slide_id, student_id, and reason are required"
            )
        
        # Combine reason and feedback
        regeneration_reason = f"{reason}: {specific_feedback}".strip(": ")
        
        result = await lesson_service.regenerate_lesson_slide(
            lesson_id=lesson_id,
            slide_id=slide_id,
            student_id=student_id,
            regeneration_reason=regeneration_reason
        )
        
        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Failed to regenerate slide")
            )
        
        return {
            "slide_regenerated": True,
            "lesson_id": lesson_id,
            "slide_id": slide_id,
            "new_slide": result["new_slide"],
            "regenerated_at": result["regenerated_at"],
            "changes_made": [
                "Updated content based on feedback",
                "Adjusted difficulty level",
                "Enhanced explanations",
                "Improved interactive elements"
            ]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to regenerate lesson slide: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Slide regeneration failed: {str(e)}"
        )

# ====================================================================
# HELPER FUNCTIONS
# ====================================================================

def _generate_performance_insights(analytics: Dict[str, Any]) -> List[str]:
    """Generate performance insights from analytics data."""
    insights = []
    
    completion_rate = analytics.get("completion_rate", 0)
    success_rate = analytics.get("engagement_metrics", {}).get("success_rate", 0)
    chat_usage_rate = analytics.get("chat_analytics", {}).get("chat_usage_rate", 0)
    
    if completion_rate >= 80:
        insights.append("🎉 Excellent completion rate - students are engaging well with the lesson")
    elif completion_rate >= 60:
        insights.append("✅ Good completion rate - most students are finishing the lesson")
    else:
        insights.append("⚠️ Low completion rate - consider reviewing lesson difficulty or length")
    
    if success_rate >= 80:
        insights.append("💪 High success rate - students understand the concepts well")
    elif success_rate >= 60:
        insights.append("👍 Moderate success rate - some concepts may need reinforcement")
    else:
        insights.append("📚 Low success rate - consider adding more examples or simplifying content")
    
    if chat_usage_rate >= 50:
        insights.append("💬 High chatbot usage - students are actively seeking help")
    elif chat_usage_rate >= 25:
        insights.append("🤔 Moderate chatbot usage - some students using available support")
    else:
        insights.append("💡 Low chatbot usage - consider promoting the chatbot feature")
    
    return insights

def _generate_lesson_recommendations(analytics: Dict[str, Any]) -> List[str]:
    """Generate recommendations based on analytics."""
    recommendations = []
    
    completion_rate = analytics.get("completion_rate", 0)
    success_rate = analytics.get("engagement_metrics", {}).get("success_rate", 0)
    
    if completion_rate < 60:
        recommendations.append("Consider shortening the lesson or breaking it into smaller parts")
        recommendations.append("Add more interactive elements to maintain engagement")
    
    if success_rate < 60:
        recommendations.append("Add more examples and practice exercises")
        recommendations.append("Consider regenerating difficult slides with simpler explanations")
    
    if analytics.get("chat_analytics", {}).get("total_questions", 0) > 0:
        recommendations.append("Review common questions to identify areas needing clearer explanation")
    
    if analytics.get("average_time_spent_minutes", 0) > 60:
        recommendations.append("Lesson may be too long - consider optimizing content length")
    
    return recommendations if recommendations else ["Lesson is performing well - no immediate changes needed"]


================================================================================
File: app/api/v1/personalized_learning.py
Size: 24.04 kB
================================================================================

# FILE: app/api/v1/personalized_learning.py

from fastapi import APIRouter, Depends, HTTPException, status, Body
from typing import Dict, Any, List, Optional
import logging

from app.models.learning_models import (
    StudentPerformance, KnowledgeGap, LearningPath, 
    LearningRecommendation, LearningStep
)
from app.models.student import Assessment
from app.core.auth import get_current_user
from app.services.assessment_analysis_service import assessment_analysis_service
from app.services.learning_path_service import learning_path_service
from app.services.enhanced_assessment_service import enhanced_assessment_service
from app.services.learning_path_monitoring_service import learning_path_monitoring_service

logger = logging.getLogger(__name__)

router = APIRouter()

# ====================================================================
# LEARNING PATH AGENT MONITORING ENDPOINTS
# ====================================================================

@router.post("/start-monitoring", response_model=Dict[str, Any])
async def start_learning_path_monitoring(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Start automated learning path monitoring for the teacher's students.
    Once activated, the learning path agent will automatically:
    1. Monitor assessment completions
    2. Analyze student performance  
    3. Generate personalized learning paths
    4. Adapt paths based on progress
    """
    teacher_uid = current_user["uid"]
    
    try:
        monitoring_result = await learning_path_monitoring_service.start_monitoring(teacher_uid)
        
        return {
            "teacher_uid": teacher_uid,
            "monitoring_activated": monitoring_result.get("monitoring_started", False),
            "agent_status": "active",
            "automation_features": [
                "Assessment completion detection",
                "Automatic performance analysis", 
                "Personalized learning path generation",
                "Progress monitoring and adaptation",
                "Real-time intervention recommendations"
            ],
            "monitoring_result": monitoring_result,
            "message": "🤖 Learning Path Agent is now monitoring your students! Assessments will automatically trigger personalized learning paths."
        }
        
    except Exception as e:
        logger.error(f"Failed to start monitoring: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Monitoring startup failed: {str(e)}"
        )

@router.get("/monitoring-status", response_model=Dict[str, Any])
async def get_monitoring_status(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get current learning path monitoring status for the teacher."""
    teacher_uid = current_user["uid"]
    
    try:
        status = await learning_path_monitoring_service.get_monitoring_status(teacher_uid)
        
        return {
            "teacher_uid": teacher_uid,
            "monitoring_status": status,
            "agent_active": status.get("monitoring_active", False),
            "automation_enabled": True,
            "last_updated": status.get("timestamp")
        }
        
    except Exception as e:
        logger.error(f"Failed to get monitoring status: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Status retrieval failed: {str(e)}"
        )

@router.post("/process-batch-assessments", response_model=Dict[str, Any])
async def process_batch_assessments(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Process all pending assessments and generate learning paths in batch.
    Useful for catching up on assessments completed before monitoring was active.
    """
    teacher_uid = current_user["uid"]
    
    try:
        batch_result = await learning_path_monitoring_service.process_batch_assessments(teacher_uid)
        
        return {
            "teacher_uid": teacher_uid,
            "batch_processing": batch_result,
            "assessments_processed": batch_result.get("assessments_processed", 0),
            "learning_paths_generated": batch_result.get("learning_paths_generated", 0),
            "students_helped": batch_result.get("students_helped", []),
            "message": f"🚀 Processed {batch_result.get('assessments_processed', 0)} assessments and generated {batch_result.get('learning_paths_generated', 0)} learning paths!"
        }
        
    except Exception as e:
        logger.error(f"Failed to process batch assessments: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Batch processing failed: {str(e)}"
        )

# ====================================================================
# ASSESSMENT ANALYSIS ENDPOINTS
# ====================================================================

@router.post("/analyze-assessment", response_model=Dict[str, Any])
async def analyze_student_assessment(
    assessment_data: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Analyze a student's assessment performance and identify learning needs.
    
    Expected format:
    {
        "student_id": "student_id",
        "assessment_id": "assessment_id", 
        "student_answers": [0, 1, 2, 0, 3],  # List of selected answer indices
        "time_taken_minutes": 25
    }
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Extract data
        student_id = assessment_data["student_id"]
        assessment_id = assessment_data["assessment_id"]
        student_answers = assessment_data["student_answers"]
        time_taken = assessment_data["time_taken_minutes"]
        
        # Get the assessment
        assessment = await enhanced_assessment_service.get_assessment_by_id(assessment_id)
        if not assessment:
            raise HTTPException(
                status_code=404,
                detail="Assessment not found"
            )
        
        # Verify teacher owns this assessment
        if assessment.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=403,
                detail="Access denied: Not your assessment"
            )
        
        # Analyze performance
        performance = await assessment_analysis_service.analyze_assessment_performance(
            student_id=student_id,
            assessment=assessment,
            student_answers=student_answers,
            time_taken_minutes=time_taken
        )
        
        # 🤖 AUTOMATICALLY TRIGGER LEARNING PATH AGENT
        # This is where the magic happens - no manual intervention needed!
        agent_response = await learning_path_monitoring_service.handle_assessment_completion(
            student_id=student_id,
            assessment_id=assessment_id,
            student_answers=student_answers,
            time_taken_minutes=time_taken,
            teacher_uid=teacher_uid
        )
        
        logger.info(f"Analyzed assessment {assessment_id} for student {student_id}")
        
        return {
            "performance_id": performance.performance_id,
            "student_id": student_id,
            "assessment_id": assessment_id,
            "overall_score": performance.score_percentage,
            "time_taken": performance.time_taken_minutes,
            "topic_scores": performance.topic_scores,
            "difficulty_scores": performance.difficulty_scores,
            "strengths": performance.strengths,
            "weaknesses": performance.weaknesses,
            "recommended_focus_areas": performance.recommended_focus_areas,
            "analysis_completed": True,
            
            # 🚀 AGENT-POWERED AUTOMATION RESULTS
            "agent_intervention": {
                "triggered": True,
                "learning_path_generated": agent_response.get("learning_path_generated", False),
                "intervention_type": agent_response.get("intervention_type"),
                "agent_response": agent_response.get("agent_response", {}),
                "automated_action": "Learning path agent automatically analyzed performance and generated personalized learning path"
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to analyze assessment: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analysis failed: {str(e)}"
        )

@router.get("/student/{student_id}/progress", response_model=Dict[str, Any])
async def get_student_progress(
    student_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get comprehensive progress summary for a student."""
    
    try:
        progress_summary = await assessment_analysis_service.get_student_progress_summary(student_id)
        
        logger.info(f"Retrieved progress summary for student {student_id}")
        return progress_summary
        
    except Exception as e:
        logger.error(f"Failed to get student progress: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get progress: {str(e)}"
        )

# ====================================================================
# LEARNING PATH ENDPOINTS  
# ====================================================================

@router.post("/generate-learning-path", response_model=Dict[str, Any])
async def generate_learning_path(
    path_request: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Generate a personalized learning path for a student.
    
    Expected format:
    {
        "student_id": "student_id",
        "target_subject": "Mathematics",
        "target_grade": 5,
        "learning_goals": ["Master addition", "Improve problem solving"],
        "include_recent_assessments": 3  // Number of recent assessments to consider
    }
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Extract request data
        student_id = path_request["student_id"]
        target_subject = path_request["target_subject"]
        target_grade = path_request["target_grade"]
        learning_goals = path_request.get("learning_goals", [])
        include_recent = path_request.get("include_recent_assessments", 3)
        
        # Get student's recent performance data
        progress_summary = await assessment_analysis_service.get_student_progress_summary(student_id)
        
        # Get recent performances (this would need to be implemented in the analysis service)
        recent_performances = []  # Placeholder - would get from progress_summary
        
        # Get current knowledge gaps
        knowledge_gaps = []  # Placeholder - would get from analysis service
        
        # Generate learning path
        learning_path = await learning_path_service.generate_personalized_learning_path(
            student_id=student_id,
            teacher_uid=teacher_uid,
            knowledge_gaps=knowledge_gaps,
            student_performances=recent_performances,
            target_subject=target_subject,
            target_grade=target_grade,
            learning_goals=learning_goals
        )
        
        logger.info(f"Generated learning path {learning_path.path_id} for student {student_id}")
        
        return {
            "path_id": learning_path.path_id,
            "title": learning_path.title,
            "description": learning_path.description,
            "total_steps": len(learning_path.steps),
            "estimated_duration_hours": learning_path.total_estimated_duration_minutes / 60,
            "learning_goals": learning_path.learning_goals,
            "addresses_gaps": len(learning_path.addresses_gaps),
            "steps_preview": [
                {
                    "step_number": step.step_number,
                    "title": step.title,
                    "topic": step.topic,
                    "difficulty": step.difficulty_level.value,
                    "estimated_minutes": step.estimated_duration_minutes
                }
                for step in learning_path.steps[:5]  # First 5 steps
            ],
            "path_created": True
        }
        
    except Exception as e:
        logger.error(f"Failed to generate learning path: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Path generation failed: {str(e)}"
        )

@router.get("/student/{student_id}/learning-paths", response_model=List[Dict[str, Any]])
async def get_student_learning_paths(
    student_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get all learning paths for a student."""
    
    try:
        learning_paths = await learning_path_service.get_student_learning_paths(student_id)
        
        return [
            {
                "path_id": path.path_id,
                "title": path.title,
                "subject": path.subject,
                "completion_percentage": path.completion_percentage,
                "current_step": path.current_step,
                "total_steps": len(path.steps),
                "created_at": path.created_at.isoformat(),
                "estimated_duration_hours": path.total_estimated_duration_minutes / 60,
                "status": "completed" if path.completed_at else "in_progress" if path.started_at else "not_started"
            }
            for path in learning_paths
        ]
        
    except Exception as e:
        logger.error(f"Failed to get learning paths: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get learning paths: {str(e)}"
        )

@router.get("/learning-path/{path_id}", response_model=Dict[str, Any])
async def get_learning_path_details(
    path_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get detailed information about a learning path."""
    
    try:
        learning_path = await learning_path_service.get_learning_path(path_id)
        
        if not learning_path:
            raise HTTPException(
                status_code=404,
                detail="Learning path not found"
            )
        
        return {
            "path_id": learning_path.path_id,
            "title": learning_path.title,
            "description": learning_path.description,
            "subject": learning_path.subject,
            "target_grade": learning_path.target_grade,
            "learning_goals": learning_path.learning_goals,
            "completion_percentage": learning_path.completion_percentage,
            "current_step": learning_path.current_step,
            "total_estimated_duration_minutes": learning_path.total_estimated_duration_minutes,
            "steps": [
                {
                    "step_id": step.step_id,
                    "step_number": step.step_number,
                    "title": step.title,
                    "description": step.description,
                    "topic": step.topic,
                    "subtopic": step.subtopic,
                    "difficulty_level": step.difficulty_level.value,
                    "learning_objective": step.learning_objective.value,
                    "content_type": step.content_type,
                    "content_text": step.content_text,
                    "estimated_duration_minutes": step.estimated_duration_minutes,
                    "is_completed": step.is_completed,
                    "completed_at": step.completed_at.isoformat() if step.completed_at else None,
                    "performance_score": step.performance_score
                }
                for step in learning_path.steps
            ],
            "created_at": learning_path.created_at.isoformat(),
            "last_updated": learning_path.last_updated.isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to get learning path details: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get learning path: {str(e)}"
        )

@router.post("/learning-path/{path_id}/update-progress")
async def update_learning_path_progress(
    path_id: str,
    progress_data: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Update progress on a learning path step.
    
    Expected format:
    {
        "step_id": "step_id",
        "completed": true,
        "performance_score": 85.5  // Optional
    }
    """
    
    try:
        step_id = progress_data["step_id"]
        completed = progress_data["completed"]
        performance_score = progress_data.get("performance_score")
        
        updated_path = await learning_path_service.update_learning_path_progress(
            path_id=path_id,
            step_id=step_id,
            completed=completed,
            performance_score=performance_score
        )
        
        logger.info(f"Updated progress for path {path_id}, step {step_id}")
        
        return {
            "path_id": path_id,
            "step_id": step_id,
            "completed": completed,
            "new_completion_percentage": updated_path.completion_percentage,
            "current_step": updated_path.current_step,
            "is_path_completed": updated_path.completed_at is not None,
            "update_successful": True
        }
        
    except Exception as e:
        logger.error(f"Failed to update learning path progress: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Progress update failed: {str(e)}"
        )

# ====================================================================
# ADAPTIVE LEARNING ENDPOINTS
# ====================================================================

@router.post("/adapt-learning-path/{path_id}")
async def adapt_learning_path(
    path_id: str,
    adaptation_data: Dict[str, Any] = Body(...),
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """
    Adapt a learning path based on new assessment results.
    
    Expected format:
    {
        "new_assessment_id": "assessment_id",
        "student_answers": [0, 1, 2],
        "time_taken_minutes": 20
    }
    """
    
    try:
        # Get new assessment performance
        assessment_id = adaptation_data["new_assessment_id"]
        student_answers = adaptation_data["student_answers"]
        time_taken = adaptation_data["time_taken_minutes"]
        
        # Get assessment and analyze performance
        assessment = await enhanced_assessment_service.get_assessment_by_id(assessment_id)
        if not assessment:
            raise HTTPException(status_code=404, detail="Assessment not found")
        
        # Get current learning path
        current_path = await learning_path_service.get_learning_path(path_id)
        if not current_path:
            raise HTTPException(status_code=404, detail="Learning path not found")
        
        # Analyze new performance
        new_performance = await assessment_analysis_service.analyze_assessment_performance(
            student_id=current_path.student_id,
            assessment=assessment,
            student_answers=student_answers,
            time_taken_minutes=time_taken
        )
        
        # Get new knowledge gaps (placeholder - would be implemented in analysis service)
        new_gaps = []  # This would come from the analysis service
        
        # Adapt the learning path
        adapted_path = await learning_path_service.adapt_learning_path(
            path_id=path_id,
            new_performance_data=new_performance,
            new_gaps=new_gaps
        )
        
        logger.info(f"Adapted learning path {path_id} based on assessment {assessment_id}")
        
        return {
            "path_id": path_id,
            "assessment_analyzed": assessment_id,
            "new_performance_score": new_performance.score_percentage,
            "adaptation_applied": True,
            "new_total_steps": len(adapted_path.steps),
            "adaptation_summary": adapted_path.adaptation_history[-1] if adapted_path.adaptation_history else None
        }
        
    except Exception as e:
        logger.error(f"Failed to adapt learning path: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Adaptation failed: {str(e)}"
        )

# ====================================================================
# ANALYTICS AND INSIGHTS ENDPOINTS
# ====================================================================

@router.get("/teacher/learning-analytics", response_model=Dict[str, Any])
async def get_teacher_learning_analytics(
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get learning analytics overview for a teacher."""
    teacher_uid = current_user["uid"]
    
    try:
        # This would aggregate data across all students
        # For now, return placeholder analytics
        
        analytics = {
            "teacher_uid": teacher_uid,
            "total_students_with_paths": 0,  # Would be calculated
            "total_active_paths": 0,
            "average_completion_rate": 0.0,
            "most_common_knowledge_gaps": [],
            "subject_performance_trends": {},
            "learning_path_effectiveness": {
                "paths_completed": 0,
                "average_improvement": 0.0,
                "student_satisfaction": 0.0
            },
            "recommendations_for_teacher": [
                "Create more assessments to gather learning data",
                "Upload subject-specific content for better RAG generation",
                "Review student progress regularly"
            ]
        }
        
        return analytics
        
    except Exception as e:
        logger.error(f"Failed to get learning analytics: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analytics retrieval failed: {str(e)}"
        )

@router.get("/student/{student_id}/learning-insights", response_model=Dict[str, Any])
async def get_student_learning_insights(
    student_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """Get detailed learning insights for a specific student."""
    
    try:
        # Get comprehensive student data
        progress_summary = await assessment_analysis_service.get_student_progress_summary(student_id)
        learning_paths = await learning_path_service.get_student_learning_paths(student_id)
        
        # Calculate insights
        insights = {
            "student_id": student_id,
            "learning_velocity": "steady",  # Would be calculated from progress data
            "strength_areas": progress_summary.get("subject_performance", {}),
            "improvement_areas": [],  # Would come from gap analysis
            "learning_style_indicators": {
                "prefers_visual": False,
                "needs_more_time": False,
                "learns_better_with_examples": True
            },
            "engagement_metrics": {
                "path_completion_rate": 0.0,
                "average_step_time": 0,
                "consistency_score": 0.0
            },
            "next_recommended_actions": [
                "Continue with current learning path",
                "Take assessment to validate progress",
                "Review challenging topics"
            ],
            "achievement_milestones": [],
            "learning_path_summary": {
                "total_paths": len(learning_paths),
                "active_paths": len([p for p in learning_paths if not p.completed_at]),
                "completed_paths": len([p for p in learning_paths if p.completed_at])
            }
        }
        
        return insights
        
    except Exception as e:
        logger.error(f"Failed to get student insights: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Insights retrieval failed: {str(e)}"
        )


================================================================================
File: app/api/v1/rag_assessments.py
Size: 11.08 kB
================================================================================

# FILE: app/api/v1/rag_assessments.py

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, status, Path, Query, UploadFile, File
from pydantic import BaseModel

from app.core.auth import get_current_user
from app.services.enhanced_assessment_service import EnhancedAssessmentService
from app.services.document_processor import DocumentProcessor
from app.models.student import AssessmentConfig, Assessment
from app.models.rag_models import ProcessedDocument

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/rag", tags=["RAG Assessments"])

# Initialize services
enhanced_assessment_service = EnhancedAssessmentService()
document_processor = DocumentProcessor()

# ====================================================================
# Document Upload and Processing Endpoints
# ====================================================================

class DocumentUploadResponse(BaseModel):
    document_id: str
    filename: str
    processing_status: str
    message: str

@router.post("/documents/upload", response_model=DocumentUploadResponse, tags=["Document Management"])
async def upload_document_for_rag(
    file: UploadFile = File(...),
    subject: Optional[str] = Query(None, description="Subject area"),
    grade_level: Optional[int] = Query(None, description="Target grade level"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> DocumentUploadResponse:
    """
    Upload a document for RAG processing.
    
    Supported formats: PDF, DOCX, TXT
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Validate file type
        allowed_types = [".pdf", ".docx", ".txt"]
        file_extension = "." + file.filename.split(".")[-1].lower()
        
        if file_extension not in allowed_types:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type. Allowed: {', '.join(allowed_types)}"
            )
        
        # Save uploaded file temporarily
        import tempfile
        import os
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # Process document
            document_id = f"doc_{teacher_uid}_{int(datetime.utcnow().timestamp())}"
            
            processed_doc = await document_processor.process_document(
                file_path=tmp_file_path,
                document_id=document_id,
                teacher_uid=teacher_uid,
                filename=file.filename,
                subject=subject,
                grade_level=grade_level
            )
            
            return DocumentUploadResponse(
                document_id=processed_doc.document_id,
                filename=processed_doc.original_filename,
                processing_status=processed_doc.processing_status.value,
                message=f"Document uploaded and processed successfully. Created {processed_doc.total_chunks} text chunks."
            )
            
        finally:
            # Clean up temporary file
            os.unlink(tmp_file_path)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Document upload failed for {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Document upload failed: {str(e)}"
        )

@router.get("/documents", response_model=List[ProcessedDocument], tags=["Document Management"])
async def get_teacher_documents(
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[ProcessedDocument]:
    """Get all documents uploaded by the teacher."""
    
    teacher_uid = current_user["uid"]
    
    try:
        documents = await document_processor.get_teacher_documents(teacher_uid)
        return documents
        
    except Exception as e:
        logger.error(f"Failed to get documents for {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve documents: {str(e)}"
        )

@router.get("/documents/{document_id}", response_model=ProcessedDocument, tags=["Document Management"])
async def get_document_details(
    document_id: str = Path(..., description="Document ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> ProcessedDocument:
    """Get details of a specific document."""
    
    teacher_uid = current_user["uid"]
    
    try:
        document = await document_processor.get_processed_document(document_id)
        
        if not document:
            raise HTTPException(
                status_code=404,
                detail="Document not found"
            )
        
        # Verify ownership
        if document.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=403,
                detail="Access denied to this document"
            )
        
        return document
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get document {document_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve document: {str(e)}"
        )

# ====================================================================
# RAG Assessment Generation Endpoints
# ====================================================================

@router.post("/configs/{config_id}/generate-rag", response_model=Assessment, tags=["RAG Generation"])
async def generate_rag_assessment(
    config_id: str = Path(..., description="Assessment configuration ID"),
    force_rag: bool = Query(True, description="Force RAG generation even with limited context"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Assessment:
    """
    Generate an assessment using RAG and AI based on uploaded documents.
    
    This endpoint uses the teacher's uploaded documents to generate 
    contextually relevant questions using AI.
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Get the configuration
        config = await enhanced_assessment_service.get_assessment_config_by_id(
            config_id=config_id,
            teacher_uid=teacher_uid
        )
        
        if not config:
            raise HTTPException(
                status_code=404,
                detail="Assessment configuration not found"
            )
        
        # Generate RAG-enhanced assessment
        assessment = await enhanced_assessment_service.create_rag_assessment(
            config=config,
            force_rag=force_rag
        )
        
        logger.info(f"Generated RAG assessment {assessment.assessment_id} from config {config_id}")
        return assessment
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate RAG assessment from config {config_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate RAG assessment: {str(e)}"
        )

@router.get("/assessments/{assessment_id}/metadata", tags=["RAG Generation"])
async def get_assessment_metadata(
    assessment_id: str = Path(..., description="Assessment ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Get detailed metadata about how an assessment was generated,
    including RAG context and AI generation details.
    """
    teacher_uid = current_user["uid"]
    
    try:
        assessment_data = await enhanced_assessment_service.get_assessment_with_metadata(assessment_id)
        
        if not assessment_data:
            raise HTTPException(
                status_code=404,
                detail="Assessment not found"
            )
        
        # TODO: Add ownership verification once we track assessment ownership
        
        # Return metadata
        metadata = {
            "assessment_id": assessment_id,
            "generation_method": assessment_data.get("generation_method", "simple"),
            "rag_metadata": assessment_data.get("rag_metadata", {}),
            "basic_info": {
                "title": assessment_data.get("title"),
                "subject": assessment_data.get("subject"),
                "grade": assessment_data.get("grade"),
                "topic": assessment_data.get("topic"),
                "question_count": len(assessment_data.get("questions", []))
            }
        }
        
        return metadata
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get assessment metadata {assessment_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment metadata: {str(e)}"
        )

# ====================================================================
# Content Search and Analytics Endpoints  
# ====================================================================

class ContentSearchRequest(BaseModel):
    search_query: str
    subject_filter: Optional[str] = None
    grade_filter: Optional[int] = None

@router.post("/content/search", tags=["Content Management"])
async def search_uploaded_content(
    request: ContentSearchRequest,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Search through uploaded document content using semantic similarity.
    """
    teacher_uid = current_user["uid"]
    
    try:
        results = await enhanced_assessment_service.search_teacher_content(
            teacher_uid=teacher_uid,
            search_query=request.search_query,
            subject_filter=request.subject_filter,
            grade_filter=request.grade_filter
        )
        
        return results
        
    except Exception as e:
        logger.error(f"Content search failed for {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Content search failed: {str(e)}"
        )

@router.get("/analytics/rag-stats", tags=["Analytics"])
async def get_rag_analytics(
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Get RAG usage analytics and content statistics for the teacher.
    """
    teacher_uid = current_user["uid"]
    
    try:
        stats = await enhanced_assessment_service.get_teacher_rag_statistics(teacher_uid)
        return stats
        
    except Exception as e:
        logger.error(f"Failed to get RAG analytics for {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve RAG analytics: {str(e)}"
        )


================================================================================
File: app/api/v1/simple_assessments.py
Size: 10.49 kB
================================================================================

# FILE: app/api/v1/simple_assessments.py

from fastapi import APIRouter, Depends, HTTPException, status, Query, Body, Path
from typing import Dict, Any, List, Optional
import logging

from app.models.student import AssessmentConfig, Assessment
from app.core.auth import get_current_user
from app.services.enhanced_assessment_service import enhanced_assessment_service

logger = logging.getLogger(__name__)

router = APIRouter()

# ====================================================================
# Assessment Configuration Endpoints
# ====================================================================

@router.post("/configs", response_model=AssessmentConfig, tags=["Assessment Configuration"])
async def create_assessment_config(
    config_data: Dict[str, Any] = Body(..., description="Assessment configuration data"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> AssessmentConfig:
    """
    Create a new assessment configuration.
    
    Args:
        config_data: Assessment configuration including name, subject, grade, difficulty, topic
        current_user: Authenticated teacher
        
    Returns:
        Created AssessmentConfig
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Validate required fields
        required_fields = ["name", "subject", "target_grade", "difficulty_level", "topic"]
        for field in required_fields:
            if field not in config_data:
                raise HTTPException(
                    status_code=400,
                    detail=f"Missing required field: {field}"
                )
        
        # Validate difficulty level
        if config_data["difficulty_level"] not in ["easy", "medium", "hard"]:
            raise HTTPException(
                status_code=400,
                detail="Difficulty level must be 'easy', 'medium', or 'hard'"
            )
        
        # Validate grade
        if not (1 <= config_data["target_grade"] <= 12):
            raise HTTPException(
                status_code=400,
                detail="Target grade must be between 1 and 12"
            )
        
        config = await enhanced_assessment_service.create_assessment_config(
            name=config_data["name"],
            subject=config_data["subject"],
            target_grade=config_data["target_grade"],
            difficulty_level=config_data["difficulty_level"],
            topic=config_data["topic"],
            teacher_uid=teacher_uid,
            question_count=config_data.get("question_count", 10),
            time_limit_minutes=config_data.get("time_limit_minutes", 30)
        )
        
        logger.info(f"Created assessment config {config.config_id} for teacher {teacher_uid}")
        return config
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create assessment config for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create assessment configuration: {str(e)}"
        )

@router.get("/configs", response_model=List[AssessmentConfig], tags=["Assessment Configuration"])
async def get_my_assessment_configs(
    subject: Optional[str] = Query(None, description="Filter by subject"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[AssessmentConfig]:
    """
    Get all assessment configurations for the current teacher.
    
    Args:
        subject: Optional subject filter
        current_user: Authenticated teacher
        
    Returns:
        List of AssessmentConfig objects
    """
    teacher_uid = current_user["uid"]
    
    try:
        configs = await enhanced_assessment_service.get_teacher_assessment_configs(
            teacher_uid=teacher_uid,
            subject_filter=subject
        )
        
        logger.info(f"Retrieved {len(configs)} assessment configs for teacher {teacher_uid}")
        return configs
        
    except Exception as e:
        logger.error(f"Failed to get assessment configs for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment configurations: {str(e)}"
        )

# ====================================================================
# Assessment Generation Endpoints
# ====================================================================

@router.post("/configs/{config_id}/generate", response_model=Assessment, tags=["Assessment Generation"])
async def generate_assessment_from_config(
    config_id: str = Path(..., description="Assessment configuration ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Assessment:
    """
    Generate a sample assessment from a configuration.
    
    Args:
        config_id: The assessment configuration ID
        current_user: Authenticated teacher
        
    Returns:
        Generated Assessment with sample questions
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Get the configuration by ID directly
        config = await enhanced_assessment_service.get_assessment_config_by_id(
            config_id=config_id,
            teacher_uid=teacher_uid
        )
        
        if not config:
            raise HTTPException(
                status_code=404,
                detail="Assessment configuration not found"
            )
        
        # Generate the assessment using RAG and AI
        assessment = await enhanced_assessment_service.create_rag_assessment(config)
        
        logger.info(f"Generated assessment {assessment.assessment_id} from config {config_id}")
        return assessment
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to generate assessment from config {config_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate assessment: {str(e)}"
        )

@router.get("/assessments/{assessment_id}", response_model=Assessment, tags=["Assessment Management"])
async def get_assessment(
    assessment_id: str = Path(..., description="Assessment ID"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Assessment:
    """
    Get a specific assessment by ID.
    
    Args:
        assessment_id: The assessment ID
        current_user: Authenticated teacher
        
    Returns:
        Assessment object
    """
    teacher_uid = current_user["uid"]
    
    try:
        assessment = await enhanced_assessment_service.get_assessment_by_id(assessment_id)
        
        if not assessment:
            raise HTTPException(
                status_code=404,
                detail="Assessment not found"
            )
        
        # Verify the assessment belongs to this teacher
        if assessment.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=403,
                detail="Access denied: Assessment does not belong to this teacher"
            )
        
        return assessment
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get assessment {assessment_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve assessment: {str(e)}"
        )

# ====================================================================
# Helper Endpoints
# ====================================================================

@router.get("/topics/{subject}/{grade}", response_model=List[str], tags=["Assessment Helpers"])
async def get_available_topics(
    subject: str = Path(..., description="Subject name"),
    grade: int = Path(..., ge=1, le=12, description="Grade level"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[str]:
    """
    Get available topics for a subject and grade.
    
    Args:
        subject: Subject name
        grade: Grade level (1-12)
        current_user: Authenticated teacher
        
    Returns:
        List of available topic names
    """
    teacher_uid = current_user["uid"]
    
    try:
        topics = await enhanced_assessment_service.get_available_topics(
            subject=subject,
            grade=grade,
            teacher_uid=teacher_uid
        )
        
        return topics
        
    except Exception as e:
        logger.error(f"Failed to get topics for {subject} grade {grade}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get available topics: {str(e)}"
        )

@router.get("/summary", tags=["Assessment Management"])
async def get_assessment_summary(
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Get summary of teacher's assessments and configurations.
    
    Args:
        current_user: Authenticated teacher
        
    Returns:
        Summary statistics
    """
    teacher_uid = current_user["uid"]
    
    try:
        configs = await enhanced_assessment_service.get_teacher_assessment_configs(teacher_uid)
        
        # Calculate statistics
        total_configs = len(configs)
        subjects = list(set(config.subject for config in configs))
        difficulty_distribution = {}
        
        for config in configs:
            difficulty = config.difficulty_level
            difficulty_distribution[difficulty] = difficulty_distribution.get(difficulty, 0) + 1
        
        return {
            "total_configurations": total_configs,
            "unique_subjects": subjects,
            "difficulty_distribution": difficulty_distribution,
            "recent_configs": [
                {
                    "config_id": config.config_id,
                    "name": config.name,
                    "subject": config.subject,
                    "grade": config.target_grade,
                    "created_at": config.created_at
                }
                for config in configs[:5]  # Most recent 5
            ]
        }
        
    except Exception as e:
        logger.error(f"Failed to get assessment summary for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get assessment summary: {str(e)}"
        )


================================================================================
File: app/api/v1/students.py
Size: 9.75 kB
================================================================================

# FILE: app/api/v1/students.py

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Query
from typing import Dict, Any, List, Optional
import logging

from app.models.student import StudentProfile, StudentBatchUploadResponse
from app.core.auth import get_current_user
from app.services.student_service import student_service

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/upload-csv", response_model=StudentBatchUploadResponse, tags=["Student Management"])
async def upload_students_csv(
    file: UploadFile = File(..., description="CSV file with student data"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> StudentBatchUploadResponse:
    """
    Upload students from CSV file.
    
    CSV format should include columns:
    - first_name: Student's first name
    - last_name: Student's last name  
    - grade: Grade level (1-12)
    - password: Default password (min 6 characters)
    
    All students will be assigned to all subjects that the teacher handles.
    If a student already exists (same name + grade), their profile will be updated.
    
    Args:
        file: CSV file with student data
        current_user: The authenticated teacher
        
    Returns:
        StudentBatchUploadResponse with upload results and summary
        
    Raises:
        HTTPException: If CSV format is invalid or upload fails
    """
    teacher_uid = current_user["uid"]
    
    # TODO: Get teacher's subjects from their profile
    # For now, using a placeholder - this should be retrieved from teacher profile
    teacher_subjects = ["Mathematics", "Science", "English"]  # Placeholder
    
    try:
        logger.info(f"Processing student CSV upload for teacher {teacher_uid}")
        
        result = await student_service.upload_students_csv(
            file=file,
            teacher_uid=teacher_uid,
            teacher_subjects=teacher_subjects
        )
        
        logger.info(f"CSV upload completed: {result.upload_summary}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Student CSV upload failed for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to upload students: {str(e)}"
        )

@router.get("/", response_model=List[StudentProfile], tags=["Student Management"])
async def get_my_students(
    grade: Optional[int] = Query(None, ge=1, le=12, description="Filter by grade level"),
    subject: Optional[str] = Query(None, description="Filter by subject"),
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> List[StudentProfile]:
    """
    Get all students managed by the current teacher.
    
    Args:
        grade: Optional filter by grade level
        subject: Optional filter by subject enrollment
        current_user: The authenticated teacher
        
    Returns:
        List of StudentProfile objects
        
    Raises:
        HTTPException: If retrieval fails
    """
    teacher_uid = current_user["uid"]
    
    try:
        logger.info(f"Retrieving students for teacher {teacher_uid}")
        
        students = await student_service.get_teacher_students(
            teacher_uid=teacher_uid,
            grade_filter=grade,
            subject_filter=subject
        )
        
        logger.info(f"Retrieved {len(students)} students for teacher {teacher_uid}")
        return students
        
    except Exception as e:
        logger.error(f"Failed to retrieve students for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve students: {str(e)}"
        )

@router.get("/{student_id}", response_model=StudentProfile, tags=["Student Management"])
async def get_student_details(
    student_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> StudentProfile:
    """
    Get detailed information about a specific student.
    
    Args:
        student_id: The student's unique identifier
        current_user: The authenticated teacher
        
    Returns:
        StudentProfile with detailed information
        
    Raises:
        HTTPException: If student not found or access denied
    """
    teacher_uid = current_user["uid"]
    
    try:
        student = await student_service.get_student_by_id(student_id)
        
        # Verify the student belongs to this teacher
        if student.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied: Student does not belong to this teacher"
            )
        
        return student
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get student {student_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve student: {str(e)}"
        )

@router.put("/{student_id}/subjects", tags=["Student Management"])
async def update_student_subjects(
    student_id: str,
    subjects: List[str],
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, str]:
    """
    Update the subjects a student is enrolled in.
    
    Args:
        student_id: The student's unique identifier
        subjects: List of subject names to enroll the student in
        current_user: The authenticated teacher
        
    Returns:
        Success message
        
    Raises:
        HTTPException: If student not found or access denied
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Verify the student belongs to this teacher
        student = await student_service.get_student_by_id(student_id)
        if student.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied: Student does not belong to this teacher"
            )
        
        await student_service.update_student_subjects(student_id, subjects)
        
        logger.info(f"Updated subjects for student {student_id}: {subjects}")
        return {"message": f"Successfully updated subjects for {student.first_name} {student.last_name}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update subjects for student {student_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update student subjects: {str(e)}"
        )

@router.delete("/{student_id}", tags=["Student Management"])
async def deactivate_student(
    student_id: str,
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, str]:
    """
    Deactivate a student profile (soft delete).
    
    Args:
        student_id: The student's unique identifier
        current_user: The authenticated teacher
        
    Returns:
        Success message
        
    Raises:
        HTTPException: If student not found or access denied
    """
    teacher_uid = current_user["uid"]
    
    try:
        # Verify the student belongs to this teacher
        student = await student_service.get_student_by_id(student_id)
        if student.teacher_uid != teacher_uid:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied: Student does not belong to this teacher"
            )
        
        await student_service.deactivate_student(student_id)
        
        logger.info(f"Deactivated student {student_id}")
        return {"message": f"Successfully deactivated {student.first_name} {student.last_name}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to deactivate student {student_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to deactivate student: {str(e)}"
        )

@router.get("/stats/summary", tags=["Student Management"])
async def get_student_summary(
    current_user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    Get summary statistics for the teacher's students.
    
    Args:
        current_user: The authenticated teacher
        
    Returns:
        Dictionary with student statistics
    """
    teacher_uid = current_user["uid"]
    
    try:
        students = await student_service.get_teacher_students(teacher_uid)
        
        # Calculate statistics
        total_students = len(students)
        grade_distribution = {}
        subject_enrollment = {}
        
        for student in students:
            # Grade distribution
            grade = student.grade
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
            
            # Subject enrollment
            for subject in student.subjects:
                subject_enrollment[subject] = subject_enrollment.get(subject, 0) + 1
        
        return {
            "total_students": total_students,
            "grade_distribution": grade_distribution,
            "subject_enrollment": subject_enrollment,
            "active_students": len([s for s in students if s.is_active])
        }
        
    except Exception as e:
        logger.error(f"Failed to get student summary for teacher {teacher_uid}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get student summary: {str(e)}"
        )


================================================================================
File: app/core/__init__.py
Size: 0 B
================================================================================



================================================================================
File: app/core/api_config.py
Size: 5.44 kB
================================================================================

"""
Production API Configuration
Controls which endpoints are exposed in production vs development environments
"""

import os
from typing import Dict, Set

# Environment-based endpoint control
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

class APIConfig:
    """Configuration class for controlling API endpoint visibility"""
    
    @staticmethod
    def get_core_teacher_endpoints() -> Set[str]:
        """Get the core endpoints needed for teacher workflow"""
        return {
            # Authentication
            "/v1/auth/signup",
            "/v1/auth/me", 
            "/v1/auth/me/profile",
            
            # Student Management
            "/v1/students/upload-csv",
            "/v1/students/",
            "/v1/students/{student_id}",
            
            # Learning Path & Assessment
            "/v1/learning/start-monitoring",
            "/v1/learning/analyze-assessment", 
            "/v1/learning/generate-learning-path",
            "/v1/learning/student/{student_id}/progress",
            "/v1/learning/student/{student_id}/learning-paths",
            "/v1/learning/learning-path/{path_id}",
            "/v1/learning/adapt-learning-path/{path_id}",
            
            # Lesson Management
            "/v1/lessons/lessons/create-from-step",
            "/v1/lessons/lessons/{lesson_id}",
            "/v1/lessons/lessons/{lesson_id}/progress",
            
            # Chatbot
            "/v1/lessons/lessons/{lesson_id}/chat/start",
            "/v1/lessons/lessons/chat/{session_id}/message",
            
            # Analytics
            "/v1/learning/teacher/learning-analytics", 
            "/v1/learning/student/{student_id}/learning-insights",
            
            # Health
            "/",
            "/health"
        }
    
    @staticmethod
    def get_advanced_endpoints() -> Set[str]:
        """Get advanced endpoints for power users"""
        return {
            "/v1/learning/monitoring-status",
            "/v1/learning/learning-path/{path_id}/update-progress",
            "/v1/lessons/lessons/student/{student_id}",
            "/v1/lessons/lessons/{lesson_id}/analytics",
            "/v1/lessons/lessons/{lesson_id}/regenerate-slide",
            "/v1/students/stats/summary"
        }
    
    @staticmethod
    def get_development_endpoints() -> Set[str]:
        """Get development/debugging endpoints"""
        return {
            "/v1/agent/invoke",
            "/v1/agent/health",
            "/v1/documents/upload",
            "/v1/documents/list",
            "/v1/assessments/configs",
            "/v1/assessments/rag/documents/upload",
            "/debug/trace/{event_id}",
            "/list-apps"
        }
    
    @staticmethod
    def should_include_endpoint(path: str) -> bool:
        """Determine if an endpoint should be included based on environment"""
        core_endpoints = APIConfig.get_core_teacher_endpoints()
        advanced_endpoints = APIConfig.get_advanced_endpoints()
        dev_endpoints = APIConfig.get_development_endpoints()
        
        # Always include core endpoints
        if path in core_endpoints:
            return True
            
        # Include advanced endpoints in development
        if ENVIRONMENT == "development" and path in advanced_endpoints:
            return True
            
        # Include dev endpoints only in development
        if ENVIRONMENT == "development" and path in dev_endpoints:
            return True
            
        return False

# Create different configurations for different use cases
TEACHER_WORKFLOW_CONFIG = {
    "title": "Edvance AI - Teacher Workflow API",
    "description": """
    ## 🎓 Core Teacher Workflow API
    
    **Streamlined documentation showing only essential endpoints for the teacher journey.**
    
    ### 🚀 Complete Teacher Workflow (22 Core Endpoints)
    
    1. **👤 Authentication** (3 endpoints)
       - Sign up, profile management, authentication
    
    2. **👥 Student Management** (3 endpoints) 
       - Upload student data, view student profiles
    
    3. **🤖 AI Learning Paths** (7 endpoints)
       - Automated monitoring, assessment analysis, path generation
    
    4. **📚 Lesson Generation** (3 endpoints)
       - Ultra-fast lesson creation (~27 seconds), progress tracking
    
    5. **💬 Chatbot Support** (2 endpoints)
       - Intelligent tutoring, context-aware responses
    
    6. **📊 Analytics & Insights** (2 endpoints)
       - Teacher dashboard, student learning insights
    
    7. **⚕️ Health Checks** (2 endpoints)
       - System status and health monitoring
    
    ### ⚡ Key Features
    - **Ultra-Fast**: Lessons generated in ~27 seconds
    - **Automated**: 95% of tasks handled by AI agents  
    - **Intelligent**: Context-aware chatbot support
    - **Adaptive**: Learning paths adjust to student progress
    - **Scalable**: Handle 30+ students effortlessly
    
    ### 📚 Full Documentation
    See `COMPLETE_TEACHER_JOURNEY.md` for the complete teacher workflow guide.
    
    ---
    *Showing {count} core endpoints (filtered from 73+ total)*
    """,
    "version": "1.0.0",
    "contact": {
        "name": "Edvance AI Support",
        "email": "support@edvance.ai"
    }
}

FULL_API_CONFIG = {
    "title": "Edvance AI - Complete API",
    "description": "Full API documentation with all available endpoints",
    "version": "1.0.0"
}


================================================================================
File: app/core/app_factory.py
Size: 3.04 kB
================================================================================

# FILE: app/core/app_factory.py

from fastapi import FastAPI
from google.adk.cli.fast_api import get_fast_api_app

from app.core.firebase import initialize_firebase
from app.core.middleware import configure_middleware
from app.core.streamlined_docs import configure_streamlined_docs
from app.api.v1 import auth as auth_router
from app.api.v1 import agent as agent_router
from app.api.v1 import documents as documents_router
from app.api.v1 import students as students_router
from app.api.v1 import simple_assessments as assessments_router
from app.api.v1 import rag_assessments as rag_router
from app.api.v1 import personalized_learning as learning_router
from app.api.v1 import lessons as lessons_router

def create_app() -> FastAPI:
    """
    Create and configure the FastAPI application.
    
    Returns:
        The configured FastAPI application
    """
    # Initialize Firebase before anything else
    initialize_firebase()
    
    # Create the base FastAPI app from the orchestrator agent directory
    app = get_fast_api_app(agents_dir="./app/agents", web=True)
    
    # Configure middleware
    configure_middleware(app)
    
    # Include routers
    app.include_router(auth_router.router, prefix="/v1/auth", tags=["Authentication"])
    app.include_router(agent_router.router, prefix="/v1/agent", tags=["Agent"])
    app.include_router(documents_router.router, prefix="/v1/documents", tags=["Documents"])
    app.include_router(students_router.router, prefix="/v1/students", tags=["Students"])
    app.include_router(assessments_router.router, prefix="/v1/assessments", tags=["Assessments"])
    app.include_router(rag_router.router, prefix="/v1/assessments", tags=["RAG Assessments"])
    app.include_router(learning_router.router, prefix="/v1/learning", tags=["Personalized Learning"])
    app.include_router(lessons_router.router, prefix="/v1/lessons", tags=["Lessons"])
    
    # Add a root health check
    @app.get("/", tags=["Health"])
    async def root():
        return {
            "message": "Edvance AI - Core Teacher Workflow API",
            "status": "running",
            "version": "1.0.0",
            "description": "Streamlined API for essential teacher journey - only core endpoints shown",
            "total_core_endpoints": 22,
            "documentation": "See COMPLETE_TEACHER_JOURNEY.md for full workflow guide"
        }
    
    @app.get("/health", tags=["Health"])
    async def health_check():
        return {
            "status": "healthy",
            "message": "Core teacher workflow API operational",
            "core_features": [
                "Authentication & Profile Management",
                "Student Data Management", 
                "AI Learning Path Generation",
                "Ultra-Fast Lesson Creation",
                "Intelligent Chatbot Support",
                "Real-Time Analytics"
            ]
        }
    
    # Configure streamlined documentation (only core teacher workflow endpoints)
    configure_streamlined_docs(app)
    
    return app


================================================================================
File: app/core/auth.py
Size: 2.51 kB
================================================================================

# FILE: app/core/auth.py

from fastapi import Depends, HTTPException, status
# === CHANGE 1: Import HTTPBearer and HTTPAuthorizationCredentials ===
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from app.core.firebase import firebase_auth
from firebase_admin import auth
import logging

# Set up logging
logger = logging.getLogger(__name__)

# === CHANGE 2: Create an instance of HTTPBearer ===
# This scheme is simpler and expects a token to be provided directly.
bearer_scheme = HTTPBearer()

# === CHANGE 3: Update the function signature to use the new scheme ===
def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)) -> dict:
    """
    Dependency to verify Firebase ID token and get the current user.

    Args:
        credentials (HTTPAuthorizationCredentials): The bearer token from the Authorization header,
                                                    handled by FastAPI's HTTPBearer scheme.

    Raises:
        HTTPException: 401 Unauthorized if the token is invalid, expired, or revoked.
        HTTPException: 500 Internal Server Error for other verification failures.

    Returns:
        dict: The decoded token payload, containing user info like uid, email, etc.
    """
    # === CHANGE 4: Get the token from the credentials object ===
    token = credentials.credentials
    
    try:
        # Verify the token against the Firebase Auth service.
        decoded_token = firebase_auth.verify_id_token(token, check_revoked=True)
        return decoded_token
    except auth.RevokedIdTokenError:
        logger.warning("Authentication failed: Token has been revoked.")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token has been revoked. Please sign in again.",
            headers={"WWW-Authenticate": "Bearer"},
        )
    except auth.InvalidIdTokenError as e:
        logger.warning(f"Authentication failed: Invalid token. Error: {e}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials.",
            headers={"WWW-Authenticate": "Bearer"},
        )
    except Exception as e:
        logger.error(f"An unexpected error occurred during token verification: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Could not validate credentials.",
            headers={"WWW-Authenticate": "Bearer"},
        )

================================================================================
File: app/core/config.py
Size: 1.24 kB
================================================================================

# FILE: app/core/config.py

from pydantic_settings import BaseSettings
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Settings(BaseSettings):
    """
    Manages application settings and environment variables.
    """
    # Firebase/Google Cloud Settings
    # The GOOGLE_APPLICATION_CREDENTIALS is automatically used by Google Cloud libraries.
    # We just need to ensure it's set in the environment.
    google_application_credentials: str = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    firebase_project_id: str
    firebase_storage_bucket: str

    # Gemini Model Settings
    gemini_model_name: str = "gemini-2.5-pro"
    
    # Google GenAI Configuration
    google_genai_use_vertexai: bool = False
    google_api_key: str = ""
    google_cloud_project: str = ""
    google_cloud_location: str = "us-west1"
    
    # Vertex AI RAG Configuration
    vertex_ai_search_engine_id: str = ""
    vertex_ai_datastore_id: str = "teacher-documents-datastore"

    class Config:
        # This tells Pydantic to load variables from a .env file
        env_file = ".env"
        env_file_encoding = 'utf-8'

# Create a single, reusable instance of the settings
settings = Settings()

================================================================================
File: app/core/firebase.py
Size: 1.16 kB
================================================================================

# FILE: app/core/firebase.py

import firebase_admin
from firebase_admin import credentials, firestore, auth, storage
from app.core.config import settings
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def initialize_firebase():
    """
    Initializes the Firebase Admin SDK if it hasn't been initialized yet.
    """
    if not firebase_admin._apps:
        try:
            logger.info("Initializing Firebase Admin SDK...")
            cred = credentials.Certificate(settings.google_application_credentials)
            firebase_admin.initialize_app(cred, {
                'projectId': settings.firebase_project_id,
                'storageBucket': settings.firebase_storage_bucket
            })
            logger.info("Firebase Admin SDK initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize Firebase Admin SDK: {e}")
            raise

# Initialize Firebase on application startup
initialize_firebase()

# Create easy-to-import instances of Firebase services
db = firestore.client()
firebase_auth = auth
storage_bucket = storage.bucket()

================================================================================
File: app/core/middleware.py
Size: 567 B
================================================================================

# FILE: app/core/middleware.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

def configure_middleware(app: FastAPI) -> None:
    """Configure all middleware for the FastAPI application."""
    
    # CORS Middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # In production, specify actual origins
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Add other middleware here as needed
    # Example: app.add_middleware(SomeOtherMiddleware)


================================================================================
File: app/core/streamlined_docs.py
Size: 4.34 kB
================================================================================

"""
Core Teacher Workflow API
Streamlined API documentation showing only essential endpoints for the teacher journey
"""

from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi
from typing import Dict, Any
from app.core.api_config import APIConfig, TEACHER_WORKFLOW_CONFIG

def create_streamlined_openapi_schema(app: FastAPI) -> Dict[str, Any]:
    """Create a streamlined OpenAPI schema with only core teacher workflow endpoints."""
    
    # Get core endpoints for the essential teacher workflow
    core_endpoints = APIConfig.get_core_teacher_endpoints()
    
    # Get the full OpenAPI schema
    full_schema = get_openapi(
        title=TEACHER_WORKFLOW_CONFIG["title"],
        version=TEACHER_WORKFLOW_CONFIG["version"],
        description=TEACHER_WORKFLOW_CONFIG["description"].format(count=len(core_endpoints)),
        routes=app.routes,
        contact=TEACHER_WORKFLOW_CONFIG.get("contact")
    )
    
    # Filter paths to only include core endpoints
    filtered_paths = {}
    for path, path_info in full_schema.get("paths", {}).items():
        # Check if this path matches any of our core endpoints
        if any(core_path == path or ('{' in core_path and _path_matches_pattern(path, core_path)) 
               for core_path in core_endpoints):
            filtered_paths[path] = path_info
    
    # Update the schema with filtered paths
    full_schema["paths"] = filtered_paths
    
    # Add custom tags for better organization
    full_schema["tags"] = [
        {
            "name": "Authentication",
            "description": "👤 Teacher account creation and profile management",
            "externalDocs": {
                "description": "Authentication Guide",
                "url": "#phase-1-teacher-authentication--setup"
            }
        },
        {
            "name": "Students", 
            "description": "👥 Student data management and upload",
            "externalDocs": {
                "description": "Student Management Guide", 
                "url": "#phase-2-student-assessment--performance-analysis"
            }
        },
        {
            "name": "Personalized Learning",
            "description": "🤖 AI-powered learning path generation and monitoring (Ultra-fast ~27s generation)",
            "externalDocs": {
                "description": "Learning Path Guide",
                "url": "#phase-4-personalized-learning-path-creation"
            }
        },
        {
            "name": "Lessons",
            "description": "📚 Ultra-fast lesson creation and chatbot support",
            "externalDocs": {
                "description": "Lesson Generation Guide",
                "url": "#phase-5-ultra-fast-ai-lesson-generation"
            }
        },
        {
            "name": "Health",
            "description": "⚕️ System health and status checks"
        }
    ]
    
    # Add info about the streamlined nature
    full_schema["info"]["x-streamlined"] = {
        "core_endpoints_count": len(core_endpoints),
        "total_available_endpoints": "73+",
        "filtering_applied": True,
        "workflow_focus": "Essential teacher journey only"
    }
    
    return full_schema

def _path_matches_pattern(actual_path: str, pattern_path: str) -> bool:
    """Check if an actual path matches a pattern with path parameters."""
    if '{' not in pattern_path:
        return actual_path == pattern_path
    
    # Simple pattern matching for path parameters
    pattern_parts = pattern_path.split('/')
    actual_parts = actual_path.split('/')
    
    if len(pattern_parts) != len(actual_parts):
        return False
    
    for pattern_part, actual_part in zip(pattern_parts, actual_parts):
        if pattern_part.startswith('{') and pattern_part.endswith('}'):
            # This is a path parameter, it matches any value
            continue
        elif pattern_part != actual_part:
            return False
    
    return True

def configure_streamlined_docs(app: FastAPI):
    """Configure the FastAPI app to show only core teacher workflow endpoints."""
    
    def custom_openapi():
        if app.openapi_schema:
            return app.openapi_schema
        
        app.openapi_schema = create_streamlined_openapi_schema(app)
        return app.openapi_schema
    
    app.openapi = custom_openapi


================================================================================
File: app/core/vertex.py
Size: 1.37 kB
================================================================================

# FILE: app/core/vertex.py

import logging
from typing import Optional
import vertexai
from vertexai.generative_models import GenerativeModel

from app.core.config import settings

logger = logging.getLogger(__name__)

def get_vertex_model(model_name: str = "gemini-2.5-flash") -> GenerativeModel:
    """
    Initialize and return a Vertex AI model instance.
    
    Args:
        model_name: Name of the model to use
        
    Returns:
        Configured Vertex AI GenerativeModel instance
    """
    try:
        # Initialize Vertex AI
        vertexai.init(
            project=settings.google_cloud_project,
            location=settings.google_cloud_location or "us-central1"
        )
        
        # Create and return the model
        model = GenerativeModel(model_name)
        
        logger.info(f"Vertex AI model '{model_name}' initialized successfully")
        return model
        
    except Exception as e:
        logger.error(f"Failed to initialize Vertex AI model: {e}")
        raise

def get_vertex_model_async(model_name: str = "gemini-2.5-flash") -> GenerativeModel:
    """
    Get a Vertex AI Generative Model instance for async operations.
    
    Args:
        model_name: Name of the model to use
        
    Returns:
        GenerativeModel instance configured for async use
    """
    return get_vertex_model(model_name)


================================================================================
File: app/get_id_token.html
Size: 3.45 kB
================================================================================

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-g">
    <title>Firebase ID Token Generator</title>
    <style>
        body { font-family: sans-serif; padding: 2em; line-height: 1.6; background-color: #f4f4f4; }
        .container { max-width: 600px; margin: auto; background: white; padding: 2em; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        input { width: 95%; padding: 10px; margin-bottom: 10px; border: 1px solid #ccc; border-radius: 4px; }
        button { padding: 10px 15px; background-color: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; }
        #token-display { margin-top: 20px; padding: 10px; background: #e9ecef; border: 1px solid #ced4da; border-radius: 4px; word-wrap: break-word; }
        #status { font-weight: bold; }
    </style>
</head>
<body>
    <div class="container">
        <h2>Firebase ID Token Generator</h2>
        <p>Use the user you created via the API to log in and get an ID token.</p>
        
        <input type="email" id="email" placeholder="Email">
        <input type="password" id="password" placeholder="Password">
        <button onclick="login()">Get ID Token</button>

        <p id="status"></p>
        <div id="token-display">Your ID token will appear here...</div>
    </div>

    <script type="module">
        import { initializeApp } from "https://www.gstatic.com/firebasejs/9.6.1/firebase-app.js";
        import { getAuth, signInWithEmailAndPassword } from "https://www.gstatic.com/firebasejs/9.6.1/firebase-auth.js";

        // =========================================================================================
        // === PASTE YOUR FIREBASE CONFIG OBJECT HERE ===
        const firebaseConfig = {
            apiKey: "AIzaSyDYIKO6nZa0VH1Dg9piIKzcJrVJCub6I6s",
            authDomain: "skilful-berm-466510-h9.firebaseapp.com",
            projectId: "skilful-berm-466510-h9",
            storageBucket: "skilful-berm-466510-h9.firebasestorage.app",
            messagingSenderId: "808751785929",
            appId: "1:808751785929:web:9fb391fc5f44a62adfea18",
            measurementId: "G-G74C3HHJZ3"
        };

        // =========================================================================================
        
        const app = initializeApp(firebaseConfig);
        const auth = getAuth(app);

        window.login = async function() {
            const email = document.getElementById('email').value;
            const password = document.getElementById('password').value;
            const statusEl = document.getElementById('status');
            const tokenEl = document.getElementById('token-display');

            statusEl.textContent = 'Logging in...';
            tokenEl.textContent = '';

            try {
                const userCredential = await signInWithEmailAndPassword(auth, email, password);
                const user = userCredential.user;
                const idToken = await user.getIdToken(true); // 'true' forces a refresh

                statusEl.textContent = 'Success! Token generated.';
                statusEl.style.color = 'green';
                tokenEl.textContent = idToken;
                
            } catch (error) {
                statusEl.textContent = `Error: ${error.message}`;
                statusEl.style.color = 'red';
                tokenEl.textContent = 'Failed to get token.';
            }
        }
    </script>
</body>
</html>

================================================================================
File: app/main.py
Size: 359 B
================================================================================

# FILE: app/main.py

"""
Edvance AI Backend - Main Application Entry Point

A clean and modular FastAPI application for AI-powered educational content management.
"""

from app.core.app_factory import create_app

# Create the FastAPI application
app = create_app()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

================================================================================
File: app/models/__init__.py
Size: 971 B
================================================================================

# FILE: app/models/__init__.py

"""Request and response models for the application."""

from .requests import (
    # Authentication models
    UserCreate,
    UserLogin,
    Token,
    UserInDB,
    UserProfileUpdate,
    
    # Agent interaction models
    AgentPrompt,
    AgentResponse,
    HealthResponse,
    
    # Document management models
    DocumentUploadResponse,
    DocumentIndexingStatus,
    DocumentMetadata,
    
    # Content generation models
    LocalContentRequest,
    GeneratedContentResponse,
)

__all__ = [
    # Authentication models
    "UserCreate",
    "UserLogin", 
    "Token",
    "UserInDB",
    "UserProfileUpdate",
    
    # Agent interaction models
    "AgentPrompt",
    "AgentResponse",
    "HealthResponse",
    
    # Document management models
    "DocumentUploadResponse",
    "DocumentIndexingStatus", 
    "DocumentMetadata",
    
    # Content generation models
    "LocalContentRequest",
    "GeneratedContentResponse",
]


================================================================================
File: app/models/learning_models.py
Size: 8.46 kB
================================================================================

# FILE: app/models/learning_models.py

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum

class DifficultyLevel(str, Enum):
    """Difficulty levels for learning content."""
    BEGINNER = "beginner"
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"
    ADVANCED = "advanced"

class LearningObjectiveType(str, Enum):
    """Types of learning objectives based on Bloom's taxonomy."""
    REMEMBER = "remember"
    UNDERSTAND = "understand"
    APPLY = "apply"
    ANALYZE = "analyze"
    EVALUATE = "evaluate"
    CREATE = "create"

class KnowledgeGap(BaseModel):
    """Represents a knowledge gap identified from assessment results."""
    gap_id: str = Field(..., description="Unique gap identifier")
    student_id: str = Field(..., description="Student who has this gap")
    subject: str = Field(..., description="Subject area")
    topic: str = Field(..., description="Specific topic")
    subtopic: Optional[str] = Field(None, description="More specific subtopic")
    difficulty_level: DifficultyLevel = Field(..., description="Difficulty level where gap exists")
    learning_objective: LearningObjectiveType = Field(..., description="Type of learning objective")
    
    # Performance metrics
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Confidence in gap identification")
    severity_score: float = Field(..., ge=0.0, le=1.0, description="How severe this gap is")
    frequency: int = Field(default=1, description="How many times this gap appeared")
    
    # Context
    source_assessments: List[str] = Field(default=[], description="Assessment IDs that identified this gap")
    related_questions: List[str] = Field(default=[], description="Question IDs that revealed this gap")
    prerequisites: List[str] = Field(default=[], description="Topics that should be learned first")
    
    identified_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated: datetime = Field(default_factory=datetime.utcnow)

class LearningStep(BaseModel):
    """A single step in a learning path."""
    step_id: str = Field(..., description="Unique step identifier")
    step_number: int = Field(..., description="Order in the learning path")
    title: str = Field(..., description="Step title")
    description: str = Field(..., description="What this step covers")
    
    # Content
    subject: str = Field(..., description="Subject area")
    topic: str = Field(..., description="Main topic")
    subtopic: Optional[str] = Field(None, description="Specific subtopic")
    difficulty_level: DifficultyLevel = Field(..., description="Difficulty level")
    learning_objective: LearningObjectiveType = Field(..., description="Learning goal")
    
    # Resources
    content_type: str = Field(..., description="Type of content (video, reading, practice, etc.)")
    content_url: Optional[str] = Field(None, description="Link to content")
    content_text: Optional[str] = Field(None, description="Text content or instructions")
    estimated_duration_minutes: int = Field(default=15, description="Expected time to complete")
    
    # Dependencies
    prerequisites: List[str] = Field(default=[], description="Step IDs that must be completed first")
    addresses_gaps: List[str] = Field(default=[], description="Knowledge gap IDs this step addresses")
    
    # Completion tracking
    is_completed: bool = Field(default=False)
    completed_at: Optional[datetime] = Field(None)
    performance_score: Optional[float] = Field(None, description="How well student performed on this step")

class LearningPath(BaseModel):
    """A personalized learning path for a student."""
    path_id: str = Field(..., description="Unique path identifier")
    student_id: str = Field(..., description="Student this path is for")
    teacher_uid: str = Field(..., description="Teacher who created/assigned this path")
    
    # Path metadata
    title: str = Field(..., description="Path title")
    description: str = Field(..., description="What this path accomplishes")
    subject: str = Field(..., description="Main subject")
    target_grade: int = Field(..., ge=1, le=12, description="Target grade level")
    
    # Learning goals
    learning_goals: List[str] = Field(default=[], description="What student will achieve")
    addresses_gaps: List[str] = Field(default=[], description="Knowledge gap IDs this path addresses")
    
    # Path structure
    steps: List[LearningStep] = Field(default=[], description="Ordered list of learning steps")
    total_estimated_duration_minutes: int = Field(default=0, description="Total expected time")
    
    # Progress tracking
    current_step: int = Field(default=0, description="Current step index")
    completion_percentage: float = Field(default=0.0, ge=0.0, le=100.0)
    started_at: Optional[datetime] = Field(None)
    completed_at: Optional[datetime] = Field(None)
    
    # Personalization metadata
    generation_method: str = Field(default="ai_generated", description="How this path was created")
    source_assessments: List[str] = Field(default=[], description="Assessments that informed this path")
    adaptation_history: List[Dict[str, Any]] = Field(default=[], description="How path was modified over time")
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated: datetime = Field(default_factory=datetime.utcnow)

class StudentPerformance(BaseModel):
    """Student performance on an assessment."""
    performance_id: str = Field(..., description="Unique performance record ID")
    student_id: str = Field(..., description="Student identifier")
    assessment_id: str = Field(..., description="Assessment taken")
    
    # Overall performance
    total_questions: int = Field(..., description="Total questions in assessment")
    correct_answers: int = Field(..., description="Number of correct answers")
    score_percentage: float = Field(..., ge=0.0, le=100.0, description="Overall score percentage")
    time_taken_minutes: int = Field(..., description="Time taken to complete")
    
    # Detailed performance
    question_performances: List[Dict[str, Any]] = Field(default=[], description="Performance on each question")
    topic_scores: Dict[str, float] = Field(default={}, description="Score by topic")
    difficulty_scores: Dict[str, float] = Field(default={}, description="Score by difficulty level")
    learning_objective_scores: Dict[str, float] = Field(default={}, description="Score by learning objective")
    
    # Analysis
    strengths: List[str] = Field(default=[], description="Topics/skills student performed well on")
    weaknesses: List[str] = Field(default=[], description="Topics/skills needing improvement")
    recommended_focus_areas: List[str] = Field(default=[], description="Areas to focus on next")
    
    completed_at: datetime = Field(default_factory=datetime.utcnow)

class LearningRecommendation(BaseModel):
    """AI-generated learning recommendation for a student."""
    recommendation_id: str = Field(..., description="Unique recommendation ID")
    student_id: str = Field(..., description="Student this is for")
    
    # Recommendation content
    title: str = Field(..., description="Recommendation title")
    description: str = Field(..., description="What this recommendation suggests")
    rationale: str = Field(..., description="Why this is recommended")
    
    # Specifics
    recommended_action: str = Field(..., description="Specific action to take")
    content_type: str = Field(..., description="Type of content to study")
    difficulty_level: DifficultyLevel = Field(..., description="Recommended difficulty")
    estimated_duration_minutes: int = Field(..., description="Expected time investment")
    
    # Priority
    priority_score: float = Field(..., ge=0.0, le=1.0, description="How important this recommendation is")
    urgency_level: str = Field(..., description="How urgent this is (low/medium/high)")
    
    # Context
    based_on_assessments: List[str] = Field(default=[], description="Assessments that informed this")
    addresses_gaps: List[str] = Field(default=[], description="Knowledge gaps this addresses")
    
    # Status
    is_active: bool = Field(default=True)
    is_completed: bool = Field(default=False)
    completed_at: Optional[datetime] = Field(None)
    
    created_at: datetime = Field(default_factory=datetime.utcnow)


================================================================================
File: app/models/lesson_models.py
Size: 9.55 kB
================================================================================

# FILE: app/models/lesson_models.py

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from enum import Enum

class SlideType(str, Enum):
    """Types of slides in a lesson."""
    INTRODUCTION = "introduction"
    CONCEPT_EXPLANATION = "concept_explanation"
    EXAMPLE = "example"
    PRACTICE = "practice"
    INTERACTIVE_EXERCISE = "interactive_exercise"
    SUMMARY = "summary"
    ASSESSMENT = "assessment"
    REFLECTION = "reflection"

class ContentElementType(str, Enum):
    """Types of content elements within a slide."""
    TEXT = "text"
    IMAGE = "image"
    VIDEO = "video"
    AUDIO = "audio"
    INTERACTIVE_WIDGET = "interactive_widget"
    QUIZ = "quiz"
    EXERCISE = "exercise"
    DIAGRAM = "diagram"
    CODE_SNIPPET = "code_snippet"

class InteractiveWidget(BaseModel):
    """Interactive widget within a slide."""
    widget_id: str = Field(..., description="Unique widget identifier")
    widget_type: str = Field(..., description="Type of widget (drag_drop, multiple_choice, fill_blank, etc.)")
    title: str = Field(..., description="Widget title")
    instructions: str = Field(..., description="Instructions for the student")
    content: Dict[str, Any] = Field(..., description="Widget-specific content and configuration")
    correct_answer: Optional[Dict[str, Any]] = Field(None, description="Correct answer for validation")
    hints: List[str] = Field(default=[], description="Hints to help students")
    points: int = Field(default=1, description="Points awarded for correct completion")

class ContentElement(BaseModel):
    """A content element within a slide."""
    element_id: str = Field(..., description="Unique element identifier")
    element_type: ContentElementType = Field(..., description="Type of content element")
    title: Optional[str] = Field(None, description="Element title")
    content: Union[str, Dict[str, Any]] = Field(..., description="Element content")
    position: int = Field(..., description="Position within the slide")
    
    # Interactive elements
    interactive_widget: Optional[InteractiveWidget] = Field(None, description="Interactive widget if applicable")
    
    # Media elements
    media_url: Optional[str] = Field(None, description="URL for media content")
    alt_text: Optional[str] = Field(None, description="Alternative text for accessibility")
    
    # Styling and layout
    styling: Dict[str, Any] = Field(default={}, description="Styling and layout options")

class LessonSlide(BaseModel):
    """A single slide in a lesson."""
    slide_id: str = Field(..., description="Unique slide identifier")
    slide_number: int = Field(..., description="Order in the lesson")
    slide_type: SlideType = Field(..., description="Type of slide")
    title: str = Field(..., description="Slide title")
    subtitle: Optional[str] = Field(None, description="Slide subtitle")
    
    # Content
    content_elements: List[ContentElement] = Field(default=[], description="Content elements in this slide")
    learning_objective: str = Field(..., description="What students will learn from this slide")
    
    # Navigation and timing
    estimated_duration_minutes: int = Field(default=5, description="Expected time to complete")
    prerequisites: List[str] = Field(default=[], description="Previous slide IDs that should be completed first")
    
    # Interactivity
    is_interactive: bool = Field(default=False, description="Whether slide requires student interaction")
    completion_criteria: Dict[str, Any] = Field(default={}, description="Criteria for slide completion")
    
    # Student progress
    is_completed: bool = Field(default=False)
    completed_at: Optional[datetime] = Field(None)
    student_responses: Dict[str, Any] = Field(default={}, description="Student responses to interactive elements")
    time_spent_seconds: int = Field(default=0, description="Time student spent on this slide")

class LessonContent(BaseModel):
    """Complete lesson content with slides and metadata."""
    lesson_id: str = Field(..., description="Unique lesson identifier")
    learning_step_id: str = Field(..., description="Associated learning step ID")
    student_id: str = Field(..., description="Student this lesson is for")
    teacher_uid: str = Field(..., description="Teacher who owns this lesson")
    
    # Lesson metadata
    title: str = Field(..., description="Lesson title")
    description: str = Field(..., description="What this lesson covers")
    subject: str = Field(..., description="Subject area")
    topic: str = Field(..., description="Main topic")
    subtopic: Optional[str] = Field(None, description="Specific subtopic")
    grade_level: int = Field(..., description="Target grade level")
    
    # Content structure
    slides: List[LessonSlide] = Field(default=[], description="Ordered list of slides")
    total_slides: int = Field(default=0, description="Total number of slides")
    
    # Learning context
    learning_objectives: List[str] = Field(default=[], description="Overall lesson objectives")
    prerequisite_knowledge: List[str] = Field(default=[], description="What students should know beforehand")
    key_concepts: List[str] = Field(default=[], description="Key concepts covered")
    
    # Progress tracking
    current_slide: int = Field(default=0, description="Current slide index")
    completion_percentage: float = Field(default=0.0, ge=0.0, le=100.0)
    started_at: Optional[datetime] = Field(None)
    completed_at: Optional[datetime] = Field(None)
    total_time_spent_minutes: int = Field(default=0)
    
    # Generation metadata
    generation_method: str = Field(default="ai_generated", description="How this lesson was created")
    content_source: List[str] = Field(default=[], description="Source materials used")
    personalization_factors: Dict[str, Any] = Field(default={}, description="Factors used for personalization")
    
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated: datetime = Field(default_factory=datetime.utcnow)

class ChatMessage(BaseModel):
    """A message in the lesson chatbot conversation."""
    message_id: str = Field(..., description="Unique message identifier")
    lesson_id: str = Field(..., description="Associated lesson ID")
    sender: str = Field(..., description="'student' or 'agent'")
    message: str = Field(..., description="Message content")
    message_type: str = Field(default="text", description="Type of message (text, image, code, etc.)")
    
    # Context
    current_slide_id: Optional[str] = Field(None, description="Slide student was on when asking")
    related_concept: Optional[str] = Field(None, description="Concept the message relates to")
    
    # Agent response metadata (for agent messages)
    confidence_score: Optional[float] = Field(None, description="Agent's confidence in response")
    sources: List[str] = Field(default=[], description="Sources used for response")
    suggested_actions: List[str] = Field(default=[], description="Suggested next steps")
    
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class LessonChatSession(BaseModel):
    """A chatbot session for a lesson."""
    session_id: str = Field(..., description="Unique session identifier")
    lesson_id: str = Field(..., description="Associated lesson ID")
    student_id: str = Field(..., description="Student in this session")
    
    # Conversation
    messages: List[ChatMessage] = Field(default=[], description="Conversation messages")
    
    # Session state
    is_active: bool = Field(default=True)
    started_at: datetime = Field(default_factory=datetime.utcnow)
    last_activity: datetime = Field(default_factory=datetime.utcnow)
    ended_at: Optional[datetime] = Field(None)
    
    # Analytics
    total_messages: int = Field(default=0)
    student_questions: int = Field(default=0)
    agent_responses: int = Field(default=0)
    session_duration_minutes: int = Field(default=0)

class LessonProgress(BaseModel):
    """Detailed progress tracking for a lesson."""
    progress_id: str = Field(..., description="Unique progress identifier")
    lesson_id: str = Field(..., description="Associated lesson ID")
    student_id: str = Field(..., description="Student ID")
    
    # Overall progress
    slides_completed: int = Field(default=0)
    slides_total: int = Field(..., description="Total slides in lesson")
    completion_percentage: float = Field(default=0.0, ge=0.0, le=100.0)
    
    # Detailed slide progress
    slide_progress: List[Dict[str, Any]] = Field(default=[], description="Progress on each slide")
    
    # Engagement metrics
    time_spent_minutes: int = Field(default=0)
    interactions_count: int = Field(default=0)
    correct_responses: int = Field(default=0)
    total_responses: int = Field(default=0)
    
    # Learning indicators
    concept_mastery: Dict[str, float] = Field(default={}, description="Mastery level for each concept")
    areas_of_difficulty: List[str] = Field(default=[], description="Concepts student struggled with")
    strengths: List[str] = Field(default=[], description="Areas where student excelled")
    
    # Chatbot usage
    chatbot_sessions: int = Field(default=0)
    questions_asked: int = Field(default=0)
    help_requests: int = Field(default=0)
    
    started_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = Field(None)


================================================================================
File: app/models/rag_models.py
Size: 5.32 kB
================================================================================

# FILE: app/models/rag_models.py

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum

class DocumentChunk(BaseModel):
    """A chunk of text extracted from a document."""
    chunk_id: str = Field(..., description="Unique identifier for the chunk")
    document_id: str = Field(..., description="ID of the source document")
    content: str = Field(..., description="Text content of the chunk")
    chunk_index: int = Field(..., description="Index of this chunk in the document")
    page_number: Optional[int] = Field(None, description="Page number if applicable")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
    embedding_vector: Optional[List[float]] = Field(None, description="Vector embedding of the content")
    created_at: datetime = Field(default_factory=datetime.utcnow)

class DocumentProcessingStatus(str, Enum):
    """Status of document processing."""
    PENDING = "pending"
    PROCESSING = "processing" 
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessedDocument(BaseModel):
    """Metadata for a processed document."""
    document_id: str = Field(..., description="Unique document identifier")
    teacher_uid: str = Field(..., description="Teacher who uploaded the document")
    original_filename: str = Field(..., description="Original filename")
    file_type: str = Field(..., description="Document type (pdf, docx, etc)")
    subject: Optional[str] = Field(None, description="Subject area")
    grade_level: Optional[int] = Field(None, description="Target grade level")
    total_chunks: int = Field(0, description="Number of chunks created")
    processing_status: DocumentProcessingStatus = Field(DocumentProcessingStatus.PENDING)
    processing_error: Optional[str] = Field(None, description="Error message if processing failed")
    processed_at: Optional[datetime] = Field(None)
    created_at: datetime = Field(default_factory=datetime.utcnow)

class RAGQuery(BaseModel):
    """Query for RAG retrieval."""
    query_text: str = Field(..., description="Query text for retrieval")
    subject: str = Field(..., description="Subject filter")
    grade_level: int = Field(..., description="Grade level filter")
    topic: Optional[str] = Field(None, description="Specific topic filter")
    max_results: int = Field(5, description="Maximum number of chunks to retrieve")
    similarity_threshold: float = Field(0.7, description="Minimum similarity score")

class RAGResult(BaseModel):
    """Result from RAG retrieval."""
    chunk: DocumentChunk = Field(..., description="Retrieved document chunk")
    similarity_score: float = Field(..., description="Similarity score (0-1)")
    document_metadata: Dict[str, Any] = Field(default_factory=dict, description="Source document metadata")

class QuestionGenerationRequest(BaseModel):
    """Request for AI question generation."""
    context_chunks: List[RAGResult] = Field(..., description="Retrieved context chunks")
    subject: str = Field(..., description="Subject area")
    grade_level: int = Field(..., description="Target grade level") 
    topic: str = Field(..., description="Specific topic")
    difficulty_level: str = Field(..., description="Difficulty: easy, medium, hard")
    question_count: int = Field(..., description="Number of questions to generate")
    question_type: str = Field("multiple_choice", description="Type of questions")

class GeneratedQuestionContext(BaseModel):
    """Additional context for a generated question."""
    source_chunks: List[str] = Field(..., description="IDs of source chunks used")
    confidence_score: float = Field(..., description="AI confidence in question quality")
    generation_metadata: Dict[str, Any] = Field(default_factory=dict, description="AI generation details")

class EnhancedAssessmentQuestion(BaseModel):
    """Extended assessment question with RAG context."""
    question_id: str = Field(..., description="Unique question identifier")
    question_text: str = Field(..., description="The question text")
    options: List[str] = Field(..., description="Multiple choice options")
    correct_answer: int = Field(..., description="Index of correct answer (0-based)")
    explanation: str = Field(..., description="Explanation of the correct answer")
    difficulty: str = Field(..., description="Question difficulty level")
    topic: str = Field(..., description="Question topic")
    
    # RAG-specific fields
    context: GeneratedQuestionContext = Field(..., description="Generation context")
    bloom_taxonomy_level: Optional[str] = Field(None, description="Cognitive level (remember, understand, apply, etc)")
    learning_objectives: List[str] = Field(default_factory=list, description="Learning objectives addressed")
    
class VectorSearchMetrics(BaseModel):
    """Metrics for vector search performance."""
    query_time_ms: float = Field(..., description="Time taken for query")
    total_documents: int = Field(..., description="Total documents in collection")
    results_returned: int = Field(..., description="Number of results returned")
    average_similarity: float = Field(..., description="Average similarity score")
    search_timestamp: datetime = Field(default_factory=datetime.utcnow)


================================================================================
File: app/models/requests.py
Size: 6.64 kB
================================================================================

# FILE: app/models/requests.py

from pydantic import BaseModel, Field, EmailStr
from typing import Optional, List, Dict, Any
from datetime import datetime

# ====================================================================
# Authentication Models
# ====================================================================

class UserCreate(BaseModel):
    email: EmailStr
    password: str = Field(..., min_length=8, description="Password must be at least 8 characters long")
    role: Optional[str] = None
    first_name: Optional[str] = None
    last_name: Optional[str] = None

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class Token(BaseModel):
    id_token: str
    token_type: str = "bearer"

class UserInDB(BaseModel):
    uid: str
    email: EmailStr
    created_at: datetime = Field(default_factory=datetime.utcnow)
    subjects: Optional[List[str]] = []
    role: Optional[str] = None
    first_name: Optional[str] = None
    last_name: Optional[str] = None

class UserProfileUpdate(BaseModel): # <-- ADD THIS NEW MODEL
    """
    Request model for updating a user's profile, e.g., their subjects.
    """
    subjects: List[str] = Field(..., description="A list of subjects the teacher handles.")

# ====================================================================
# Agent Interaction Models
# ====================================================================

class AgentPrompt(BaseModel):
    """Request model for agent invocation."""
    prompt: str = Field(..., description="The user's prompt to send to the agent", min_length=1)
    
class AgentResponse(BaseModel):
    """Response model for agent invocation."""
    response: str = Field(..., description="The agent's response text")
    session_id: Optional[str] = Field(None, description="The session ID for conversation continuity")
    
class HealthResponse(BaseModel):
    """Response model for health check."""
    status: str = Field(..., description="The health status")
    message: str = Field(..., description="Additional health information")

# ====================================================================
# Document Management Models
# ====================================================================

class DocumentUploadResponse(BaseModel):
    """Response model for document upload."""
    document_id: str = Field(..., description="Unique identifier for the uploaded document")
    filename: str = Field(..., description="Original filename")
    file_size: int = Field(..., description="File size in bytes")
    file_type: str = Field(..., description="MIME type of the file")
    subject: str = Field(..., description="Subject category for the document")
    grade_level: int = Field(..., ge=1, le=12, description="Grade level (1-12)")
    upload_status: str = Field(default="uploaded", description="Current upload status")
    storage_url: str = Field(..., description="Firebase Storage URL")
    created_at: datetime = Field(default_factory=datetime.utcnow)

class DocumentIndexingStatus(BaseModel):
    """Response model for document indexing status."""
    document_id: str = Field(..., description="Document identifier")
    indexing_status: str = Field(..., description="Current indexing status: pending, processing, completed, failed")
    progress_percentage: int = Field(default=0, description="Indexing progress (0-100)")
    estimated_completion: Optional[datetime] = Field(None, description="Estimated completion time")
    error_message: Optional[str] = Field(None, description="Error message if indexing failed")
    vertex_ai_index_id: Optional[str] = Field(None, description="Vertex AI Search index ID")

class ExtractedFileInfo(BaseModel):
    """Information about a file extracted from a ZIP archive."""
    filename: str = Field(..., description="Original filename from ZIP")
    document_id: str = Field(..., description="Generated document ID")
    file_size: int = Field(..., description="File size in bytes")
    file_type: str = Field(..., description="MIME type of the file")
    extraction_status: str = Field(..., description="Status: success, skipped, failed")
    storage_url: Optional[str] = Field(None, description="Firebase Storage URL if successful")
    error_message: Optional[str] = Field(None, description="Error message if extraction failed")

class ZipUploadResponse(BaseModel):
    """Enhanced response model for ZIP file upload."""
    zip_filename: str = Field(..., description="Original ZIP filename")
    zip_file_size: int = Field(..., description="ZIP file size in bytes")
    total_files_found: int = Field(..., description="Total files found in ZIP")
    files_processed: int = Field(..., description="Number of files successfully processed")
    files_skipped: int = Field(..., description="Number of files skipped (unsupported/too large)")
    files_failed: int = Field(..., description="Number of files that failed to process")
    extracted_files: List[ExtractedFileInfo] = Field(..., description="Details of each extracted file")
    upload_status: str = Field(default="completed", description="Overall upload status")
    subject: str = Field(..., description="Subject category for documents")
    grade_level: int = Field(..., ge=1, le=12, description="Grade level (1-12)")
    created_at: datetime = Field(default_factory=datetime.utcnow)

class DocumentMetadata(BaseModel):
    """Document metadata for storage and search."""
    document_id: str
    teacher_uid: str
    filename: str
    file_type: str
    file_size: int
    subject: str
    grade_level: int
    storage_path: str
    firebase_url: str
    upload_date: datetime
    indexing_status: str
    vertex_ai_index_id: Optional[str] = None
    page_count: Optional[int] = None
    text_content_preview: Optional[str] = None
    content_analysis: Optional[Dict[str, Any]] = None  # AI-generated content analysis
    metadata: Optional[Dict[str, Any]] = None  # Additional metadata like parent_zip

# ====================================================================
# Content Generation Models
# ====================================================================

class LocalContentRequest(BaseModel):
    topic: str
    language: str
    cultural_context: Optional[str] = None
    grade_level: int = Field(..., ge=1, le=12, description="Grade level from 1 to 12")
    content_type: str = Field(..., pattern="^(story|explanation)$", description="Must be 'story' or 'explanation'")

class GeneratedContentResponse(BaseModel):
    id: str
    title: str
    content: str
    topic: str
    grade_level: int
    language: str
    created_at: datetime = Field(default_factory=datetime.utcnow)


================================================================================
File: app/models/student.py
Size: 7.47 kB
================================================================================

# FILE: app/models/student.py

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
from datetime import datetime
from enum import Enum

class StudentProfile(BaseModel):
    """Student profile model."""
    student_id: str = Field(..., description="Unique student identifier")
    teacher_uid: str = Field(..., description="Teacher who manages this student")
    first_name: str = Field(..., min_length=1, description="Student's first name")
    last_name: str = Field(..., min_length=1, description="Student's last name")
    grade: int = Field(..., ge=1, le=12, description="Student's current grade level")
    default_password: str = Field(..., min_length=6, description="Teacher-generated default password")
    subjects: List[str] = Field(default=[], description="Subjects the student is enrolled in")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_login: Optional[datetime] = None
    is_active: bool = Field(default=True)
    
    # Learning analytics
    current_learning_paths: Dict[str, str] = Field(default={}, description="Subject -> Learning Path ID mapping")
    completed_assessments: List[str] = Field(default=[], description="List of completed assessment IDs")
    performance_metrics: Dict[str, Any] = Field(default={}, description="Performance tracking data")

class StudentCSVRow(BaseModel):
    """Model for validating CSV row data."""
    first_name: str = Field(..., min_length=1)
    last_name: str = Field(..., min_length=1) 
    grade: int = Field(..., ge=1, le=12)
    password: str = Field(..., min_length=6)
    
    @validator('first_name', 'last_name')
    def validate_names(cls, v):
        if not v.strip():
            raise ValueError('Name cannot be empty')
        return v.strip().title()
    
    @validator('password')
    def validate_password(cls, v):
        if len(v.strip()) < 6:
            raise ValueError('Password must be at least 6 characters')
        return v.strip()

class StudentBatchUploadResponse(BaseModel):
    """Response for student batch upload."""
    total_students: int = Field(..., description="Total students in CSV")
    students_created: int = Field(..., description="Successfully created students")
    students_updated: int = Field(..., description="Updated existing students")
    students_failed: int = Field(..., description="Failed to process students")
    failed_students: List[Dict[str, Any]] = Field(default=[], description="Details of failed student creations")
    created_student_ids: List[str] = Field(default=[], description="IDs of successfully created students")
    upload_summary: str = Field(..., description="Summary message")

class AssessmentConfig(BaseModel):
    """Configuration for assessment creation."""
    config_id: str = Field(..., description="Unique configuration ID")
    teacher_uid: str = Field(..., description="Teacher who created this config")
    name: str = Field(..., description="Assessment configuration name")
    subject: str = Field(..., description="Subject for assessment")
    target_grade: int = Field(..., ge=1, le=12, description="Target grade level")
    difficulty_level: str = Field(..., pattern="^(easy|medium|hard)$", description="Difficulty level")
    topic: str = Field(..., description="Specific topic to assess")
    question_count: int = Field(default=10, ge=5, le=20, description="Number of MCQ questions")
    time_limit_minutes: int = Field(default=30, ge=10, le=120, description="Time limit in minutes")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    is_active: bool = Field(default=True)

class AssessmentQuestion(BaseModel):
    """Individual assessment question."""
    question_id: str = Field(..., description="Unique question ID")
    question_text: str = Field(..., description="The question text")
    options: List[str] = Field(..., min_items=4, max_items=4, description="Four answer options")
    correct_answer: int = Field(..., ge=0, le=3, description="Index of correct answer (0-3)")
    explanation: str = Field(..., description="Explanation for the correct answer")
    difficulty: str = Field(..., description="Question difficulty level")
    topic: str = Field(..., description="Topic this question covers")

class Assessment(BaseModel):
    """Complete assessment model."""
    assessment_id: str = Field(..., description="Unique assessment ID")
    config_id: str = Field(..., description="Configuration used to create this assessment")
    teacher_uid: str = Field(..., description="Teacher who created the assessment")
    title: str = Field(..., description="Assessment title")
    subject: str = Field(..., description="Subject")
    grade: int = Field(..., description="Target grade")
    difficulty: str = Field(..., description="Difficulty level")
    topic: str = Field(..., description="Topic")
    questions: List[AssessmentQuestion] = Field(..., description="List of questions")
    time_limit_minutes: int = Field(..., description="Time limit")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    is_active: bool = Field(default=True)
    
class StudentAssessmentResult(BaseModel):
    """Student's assessment result."""
    result_id: str = Field(..., description="Unique result ID")
    student_id: str = Field(..., description="Student who took the assessment")
    assessment_id: str = Field(..., description="Assessment that was taken")
    answers: List[int] = Field(..., description="Student's answers (indices)")
    score: int = Field(..., ge=0, description="Number of correct answers")
    total_questions: int = Field(..., description="Total number of questions")
    percentage: float = Field(..., ge=0.0, le=100.0, description="Percentage score")
    time_taken_minutes: int = Field(..., description="Time taken to complete")
    submitted_at: datetime = Field(default_factory=datetime.utcnow)
    
    # Analysis results
    strengths: List[str] = Field(default=[], description="Topics student performed well in")
    weaknesses: List[str] = Field(default=[], description="Topics student needs improvement in")
    recommended_learning_path: Optional[str] = Field(None, description="Recommended learning path ID")

class LearningPath(BaseModel):
    """Personalized learning path for a student."""
    path_id: str = Field(..., description="Unique learning path ID")
    student_id: str = Field(..., description="Student this path is for")
    subject: str = Field(..., description="Subject")
    current_grade: int = Field(..., description="Student's current grade")
    target_grade: int = Field(..., description="Target grade level")
    
    # Path content
    topics_to_cover: List[str] = Field(..., description="Ordered list of topics to study")
    current_topic_index: int = Field(default=0, description="Current position in learning path")
    difficulty_progression: List[str] = Field(..., description="Difficulty level for each topic")
    
    # Resources
    assigned_documents: List[str] = Field(default=[], description="Document IDs assigned to this path")
    completed_topics: List[str] = Field(default=[], description="Topics student has completed")
    
    # Progress tracking
    progress_percentage: float = Field(default=0.0, ge=0.0, le=100.0, description="Overall progress")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    is_active: bool = Field(default=True)


================================================================================
File: app/models.py
Size: 1.7 kB
================================================================================

# FILE: app/models.py

from pydantic import BaseModel, Field, EmailStr
from typing import Optional, List
from datetime import datetime

# ====================================================================
# Authentication Models
# ====================================================================

class UserCreate(BaseModel):
    email: EmailStr
    password: str = Field(..., min_length=8, description="Password must be at least 8 characters long")

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class Token(BaseModel):
    id_token: str
    token_type: str = "bearer"

class UserInDB(BaseModel):
    uid: str
    email: EmailStr
    created_at: datetime = Field(default_factory=datetime.utcnow)
    subjects: Optional[List[str]] = [] # <-- ADD THIS LINE

class UserProfileUpdate(BaseModel): # <-- ADD THIS NEW MODEL
    """
    Request model for updating a user's profile, e.g., their subjects.
    """
    subjects: List[str] = Field(..., description="A list of subjects the teacher handles.")


# ====================================================================
# Content Generation Models
# ====================================================================

class LocalContentRequest(BaseModel):
    topic: str
    language: str
    cultural_context: Optional[str] = None
    grade_level: int = Field(..., ge=1, le=12, description="Grade level from 1 to 12")
    content_type: str = Field(..., pattern="^(story|explanation)$", description="Must be 'story' or 'explanation'")

class GeneratedContentResponse(BaseModel):
    id: str
    title: str
    content: str
    language: str
    grade_level: int
    topic: str
    created_at: datetime
    teacher_id: str

================================================================================
File: app/services/__init__.py
Size: 85 B
================================================================================

# FILE: app/services/__init__.py

"""Business logic services for the application."""


================================================================================
File: app/services/agent_registry.py
Size: 4.02 kB
================================================================================

# FILE: app/services/agent_registry.py

from typing import Dict, List, Optional, Any, Type
from abc import ABC, abstractmethod
import logging

from google.adk.agents import Agent
from app.core.config import settings

logger = logging.getLogger(__name__)

class BaseAgentHandler(ABC):
    """Abstract base class for agent handlers."""
    
    @property
    @abstractmethod
    def agent_name(self) -> str:
        """Return the unique name of this agent."""
        pass
    
    @property
    @abstractmethod
    def description(self) -> str:
        """Return a description of what this agent does."""
        pass
    
    @property
    @abstractmethod
    def capabilities(self) -> List[str]:
        """Return a list of capabilities this agent provides."""
        pass
    
    @abstractmethod
    def create_agent(self) -> Agent:
        """Create and return the agent instance."""
        pass
    
    @abstractmethod
    def matches_request(self, prompt: str, context: Dict[str, Any] = None) -> float:
        """
        Return a confidence score (0.0 to 1.0) for how well this agent matches the request.
        
        Args:
            prompt: The user's prompt
            context: Additional context about the user/session
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        pass

class AgentRegistry:
    """Registry for managing available agents and their capabilities."""
    
    def __init__(self):
        self._handlers: Dict[str, BaseAgentHandler] = {}
        self._agent_cache: Dict[str, Agent] = {}
    
    def register_handler(self, handler: BaseAgentHandler) -> None:
        """Register an agent handler."""
        self._handlers[handler.agent_name] = handler
        logger.info(f"Registered agent handler: {handler.agent_name}")
    
    def get_handler(self, agent_name: str) -> Optional[BaseAgentHandler]:
        """Get a specific agent handler by name."""
        return self._handlers.get(agent_name)
    
    def get_agent(self, agent_name: str) -> Optional[Agent]:
        """Get or create an agent by name."""
        if agent_name not in self._agent_cache:
            handler = self.get_handler(agent_name)
            if handler:
                self._agent_cache[agent_name] = handler.create_agent()
            else:
                return None
        return self._agent_cache.get(agent_name)
    
    def find_best_agent(self, prompt: str, context: Dict[str, Any] = None) -> Optional[str]:
        """
        Find the best agent for handling the given prompt.
        
        Args:
            prompt: The user's prompt
            context: Additional context
            
        Returns:
            The name of the best matching agent, or None if no good match
        """
        if not self._handlers:
            return None
        
        best_score = 0.0
        best_agent = None
        
        for handler in self._handlers.values():
            score = handler.matches_request(prompt, context)
            logger.debug(f"Agent {handler.agent_name} scored {score} for prompt: {prompt[:50]}")
            
            if score > best_score:
                best_score = score
                best_agent = handler.agent_name
        
        # Only return an agent if the confidence is reasonably high
        if best_score >= 0.3:
            logger.info(f"Selected agent {best_agent} with confidence {best_score}")
            return best_agent
        
        logger.warning(f"No agent found with sufficient confidence for prompt: {prompt[:50]}")
        return None
    
    def list_agents(self) -> List[Dict[str, Any]]:
        """List all registered agents and their information."""
        return [
            {
                "name": handler.agent_name,
                "description": handler.description,
                "capabilities": handler.capabilities
            }
            for handler in self._handlers.values()
        ]

# Global registry instance
agent_registry = AgentRegistry()


================================================================================
File: app/services/agent_service.py
Size: 5.08 kB
================================================================================

# FILE: app/services/agent_service.py

from __future__ import annotations
from typing import AsyncGenerator, Dict, Any
import logging

from google.adk.runners import InMemoryRunner
from google.genai import types

from app.agents.orchestrator_agent.agent import root_agent as orchestrator_agent
from app.agents.tools.profile_tools import current_user_uid
from app.agents.tools.onboarding_tools import current_user_uid as onboarding_current_user_uid
from app.agents.tools.onboarding_tools import current_user_uid as onboarding_current_user_uid

logger = logging.getLogger(__name__)

class AgentService:
    """Service for handling comprehensive teacher agent interactions."""
    
    def __init__(self):
        self.app_name = "comprehensive_teacher_agent"
    
    async def invoke_agent(self, user_uid: str, prompt: str) -> str:
        """
        Invoke the comprehensive teacher agent with a user's prompt.
        This agent handles both onboarding for new users and profile management for existing users.
        
        Args:
            user_uid: The authenticated user's UID
            prompt: The user's prompt text
            
        Returns:
            The agent's response text
            
        Raises:
            Exception: If agent invocation fails
        """
        try:
            # Set the user UID in context for all tools to use
            current_user_uid.set(user_uid)
            onboarding_current_user_uid.set(user_uid)
            
            # Create an InMemoryRunner with the orchestrator_agent
            runner = InMemoryRunner(orchestrator_agent, app_name=self.app_name)
            
            # Create or get session for the user
            session_id = f"session_{user_uid}"
            
            # Try to get existing session, create if it doesn't exist
            session = await runner.session_service.get_session(
                app_name=self.app_name,
                user_id=user_uid,
                session_id=session_id
            )
            
            if session is None:
                # Session doesn't exist, create it
                session = await runner.session_service.create_session(
                    app_name=self.app_name,
                    user_id=user_uid,
                    session_id=session_id
                )
            
            # Create a Content object with the user's prompt
            user_message = types.Content(
                parts=[types.Part(text=prompt)],
                role="user"
            )
            
            # Use the new API to run the agent
            response_generator = runner.run_async(
                user_id=user_uid,
                session_id=session_id,
                new_message=user_message
            )
            
            # Process the response
            final_response = await self._process_agent_response(response_generator)
            
            logger.info(f"Agent response processed successfully for user {user_uid}")
            return final_response
            
        except Exception as e:
            logger.error(f"Failed to invoke agent for user {user_uid}: {str(e)}")
            raise
    
    async def _process_agent_response(self, response_generator: AsyncGenerator) -> str:
        """
        Process the agent response generator and extract text.
        
        Args:
            response_generator: The async generator from agent execution
            
        Returns:
            The final response text
        """
        final_response = ""
        
        async for event in response_generator:
            # Debug logging (remove in production)
            logger.debug(f"Event type: {type(event)}")
            logger.debug(f"Event has content: {hasattr(event, 'content')}")
            
            if hasattr(event, 'content') and event.content:
                logger.debug(f"Event.content type: {type(event.content)}")
                
                # Check if content has parts
                if hasattr(event.content, 'parts') and event.content.parts:
                    for part in event.content.parts:
                        if hasattr(part, 'text') and part.text:
                            logger.debug(f"Found text in part: {part.text}")
                            final_response += part.text
                # Check if content has text directly
                elif hasattr(event.content, 'text') and event.content.text:
                    logger.debug(f"Found text in content: {event.content.text}")
                    final_response += event.content.text
            
            # Check if event has text directly
            elif hasattr(event, 'text') and event.text:
                logger.debug(f"Found text in event: {event.text}")
                final_response += event.text
        
        # If we still don't have a response, return a success message
        if not final_response.strip():
            final_response = "Task completed successfully."
        
        return final_response

# Create a singleton instance
agent_service = AgentService()


================================================================================
File: app/services/assessment_analysis_service.py
Size: 22.44 kB
================================================================================

# FILE: app/services/assessment_analysis_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from collections import defaultdict

from app.models.learning_models import (
    StudentPerformance, KnowledgeGap, LearningObjectiveType, 
    DifficultyLevel, LearningRecommendation
)
from app.models.student import Assessment, AssessmentQuestion
from app.core.firebase import db
from app.core.vertex import get_vertex_model

logger = logging.getLogger(__name__)

class AssessmentAnalysisService:
    """Service for analyzing student assessment performance and identifying learning needs."""
    
    def __init__(self):
        self.model = get_vertex_model("gemini-2.5-pro")
        self.performances_collection = "student_performances"
        self.knowledge_gaps_collection = "knowledge_gaps"
        self.recommendations_collection = "learning_recommendations"
    
    async def analyze_assessment_performance(
        self, 
        student_id: str,
        assessment: Assessment,
        student_answers: List[int],
        time_taken_minutes: int
    ) -> StudentPerformance:
        """Analyze a student's performance on an assessment."""
        
        try:
            logger.info(f"Analyzing assessment performance for student {student_id}")
            
            # Calculate basic metrics
            total_questions = len(assessment.questions)
            correct_answers = sum(1 for i, answer in enumerate(student_answers) 
                                if i < len(assessment.questions) and answer == assessment.questions[i].correct_answer)
            score_percentage = (correct_answers / total_questions) * 100 if total_questions > 0 else 0
            
            # Analyze performance by question
            question_performances = []
            topic_scores = defaultdict(list)
            difficulty_scores = defaultdict(list)
            
            for i, (question, student_answer) in enumerate(zip(assessment.questions, student_answers)):
                is_correct = student_answer == question.correct_answer
                
                question_performance = {
                    "question_id": question.question_id,
                    "question_text": question.question_text,
                    "student_answer": student_answer,
                    "correct_answer": question.correct_answer,
                    "is_correct": is_correct,
                    "topic": question.topic,
                    "difficulty": question.difficulty,
                    "time_spent_seconds": (time_taken_minutes * 60) // total_questions  # Rough estimate
                }
                question_performances.append(question_performance)
                
                # Group by topic and difficulty
                topic_scores[question.topic].append(1 if is_correct else 0)
                difficulty_scores[question.difficulty].append(1 if is_correct else 0)
            
            # Calculate topic and difficulty scores
            topic_score_averages = {
                topic: sum(scores) / len(scores) * 100 
                for topic, scores in topic_scores.items()
            }
            difficulty_score_averages = {
                difficulty: sum(scores) / len(scores) * 100 
                for difficulty, scores in difficulty_scores.items()
            }
            
            # Identify strengths and weaknesses
            strengths = [topic for topic, score in topic_score_averages.items() if score >= 80]
            weaknesses = [topic for topic, score in topic_score_averages.items() if score < 60]
            
            # Create performance record
            performance = StudentPerformance(
                performance_id=str(uuid.uuid4()),
                student_id=student_id,
                assessment_id=assessment.assessment_id,
                total_questions=total_questions,
                correct_answers=correct_answers,
                score_percentage=score_percentage,
                time_taken_minutes=time_taken_minutes,
                question_performances=question_performances,
                topic_scores=topic_score_averages,
                difficulty_scores=difficulty_score_averages,
                learning_objective_scores={},  # Will be enhanced with AI analysis
                strengths=strengths,
                weaknesses=weaknesses,
                recommended_focus_areas=weaknesses  # Initial recommendation
            )
            
            # Save performance to Firestore
            await self._save_performance(performance)
            
            # Generate detailed analysis using AI
            enhanced_performance = await self._enhance_performance_with_ai(performance, assessment)
            
            # Identify knowledge gaps
            knowledge_gaps = await self.identify_knowledge_gaps(enhanced_performance, assessment)
            
            # Generate learning recommendations
            recommendations = await self.generate_learning_recommendations(
                student_id, enhanced_performance, knowledge_gaps
            )
            
            logger.info(f"Completed analysis for student {student_id}: {score_percentage:.1f}% score, {len(knowledge_gaps)} gaps identified")
            
            return enhanced_performance
            
        except Exception as e:
            logger.error(f"Failed to analyze assessment performance: {str(e)}")
            raise e
    
    async def _enhance_performance_with_ai(
        self, 
        performance: StudentPerformance,
        assessment: Assessment
    ) -> StudentPerformance:
        """Use AI to enhance performance analysis with deeper insights."""
        
        try:
            # Prepare analysis prompt
            prompt = f"""You are an expert educational analyst. Analyze this student's assessment performance and provide detailed insights.

ASSESSMENT INFORMATION:
- Subject: {assessment.subject}
- Topic: {assessment.topic}
- Grade Level: {assessment.grade}
- Difficulty: {assessment.difficulty}
- Total Questions: {performance.total_questions}

STUDENT PERFORMANCE:
- Overall Score: {performance.score_percentage:.1f}%
- Correct Answers: {performance.correct_answers}/{performance.total_questions}
- Time Taken: {performance.time_taken_minutes} minutes

QUESTION-BY-QUESTION ANALYSIS:
{self._format_question_analysis(performance.question_performances)}

TOPIC PERFORMANCE:
{self._format_topic_scores(performance.topic_scores)}

DIFFICULTY PERFORMANCE:
{self._format_difficulty_scores(performance.difficulty_scores)}

Please provide a detailed analysis in JSON format:

{{
  "learning_objective_scores": {{
    "remember": score_0_to_100,
    "understand": score_0_to_100,
    "apply": score_0_to_100,
    "analyze": score_0_to_100,
    "evaluate": score_0_to_100,
    "create": score_0_to_100
  }},
  "detailed_strengths": ["specific strength 1", "specific strength 2"],
  "detailed_weaknesses": ["specific weakness 1", "specific weakness 2"],
  "learning_patterns": ["pattern 1", "pattern 2"],
  "recommended_focus_areas": ["focus area 1", "focus area 2"],
  "next_steps": ["specific action 1", "specific action 2"],
  "confidence_indicators": ["indicator 1", "indicator 2"],
  "improvement_suggestions": ["suggestion 1", "suggestion 2"]
}}"""

            # Get AI analysis
            response = self.model.generate_content(prompt)
            ai_analysis = self._parse_ai_analysis(response.text)
            
            # Update performance with AI insights
            performance.learning_objective_scores = ai_analysis.get("learning_objective_scores", {})
            performance.strengths = ai_analysis.get("detailed_strengths", performance.strengths)
            performance.weaknesses = ai_analysis.get("detailed_weaknesses", performance.weaknesses)
            performance.recommended_focus_areas = ai_analysis.get("recommended_focus_areas", performance.recommended_focus_areas)
            
            # Add AI analysis metadata
            performance.question_performances.append({
                "ai_analysis": {
                    "learning_patterns": ai_analysis.get("learning_patterns", []),
                    "next_steps": ai_analysis.get("next_steps", []),
                    "confidence_indicators": ai_analysis.get("confidence_indicators", []),
                    "improvement_suggestions": ai_analysis.get("improvement_suggestions", [])
                }
            })
            
            # Update in Firestore
            await self._save_performance(performance)
            
            return performance
            
        except Exception as e:
            logger.warning(f"AI enhancement failed, using basic analysis: {str(e)}")
            return performance
    
    async def identify_knowledge_gaps(
        self, 
        performance: StudentPerformance,
        assessment: Assessment
    ) -> List[KnowledgeGap]:
        """Identify specific knowledge gaps from performance analysis."""
        
        try:
            gaps = []
            
            # Analyze incorrect answers for patterns
            incorrect_questions = [
                (q_perf, next((q for q in assessment.questions if q.question_id == q_perf["question_id"]), None))
                for q_perf in performance.question_performances
                if isinstance(q_perf, dict) and not q_perf.get("is_correct", True)
            ]
            
            # Group by topic and difficulty
            topic_gaps = defaultdict(list)
            for q_perf, question in incorrect_questions:
                if question:
                    topic_gaps[question.topic].append((q_perf, question))
            
            # Create gap records for topics with multiple incorrect answers
            for topic, topic_questions in topic_gaps.items():
                if len(topic_questions) >= 2:  # Multiple mistakes in same topic
                    gap = KnowledgeGap(
                        gap_id=str(uuid.uuid4()),
                        student_id=performance.student_id,
                        subject=assessment.subject,
                        topic=topic,
                        difficulty_level=DifficultyLevel(assessment.difficulty),
                        learning_objective=LearningObjectiveType.UNDERSTAND,  # Default, will be refined
                        confidence_score=min(0.9, len(topic_questions) / len(assessment.questions)),
                        severity_score=1.0 - (performance.topic_scores.get(topic, 0) / 100),
                        frequency=len(topic_questions),
                        source_assessments=[assessment.assessment_id],
                        related_questions=[q_perf["question_id"] for q_perf, _ in topic_questions]
                    )
                    gaps.append(gap)
            
            # Save gaps to Firestore
            for gap in gaps:
                await self._save_knowledge_gap(gap)
            
            logger.info(f"Identified {len(gaps)} knowledge gaps for student {performance.student_id}")
            return gaps
            
        except Exception as e:
            logger.error(f"Failed to identify knowledge gaps: {str(e)}")
            return []
    
    async def generate_learning_recommendations(
        self,
        student_id: str,
        performance: StudentPerformance,
        knowledge_gaps: List[KnowledgeGap]
    ) -> List[LearningRecommendation]:
        """Generate personalized learning recommendations."""
        
        try:
            recommendations = []
            
            # Create recommendations for each knowledge gap
            for gap in knowledge_gaps:
                if gap.severity_score > 0.3:  # Only for significant gaps
                    recommendation = LearningRecommendation(
                        recommendation_id=str(uuid.uuid4()),
                        student_id=student_id,
                        title=f"Improve {gap.topic} Understanding",
                        description=f"Focus on strengthening your understanding of {gap.topic} concepts",
                        rationale=f"You had difficulty with {gap.frequency} questions on {gap.topic}",
                        recommended_action=f"Practice {gap.topic} problems at {gap.difficulty_level.value} level",
                        content_type="practice_problems",
                        difficulty_level=gap.difficulty_level,
                        estimated_duration_minutes=30,
                        priority_score=gap.severity_score,
                        urgency_level="medium" if gap.severity_score > 0.7 else "low",
                        based_on_assessments=[performance.assessment_id],
                        addresses_gaps=[gap.gap_id]
                    )
                    recommendations.append(recommendation)
            
            # Generate general recommendations based on performance
            if performance.score_percentage < 70:
                general_rec = LearningRecommendation(
                    recommendation_id=str(uuid.uuid4()),
                    student_id=student_id,
                    title="Review Fundamental Concepts",
                    description="Strengthen your foundation in this subject area",
                    rationale=f"Overall score of {performance.score_percentage:.1f}% suggests need for review",
                    recommended_action="Review basic concepts and practice easier problems first",
                    content_type="review_materials",
                    difficulty_level=DifficultyLevel.EASY,
                    estimated_duration_minutes=45,
                    priority_score=0.8,
                    urgency_level="high",
                    based_on_assessments=[performance.assessment_id],
                    addresses_gaps=[gap.gap_id for gap in knowledge_gaps]
                )
                recommendations.append(general_rec)
            
            # Save recommendations
            for rec in recommendations:
                await self._save_recommendation(rec)
            
            logger.info(f"Generated {len(recommendations)} learning recommendations for student {student_id}")
            return recommendations
            
        except Exception as e:
            logger.error(f"Failed to generate recommendations: {str(e)}")
            return []
    
    async def get_student_progress_summary(self, student_id: str) -> Dict[str, Any]:
        """Get comprehensive progress summary for a student."""
        
        try:
            # Get recent performances - handling Firestore index requirement
            try:
                # Try with ordering (requires composite index)
                performances_query = (db.collection(self.performances_collection)
                                    .where("student_id", "==", student_id)
                                    .order_by("completed_at", direction="DESCENDING")
                                    .limit(10))
                
                performance_docs = performances_query.get()
                performances = [StudentPerformance(**doc.to_dict()) for doc in performance_docs]
                
            except Exception as index_error:
                logger.warning(f"Composite index not available, falling back to simple query: {index_error}")
                # Fallback: Get all performances for student and sort in Python
                performances_query = (db.collection(self.performances_collection)
                                    .where("student_id", "==", student_id))
                
                performance_docs = performances_query.get()
                all_performances = [StudentPerformance(**doc.to_dict()) for doc in performance_docs]
                
                # Sort by completed_at in Python and limit to 10
                performances = sorted(
                    all_performances, 
                    key=lambda p: p.completed_at if p.completed_at else datetime.min, 
                    reverse=True
                )[:10]
            
            # Get active knowledge gaps
            try:
                gaps_query = (db.collection(self.knowledge_gaps_collection)
                             .where("student_id", "==", student_id))
                
                gap_docs = gaps_query.get()
                gaps = [KnowledgeGap(**doc.to_dict()) for doc in gap_docs]
            except Exception as e:
                logger.warning(f"Failed to get knowledge gaps for {student_id}: {e}")
                gaps = []
            
            # Get active recommendations
            try:
                recs_query = (db.collection(self.recommendations_collection)
                             .where("student_id", "==", student_id)
                             .where("is_active", "==", True))
                
                rec_docs = recs_query.get()
                recommendations = [LearningRecommendation(**doc.to_dict()) for doc in rec_docs]
            except Exception as e:
                logger.warning(f"Failed to get recommendations for {student_id}: {e}")
                recommendations = []
            
            # Calculate progress metrics
            if performances:
                avg_score = sum(p.score_percentage for p in performances) / len(performances)
                score_trend = self._calculate_score_trend(performances)
                subject_performance = self._aggregate_subject_performance(performances)
            else:
                avg_score = 0
                score_trend = "no_data"
                subject_performance = {}
            
            return {
                "student_id": student_id,
                "average_score": round(avg_score, 1),
                "score_trend": score_trend,
                "total_assessments": len(performances),
                "active_knowledge_gaps": len(gaps),
                "active_recommendations": len(recommendations),
                "subject_performance": subject_performance,
                "recent_performances": [p.dict() for p in performances[:5]],
                "priority_gaps": [g.dict() for g in gaps if g.severity_score > 0.7],
                "top_recommendations": [r.dict() for r in recommendations[:3]],
                "last_updated": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get progress summary for {student_id}: {str(e)}")
            return {"error": str(e)}
    
    # Helper methods
    def _format_question_analysis(self, performances: List[Dict[str, Any]]) -> str:
        """Format question performance for AI analysis."""
        formatted = []
        for i, perf in enumerate(performances):
            if isinstance(perf, dict) and "question_id" in perf:
                status = "✓ Correct" if perf.get("is_correct") else "✗ Incorrect"
                formatted.append(f"Q{i+1}: {status} - Topic: {perf.get('topic', 'Unknown')} - Difficulty: {perf.get('difficulty', 'Unknown')}")
        return "\\n".join(formatted)
    
    def _format_topic_scores(self, topic_scores: Dict[str, float]) -> str:
        """Format topic scores for display."""
        return "\\n".join([f"{topic}: {score:.1f}%" for topic, score in topic_scores.items()])
    
    def _format_difficulty_scores(self, difficulty_scores: Dict[str, float]) -> str:
        """Format difficulty scores for display."""
        return "\\n".join([f"{difficulty}: {score:.1f}%" for difficulty, score in difficulty_scores.items()])
    
    def _parse_ai_analysis(self, response_text: str) -> Dict[str, Any]:
        """Parse AI analysis response."""
        try:
            import json
            # Extract JSON from response
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_text = response_text[start_idx:end_idx]
                return json.loads(json_text)
            
            return {}
        except Exception as e:
            logger.warning(f"Failed to parse AI analysis: {str(e)}")
            return {}
    
    def _calculate_score_trend(self, performances: List[StudentPerformance]) -> str:
        """Calculate if scores are improving, declining, or stable."""
        if len(performances) < 2:
            return "insufficient_data"
        
        recent_scores = [p.score_percentage for p in performances[:3]]
        older_scores = [p.score_percentage for p in performances[3:6]]
        
        if not older_scores:
            return "insufficient_data"
        
        recent_avg = sum(recent_scores) / len(recent_scores)
        older_avg = sum(older_scores) / len(older_scores)
        
        if recent_avg > older_avg + 5:
            return "improving"
        elif recent_avg < older_avg - 5:
            return "declining"
        else:
            return "stable"
    
    def _aggregate_subject_performance(self, performances: List[StudentPerformance]) -> Dict[str, float]:
        """Aggregate performance by subject."""
        subject_scores = defaultdict(list)
        
        for perf in performances:
            # We'd need to get the assessment to know the subject
            # For now, use a placeholder
            subject_scores["general"].append(perf.score_percentage)
        
        return {
            subject: sum(scores) / len(scores)
            for subject, scores in subject_scores.items()
        }
    
    async def _save_performance(self, performance: StudentPerformance) -> None:
        """Save performance to Firestore."""
        doc_ref = db.collection(self.performances_collection).document(performance.performance_id)
        doc_ref.set(performance.dict())
    
    async def _save_knowledge_gap(self, gap: KnowledgeGap) -> None:
        """Save knowledge gap to Firestore."""
        doc_ref = db.collection(self.knowledge_gaps_collection).document(gap.gap_id)
        doc_ref.set(gap.dict())
    
    async def _save_recommendation(self, recommendation: LearningRecommendation) -> None:
        """Save recommendation to Firestore."""
        doc_ref = db.collection(self.recommendations_collection).document(recommendation.recommendation_id)
        doc_ref.set(recommendation.dict())

# Global instance
assessment_analysis_service = AssessmentAnalysisService()


================================================================================
File: app/services/assessment_service.py
Size: 14.54 kB
================================================================================

# FILE: app/services/assessment_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from fastapi import HTTPException
from app.core.firebase import db
from app.models.student import (
    AssessmentConfig, Assessment, AssessmentQuestion, 
    StudentAssessmentResult, LearningPath
)
from app.services.vertex_rag_service import vertex_rag_service
# Import assessment agent when needed to avoid initialization issues

logger = logging.getLogger(__name__)

class AssessmentService:
    """Service for managing assessment configurations and generation."""
    
    def __init__(self):
        self.assessment_configs_collection = "assessment_configs"
        self.assessments_collection = "assessments"
        self.assessment_results_collection = "assessment_results"
        
    async def create_assessment_config(
        self,
        teacher_uid: str,
        name: str,
        subject: str,
        target_grade: int,
        difficulty_level: str,
        topic: str,
        question_count: int = 10,
        time_limit_minutes: int = 30
    ) -> AssessmentConfig:
        """
        Create a new assessment configuration.
        
        Args:
            teacher_uid: UID of the teacher creating the config
            name: Name for the assessment configuration
            subject: Subject for the assessment
            target_grade: Target grade level (1-12)
            difficulty_level: Difficulty level (easy, medium, hard)
            topic: Specific topic to assess
            question_count: Number of questions (5-20)
            time_limit_minutes: Time limit in minutes (10-120)
            
        Returns:
            AssessmentConfig object
        """
        try:
            config_id = str(uuid.uuid4())
            
            config = AssessmentConfig(
                config_id=config_id,
                teacher_uid=teacher_uid,
                name=name,
                subject=subject,
                target_grade=target_grade,
                difficulty_level=difficulty_level,
                topic=topic,
                question_count=question_count,
                time_limit_minutes=time_limit_minutes
            )
            
            # Save to Firestore
            doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
            doc_ref.set(config.dict())
            
            logger.info(f"Created assessment config {config_id} for teacher {teacher_uid}")
            return config
            
        except Exception as e:
            logger.error(f"Failed to create assessment config: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to create assessment configuration: {str(e)}"
            )
    
    async def get_teacher_assessment_configs(
        self,
        teacher_uid: str,
        subject_filter: Optional[str] = None
    ) -> List[AssessmentConfig]:
        """Get all assessment configurations for a teacher."""
        try:
            query = db.collection(self.assessment_configs_collection).where("teacher_uid", "==", teacher_uid)
            
            if subject_filter:
                query = query.where("subject", "==", subject_filter)
            
            docs = query.where("is_active", "==", True).order_by("created_at", direction="DESCENDING").get()
            
            configs = []
            for doc in docs:
                config_data = doc.to_dict()
                configs.append(AssessmentConfig(**config_data))
            
            return configs
            
        except Exception as e:
            logger.error(f"Failed to get assessment configs: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve assessment configurations: {str(e)}"
            )
    
    async def get_assessment_config(self, config_id: str) -> AssessmentConfig:
        """Get a specific assessment configuration."""
        try:
            doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                raise HTTPException(status_code=404, detail="Assessment configuration not found")
            
            return AssessmentConfig(**doc.to_dict())
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to get assessment config: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve assessment configuration: {str(e)}"
            )
    
    async def update_assessment_config(
        self,
        config_id: str,
        teacher_uid: str,
        **update_fields
    ) -> AssessmentConfig:
        """Update an assessment configuration."""
        try:
            # Verify ownership
            config = await self.get_assessment_config(config_id)
            if config.teacher_uid != teacher_uid:
                raise HTTPException(status_code=403, detail="Access denied")
            
            # Update fields
            update_data = {k: v for k, v in update_fields.items() if v is not None}
            update_data["updated_at"] = datetime.utcnow()
            
            doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
            doc_ref.update(update_data)
            
            # Return updated config
            return await self.get_assessment_config(config_id)
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to update assessment config: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to update assessment configuration: {str(e)}"
            )
    
    async def deactivate_assessment_config(self, config_id: str, teacher_uid: str) -> None:
        """Deactivate an assessment configuration."""
        try:
            # Verify ownership
            config = await self.get_assessment_config(config_id)
            if config.teacher_uid != teacher_uid:
                raise HTTPException(status_code=403, detail="Access denied")
            
            doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
            doc_ref.update({
                "is_active": False,
                "updated_at": datetime.utcnow()
            })
            
            logger.info(f"Deactivated assessment config {config_id}")
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to deactivate assessment config: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to deactivate assessment configuration: {str(e)}"
            )
    
    async def get_available_topics_for_subject(
        self,
        teacher_uid: str,
        subject: str,
        grade_level: int
    ) -> List[str]:
        """
        Get available topics from uploaded documents for a subject and grade.
        This will query the RAG system to find what topics are available.
        """
        try:
            # Query documents collection to find available topics
            query = (db.collection("documents")
                    .where("teacher_uid", "==", teacher_uid)
                    .where("subject", "==", subject)
                    .where("grade_level", "==", grade_level)
                    .where("indexing_status", "==", "completed"))
            
            docs = query.get()
            
            # For now, we'll extract topics from filenames and content
            # Later, this could use AI to analyze document content for topics
            topics = set()
            
            for doc in docs:
                doc_data = doc.to_dict()
                filename = doc_data.get("filename", "")
                
                # Extract potential topics from filename
                # Remove file extension and split by common delimiters
                topic_parts = filename.replace(".pdf", "").replace(".txt", "").replace(".doc", "")
                topic_parts = topic_parts.replace("_", " ").replace("-", " ")
                
                # Add as potential topic
                if len(topic_parts.strip()) > 0:
                    topics.add(topic_parts.strip().title())
            
            # Add some common topics if none found
            if not topics:
                default_topics = {
                    "Mathematics": ["Addition", "Subtraction", "Multiplication", "Division", "Fractions", "Geometry"],
                    "Science": ["Plants", "Animals", "Weather", "Space", "Matter", "Energy"],
                    "English": ["Reading Comprehension", "Grammar", "Vocabulary", "Writing", "Literature"],
                    "History": ["Ancient Civilizations", "World Wars", "American History", "Geography"],
                    "Geography": ["Countries", "Continents", "Climate", "Natural Resources"]
                }
                
                if subject in default_topics:
                    topics.update(default_topics[subject])
            
            return sorted(list(topics))
            
        except Exception as e:
            logger.error(f"Failed to get available topics: {e}")
            # Return empty list on error rather than failing
            return []
    
    async def generate_assessment_from_config(
        self,
        config_id: str,
        teacher_uid: str
    ) -> Assessment:
        """
        Generate a new assessment from a configuration.
        
        Args:
            config_id: ID of the assessment configuration
            teacher_uid: UID of the teacher requesting generation
            
        Returns:
            Assessment object with generated questions
        """
        try:
            # Get the configuration
            config = await self.get_assessment_config(config_id)
            
            # Verify ownership
            if config.teacher_uid != teacher_uid:
                raise HTTPException(status_code=403, detail="Access denied")
            
            # Import agent when needed to avoid initialization issues
            from app.agents.assessment_generation.agent import assessment_generation_agent
            
            # Generate assessment using AI agent
            assessment = await assessment_generation_agent.generate_assessment(
                config=config,
                teacher_uid=teacher_uid
            )
            
            # Save assessment to database
            await self._save_assessment(assessment)
            
            logger.info(f"Generated and saved assessment {assessment.assessment_id}")
            return assessment
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to generate assessment: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to generate assessment: {str(e)}"
            )
    
    async def _save_assessment(self, assessment: Assessment) -> None:
        """Save assessment to Firestore."""
        try:
            doc_ref = db.collection(self.assessments_collection).document(assessment.assessment_id)
            doc_ref.set(assessment.dict())
            
        except Exception as e:
            logger.error(f"Failed to save assessment: {e}")
            raise
    
    async def get_teacher_assessments(
        self,
        teacher_uid: str,
        subject_filter: Optional[str] = None,
        active_only: bool = True
    ) -> List[Assessment]:
        """Get all assessments created by a teacher."""
        try:
            query = db.collection(self.assessments_collection).where("teacher_uid", "==", teacher_uid)
            
            if subject_filter:
                query = query.where("subject", "==", subject_filter)
            
            if active_only:
                query = query.where("is_active", "==", True)
            
            docs = query.order_by("created_at", direction="DESCENDING").get()
            
            assessments = []
            for doc in docs:
                assessment_data = doc.to_dict()
                assessments.append(Assessment(**assessment_data))
            
            return assessments
            
        except Exception as e:
            logger.error(f"Failed to get teacher assessments: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve assessments: {str(e)}"
            )
    
    async def get_assessment(self, assessment_id: str) -> Assessment:
        """Get a specific assessment by ID."""
        try:
            doc_ref = db.collection(self.assessments_collection).document(assessment_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                raise HTTPException(status_code=404, detail="Assessment not found")
            
            return Assessment(**doc.to_dict())
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to get assessment: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve assessment: {str(e)}"
            )
    
    async def deactivate_assessment(self, assessment_id: str, teacher_uid: str) -> None:
        """Deactivate an assessment."""
        try:
            # Verify ownership
            assessment = await self.get_assessment(assessment_id)
            if assessment.teacher_uid != teacher_uid:
                raise HTTPException(status_code=403, detail="Access denied")
            
            doc_ref = db.collection(self.assessments_collection).document(assessment_id)
            doc_ref.update({
                "is_active": False,
                "updated_at": datetime.utcnow()
            })
            
            logger.info(f"Deactivated assessment {assessment_id}")
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to deactivate assessment: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to deactivate assessment: {str(e)}"
            )

# Create singleton instance
assessment_service = AssessmentService()


================================================================================
File: app/services/document_processor.py
Size: 12.58 kB
================================================================================

# FILE: app/services/document_processor.py

import logging
import uuid
from typing import List, Dict, Any, Optional
from pathlib import Path
import PyPDF2
try:
    from docx import Document as DocxDocument
except ImportError:
    DocxDocument = None
import re

from app.models.rag_models import DocumentChunk, ProcessedDocument, DocumentProcessingStatus
from app.core.firebase import db
from app.services.vertex_rag_service import VertexAIRAGService

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """Service for processing documents and creating text chunks."""
    
    def __init__(self):
        self.chunk_size = 1000  # Characters per chunk
        self.chunk_overlap = 200  # Overlap between chunks
        self.processed_docs_collection = "processed_documents"
        self.document_chunks_collection = "document_chunks"
        self.vector_service = VertexAIRAGService()
    
    async def process_document(
        self,
        file_path: str,
        document_id: str,
        teacher_uid: str,
        filename: str,
        subject: Optional[str] = None,
        grade_level: Optional[int] = None
    ) -> ProcessedDocument:
        """Process a document and create chunks."""
        
        try:
            logger.info(f"Starting document processing: {document_id}")
            
            # Create initial document record
            processed_doc = ProcessedDocument(
                document_id=document_id,
                teacher_uid=teacher_uid,
                original_filename=filename,
                file_type=self._get_file_type(filename),
                subject=subject,
                grade_level=grade_level,
                processing_status=DocumentProcessingStatus.PROCESSING
            )
            
            # Save initial status
            await self._save_processed_document(processed_doc)
            
            # Extract text based on file type
            text_content = await self._extract_text(file_path, filename)
            
            if not text_content.strip():
                raise ValueError("No text content found in document")
            
            # Create chunks
            chunks = await self._create_chunks(
                text_content=text_content,
                document_id=document_id,
                metadata={
                    "subject": subject,
                    "grade_level": grade_level,
                    "teacher_uid": teacher_uid,
                    "filename": filename
                }
            )
            
            # Save chunks to database and vector store
            await self._save_chunks_to_vector_store(chunks)
            
            # Update document status
            processed_doc.total_chunks = len(chunks)
            processed_doc.processing_status = DocumentProcessingStatus.COMPLETED
            processed_doc.processed_at = processed_doc.created_at
            
            await self._save_processed_document(processed_doc)
            
            logger.info(f"Document processing completed: {document_id}, chunks: {len(chunks)}")
            return processed_doc
            
        except Exception as e:
            logger.error(f"Document processing failed for {document_id}: {str(e)}")
            
            # Update document with error status
            processed_doc.processing_status = DocumentProcessingStatus.FAILED
            processed_doc.processing_error = str(e)
            await self._save_processed_document(processed_doc)
            
            raise e
    
    async def _extract_text(self, file_path: str, filename: str) -> str:
        """Extract text from various file types."""
        
        file_type = self._get_file_type(filename)
        
        try:
            if file_type == "pdf":
                return await self._extract_pdf_text(file_path)
            elif file_type == "docx":
                return await self._extract_docx_text(file_path)
            elif file_type == "txt":
                return await self._extract_txt_text(file_path)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
                
        except Exception as e:
            logger.error(f"Text extraction failed for {filename}: {str(e)}")
            raise e
    
    async def _extract_pdf_text(self, file_path: str) -> str:
        """Extract text from PDF file."""
        text_content = ""
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_content += f"\\n[Page {page_num + 1}]\\n{page_text}\\n"
                        
        except Exception as e:
            logger.error(f"PDF extraction failed: {str(e)}")
            raise e
            
        return text_content.strip()
    
    async def _extract_docx_text(self, file_path: str) -> str:
        """Extract text from DOCX file."""
        text_content = ""
        
        try:
            doc = DocxDocument(file_path)
            
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_content += paragraph.text + "\\n"
                    
        except Exception as e:
            logger.error(f"DOCX extraction failed: {str(e)}")
            raise e
            
        return text_content.strip()
    
    async def _extract_txt_text(self, file_path: str) -> str:
        """Extract text from plain text file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except Exception as e:
            logger.error(f"TXT extraction failed: {str(e)}")
            raise e
    
    async def _create_chunks(
        self,
        text_content: str,
        document_id: str,
        metadata: Dict[str, Any]
    ) -> List[DocumentChunk]:
        """Create overlapping text chunks from document content."""
        
        chunks = []
        
        # Clean and normalize text
        cleaned_text = self._clean_text(text_content)
        
        # Split into sentences first
        sentences = self._split_into_sentences(cleaned_text)
        
        current_chunk = ""
        current_chunk_sentences = []
        chunk_index = 0
        
        for sentence in sentences:
            # Check if adding this sentence would exceed chunk size
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
                current_chunk_sentences.append(sentence)
            else:
                # Save current chunk if it has content
                if current_chunk.strip():
                    chunk = DocumentChunk(
                        chunk_id=str(uuid.uuid4()),
                        document_id=document_id,
                        content=current_chunk.strip(),
                        chunk_index=chunk_index,
                        metadata=metadata
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                
                # Start new chunk with overlap
                overlap_sentences = current_chunk_sentences[-self._calculate_overlap_sentences():]
                current_chunk = " ".join(overlap_sentences + [sentence])
                current_chunk_sentences = overlap_sentences + [sentence]
        
        # Add final chunk
        if current_chunk.strip():
            chunk = DocumentChunk(
                chunk_id=str(uuid.uuid4()),
                document_id=document_id,
                content=current_chunk.strip(),
                chunk_index=chunk_index,
                metadata=metadata
            )
            chunks.append(chunk)
        
        return chunks
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text content."""
        # Remove excessive whitespace
        text = re.sub(r'\\s+', ' ', text)
        
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?;:()\-\"\']', ' ', text)
        
        # Remove multiple spaces
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        # Simple sentence splitting (could be enhanced with NLTK)
        sentence_endings = r'[.!?]+(?=\\s+[A-Z]|$)'
        sentences = re.split(sentence_endings, text)
        
        # Clean and filter sentences
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # Filter out very short fragments
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _calculate_overlap_sentences(self) -> int:
        """Calculate number of sentences to include in overlap."""
        # Rough estimate: if chunk_overlap is 200 chars and avg sentence is ~50 chars
        return max(1, self.chunk_overlap // 50)
    
    def _get_file_type(self, filename: str) -> str:
        """Get file type from filename."""
        return Path(filename).suffix.lower().lstrip('.')
    
    async def _save_processed_document(self, doc: ProcessedDocument) -> None:
        """Save processed document metadata to Firestore."""
        try:
            doc_ref = db.collection(self.processed_docs_collection).document(doc.document_id)
            doc_ref.set(doc.dict())
            logger.info(f"Saved processed document: {doc.document_id}")
        except Exception as e:
            logger.error(f"Failed to save processed document {doc.document_id}: {str(e)}")
            raise e
    
    async def _save_chunks(self, chunks: List[DocumentChunk]) -> None:
        """Save document chunks to Firestore."""
        try:
            batch = db.batch()
            
            for chunk in chunks:
                doc_ref = db.collection(self.document_chunks_collection).document(chunk.chunk_id)
                batch.set(doc_ref, chunk.dict())
            
            batch.commit()
            logger.info(f"Saved {len(chunks)} chunks to Firestore")
            
        except Exception as e:
            logger.error(f"Failed to save chunks: {str(e)}")
            raise e
    
    async def _save_chunks_to_vector_store(self, chunks: List[DocumentChunk]) -> None:
        """Save document chunks to vector store with embeddings."""
        try:
            # Use the vector service to add chunks with embeddings
            success = await self.vector_service.add_chunks(chunks)
            
            if success:
                logger.info(f"Saved {len(chunks)} chunks to vector store with embeddings")
            else:
                logger.error("Failed to save chunks to vector store")
                raise Exception("Vector store save failed")
                
        except Exception as e:
            logger.error(f"Failed to save chunks to vector store: {str(e)}")
            raise e
    
    async def get_processed_document(self, document_id: str) -> Optional[ProcessedDocument]:
        """Get processed document metadata."""
        try:
            doc_ref = db.collection(self.processed_docs_collection).document(document_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return ProcessedDocument(**doc.to_dict())
            return None
            
        except Exception as e:
            logger.error(f"Failed to get processed document {document_id}: {str(e)}")
            return None
    
    async def get_teacher_documents(self, teacher_uid: str) -> List[ProcessedDocument]:
        """Get all processed documents for a teacher."""
        try:
            docs_ref = db.collection(self.processed_docs_collection)
            query = docs_ref.where("teacher_uid", "==", teacher_uid).order_by("created_at", direction="DESCENDING")
            
            docs = query.stream()
            
            processed_docs = []
            for doc in docs:
                processed_docs.append(ProcessedDocument(**doc.to_dict()))
                
            return processed_docs
            
        except Exception as e:
            logger.error(f"Failed to get teacher documents for {teacher_uid}: {str(e)}")
            return []


================================================================================
File: app/services/document_service.py
Size: 22.44 kB
================================================================================

# FILE: app/services/document_service.py

from __future__ import annotations
import logging
import uuid
import asyncio
import zipfile
import io
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import os
import tempfile

from fastapi import UploadFile, HTTPException
from google.cloud import storage
import PyPDF2
from PIL import Image

from app.core.config import settings
from app.core.firebase import db, storage_bucket
from app.models.requests import DocumentUploadResponse, DocumentIndexingStatus, DocumentMetadata, ZipUploadResponse, ExtractedFileInfo
from app.services.vertex_rag_service import vertex_rag_service

logger = logging.getLogger(__name__)

class MockUploadFile:
    """Mock UploadFile for extracted ZIP contents."""
    def __init__(self, filename: str, content: bytes, content_type: str):
        self.filename = filename
        self.content_type = content_type
        self.file = io.BytesIO(content)
        self.size = len(content)
    
    async def read(self, size: int = -1) -> bytes:
        return self.file.read(size)
    
    def seek(self, offset: int, whence: int = 0):
        return self.file.seek(offset, whence)

class DocumentService:
    """Service for handling document upload, storage, and indexing operations."""
    
    def __init__(self):
        self.storage_client = storage.Client()
        self.allowed_file_types = {
            "application/pdf": ".pdf",
            "image/jpeg": ".jpg",
            "image/png": ".png",
            "image/tiff": ".tiff",
            "text/plain": ".txt",
            "application/msword": ".doc",
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
            "application/zip": ".zip",
            "application/x-zip-compressed": ".zip"
        }
        self.max_file_size = 50 * 1024 * 1024  # 50MB
    
    def validate_file(self, file: UploadFile) -> None:
        """
        Validate uploaded file type and size.
        
        Args:
            file: The uploaded file
            
        Raises:
            HTTPException: If file is invalid
        """
        # Check file type
        if file.content_type not in self.allowed_file_types:
            raise HTTPException(
                status_code=400,
                detail=f"File type {file.content_type} not supported. Allowed types: {list(self.allowed_file_types.keys())}"
            )
        
        # Check file size (approximate, based on headers)
        if hasattr(file, 'size') and file.size and file.size > self.max_file_size:
            raise HTTPException(
                status_code=400,
                detail=f"File size {file.size} exceeds maximum allowed size of {self.max_file_size} bytes"
            )
    
    async def extract_zip_contents(
        self, 
        zip_content: bytes, 
        subject: str, 
        grade_level: int,
        teacher_uid: str,
        original_filename: str
    ) -> ZipUploadResponse:
        """
        Extract ZIP file contents and upload each file individually.
        
        Args:
            zip_content: ZIP file content as bytes
            subject: Subject category for documents
            grade_level: Grade level (1-12) for the documents
            teacher_uid: UID of the teacher uploading the documents
            original_filename: Original ZIP filename
            
        Returns:
            ZipUploadResponse with detailed extraction information
        """
        extracted_files = []
        files_processed = 0
        files_skipped = 0
        files_failed = 0
        total_files_found = 0
        
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content), 'r') as zip_ref:
                # Get list of files in the ZIP
                file_list = zip_ref.namelist()
                
                # Filter out directories and system files
                actual_files = [f for f in file_list if not f.endswith('/') and not f.startswith('.') and '/__MACOSX/' not in f]
                total_files_found = len(actual_files)
                
                logger.info(f"Found {total_files_found} processable files in ZIP: {original_filename}")
                
                for file_path in actual_files:
                    try:
                        # Extract file content
                        file_content = zip_ref.read(file_path)
                        
                        # Get just the filename from the path
                        filename = os.path.basename(file_path)
                        if not filename:
                            continue
                        
                        # Determine file type based on extension
                        file_extension = os.path.splitext(filename)[1].lower()
                        content_type = self._get_content_type_from_extension(file_extension)
                        
                        if not content_type:
                            files_skipped += 1
                            extracted_files.append(ExtractedFileInfo(
                                filename=filename,
                                document_id="",
                                file_size=len(file_content),
                                file_type="unknown",
                                extraction_status="skipped",
                                error_message=f"Unsupported file type: {file_extension}"
                            ))
                            logger.warning(f"Skipping unsupported file type: {filename}")
                            continue
                        
                        # Validate file size
                        if len(file_content) > self.max_file_size:
                            files_skipped += 1
                            extracted_files.append(ExtractedFileInfo(
                                filename=filename,
                                document_id="",
                                file_size=len(file_content),
                                file_type=content_type,
                                extraction_status="skipped",
                                error_message=f"File size ({len(file_content)} bytes) exceeds limit ({self.max_file_size} bytes)"
                            ))
                            logger.warning(f"Skipping file {filename}: size exceeds limit")
                            continue
                        
                        # Create a mock UploadFile for the extracted content
                        mock_file = MockUploadFile(filename, file_content, content_type)
                        
                        # Upload the extracted file
                        result = await self._upload_single_document(
                            mock_file, subject, grade_level, teacher_uid, 
                            parent_zip=original_filename
                        )
                        
                        files_processed += 1
                        extracted_files.append(ExtractedFileInfo(
                            filename=filename,
                            document_id=result.document_id,
                            file_size=result.file_size,
                            file_type=result.file_type,
                            extraction_status="success",
                            storage_url=result.storage_url
                        ))
                        
                        logger.info(f"Successfully extracted and uploaded: {filename}")
                        
                    except Exception as e:
                        files_failed += 1
                        extracted_files.append(ExtractedFileInfo(
                            filename=filename if 'filename' in locals() else file_path,
                            document_id="",
                            file_size=len(file_content) if 'file_content' in locals() else 0,
                            file_type=content_type if 'content_type' in locals() else "unknown",
                            extraction_status="failed",
                            error_message=str(e)
                        ))
                        logger.error(f"Failed to process file {file_path} from ZIP: {e}")
                        continue
                        
        except zipfile.BadZipFile:
            raise HTTPException(
                status_code=400,
                detail="Invalid ZIP file format"
            )
        except Exception as e:
            logger.error(f"Failed to process ZIP file {original_filename}: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to process ZIP file: {str(e)}"
            )
        
        return ZipUploadResponse(
            zip_filename=original_filename,
            zip_file_size=len(zip_content),
            total_files_found=total_files_found,
            files_processed=files_processed,
            files_skipped=files_skipped,
            files_failed=files_failed,
            extracted_files=extracted_files,
            upload_status="completed" if files_failed == 0 else "partial",
            subject=subject,
            grade_level=grade_level
        )
    
    def _get_content_type_from_extension(self, extension: str) -> Optional[str]:
        """Get content type from file extension."""
        extension_map = {
            '.pdf': 'application/pdf',
            '.txt': 'text/plain',
            '.doc': 'application/msword',
            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.tiff': 'image/tiff',
            '.tif': 'image/tiff'
        }
        return extension_map.get(extension.lower())
    
    async def _upload_single_document(
        self,
        file: UploadFile,
        subject: str,
        grade_level: int,
        teacher_uid: str,
        parent_zip: Optional[str] = None
    ) -> DocumentUploadResponse:
        """Upload a single document (used for both direct uploads and ZIP extraction)."""
        # Generate unique document ID
        document_id = str(uuid.uuid4())
        
        # Read file content
        file_content = await file.read()
        file_size = len(file_content)
        
        # Create storage path
        file_extension = self.allowed_file_types.get(file.content_type, "")
        if not file_extension:
            # Try to get extension from filename
            file_extension = os.path.splitext(file.filename)[1]
        
        filename_clean = file.filename.replace(" ", "_").replace("/", "_")
        
        # If this file is from a ZIP, include the ZIP name in the path
        if parent_zip:
            zip_name_clean = parent_zip.replace(" ", "_").replace("/", "_").replace(".zip", "")
            storage_path = f"documents/{teacher_uid}/{subject}/{zip_name_clean}/{document_id}_{filename_clean}"
        else:
            storage_path = f"documents/{teacher_uid}/{subject}/{document_id}_{filename_clean}"
        
        # Upload to Firebase Storage
        blob = storage_bucket.blob(storage_path)
        blob.upload_from_string(
            file_content,
            content_type=file.content_type
        )
        
        # Get the public URL
        storage_url = blob.public_url
        
        # Create document metadata
        metadata = DocumentMetadata(
            document_id=document_id,
            teacher_uid=teacher_uid,
            filename=file.filename,
            file_type=file.content_type,
            file_size=file_size,
            subject=subject,
            grade_level=grade_level,
            storage_path=storage_path,
            firebase_url=storage_url,
            upload_date=datetime.utcnow(),
            indexing_status="pending"
        )
        
        # Add parent ZIP info if applicable
        if parent_zip:
            metadata.metadata = {"parent_zip": parent_zip}
        
        # Save metadata to Firestore
        await self.save_document_metadata(metadata)
        
        # Start background indexing task
        asyncio.create_task(self.index_document_background(document_id, metadata))
        
        return DocumentUploadResponse(
            document_id=document_id,
            filename=file.filename,
            file_size=file_size,
            file_type=file.content_type,
            subject=subject,
            grade_level=grade_level,
            upload_status="uploaded",
            storage_url=storage_url
        )
    
    async def upload_document(
        self, 
        file: UploadFile, 
        subject: str, 
        grade_level: int,
        teacher_uid: str
    ) -> Union[DocumentUploadResponse, ZipUploadResponse]:
        """
        Upload a document to Firebase Storage and create metadata.
        Handles both individual files and ZIP archives.
        
        Args:
            file: The uploaded file
            subject: Subject category for the document
            grade_level: Grade level (1-12) for the document
            teacher_uid: UID of the teacher uploading the document
            
        Returns:
            DocumentUploadResponse for single files, ZipUploadResponse for ZIP files
            
        Raises:
            HTTPException: If upload fails
        """
        try:
            # Validate file
            self.validate_file(file)
            
            # Check if this is a ZIP file
            if file.content_type in ["application/zip", "application/x-zip-compressed"]:
                # Read ZIP content
                zip_content = await file.read()
                
                # Extract and upload all files in the ZIP
                zip_response = await self.extract_zip_contents(
                    zip_content, subject, grade_level, teacher_uid, file.filename
                )
                
                if zip_response.files_processed == 0:
                    raise HTTPException(
                        status_code=400,
                        detail="No valid documents could be extracted from the ZIP file"
                    )
                
                return zip_response
            else:
                # Handle single file upload
                return await self._upload_single_document(file, subject, grade_level, teacher_uid)
                
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to upload document: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to upload document: {str(e)}"
            )
    
    async def save_document_metadata(self, metadata: DocumentMetadata) -> None:
        """
        Save document metadata to Firestore.
        
        Args:
            metadata: The document metadata to save
        """
        try:
            doc_ref = db.collection("documents").document(metadata.document_id)
            doc_ref.set(metadata.dict())
            logger.info(f"Saved metadata for document {metadata.document_id}")
        except Exception as e:
            logger.error(f"Failed to save document metadata: {e}")
            raise
    
    async def index_document_background(self, document_id: str, metadata: DocumentMetadata) -> None:
        """
        Background task to index document in Vertex AI RAG.
        
        Args:
            document_id: The document ID
            metadata: Document metadata
        """
        try:
            logger.info(f"Starting indexing for document {document_id}")
            
            # Update status to processing
            await self.update_indexing_status(document_id, "processing", 20)
            
            # Prepare metadata for Vertex AI
            vertex_metadata = {
                "teacher_uid": metadata.teacher_uid,
                "subject": metadata.subject,
                "grade_level": metadata.grade_level,
                "filename": metadata.filename,
                "file_type": metadata.file_type,
                "upload_date": metadata.upload_date.isoformat()
            }
            
            # Import to Vertex AI Search
            vertex_doc_id = await vertex_rag_service.import_document_to_vertex_ai(
                document_id=document_id,
                firebase_storage_url=metadata.firebase_url,
                metadata=vertex_metadata
            )
            
            # Update status to completed
            await self.update_indexing_status(document_id, "completed", 100, vertex_doc_id)
            
            logger.info(f"Successfully indexed document {document_id}")
            
        except Exception as e:
            logger.error(f"Failed to index document {document_id}: {e}")
            await self.update_indexing_status(document_id, "failed", 0, error_message=str(e))
    
    async def update_indexing_status(
        self, 
        document_id: str, 
        status: str, 
        progress: int,
        vertex_ai_index_id: Optional[str] = None,
        error_message: Optional[str] = None
    ) -> None:
        """
        Update the indexing status of a document.
        
        Args:
            document_id: The document ID
            status: New status (pending, processing, completed, failed)
            progress: Progress percentage (0-100)
            vertex_ai_index_id: Vertex AI index ID if completed
            error_message: Error message if failed
        """
        try:
            doc_ref = db.collection("documents").document(document_id)
            update_data = {
                "indexing_status": status,
                "indexing_progress": progress,
                "updated_at": datetime.utcnow()
            }
            
            if vertex_ai_index_id:
                update_data["vertex_ai_index_id"] = vertex_ai_index_id
            
            if error_message:
                update_data["error_message"] = error_message
            
            doc_ref.update(update_data)
            logger.info(f"Updated indexing status for document {document_id}: {status} ({progress}%)")
            
        except Exception as e:
            logger.error(f"Failed to update indexing status: {e}")
    
    async def get_indexing_status(self, document_id: str) -> DocumentIndexingStatus:
        """
        Get the current indexing status of a document.
        
        Args:
            document_id: The document ID
            
        Returns:
            DocumentIndexingStatus with current status
        """
        try:
            doc_ref = db.collection("documents").document(document_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                raise HTTPException(status_code=404, detail="Document not found")
            
            doc_data = doc.to_dict()
            
            return DocumentIndexingStatus(
                document_id=document_id,
                indexing_status=doc_data.get("indexing_status", "unknown"),
                progress_percentage=doc_data.get("indexing_progress", 0),
                vertex_ai_index_id=doc_data.get("vertex_ai_index_id"),
                error_message=doc_data.get("error_message")
            )
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to get indexing status: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to get indexing status: {str(e)}"
            )
    
    async def list_teacher_documents(
        self, 
        teacher_uid: str, 
        subject_filter: Optional[str] = None
    ) -> List[DocumentMetadata]:
        """
        List all documents uploaded by a teacher.
        
        Args:
            teacher_uid: The teacher's UID
            subject_filter: Optional subject filter
            
        Returns:
            List of DocumentMetadata
        """
        try:
            query = db.collection("documents").where("teacher_uid", "==", teacher_uid)
            
            if subject_filter:
                query = query.where("subject", "==", subject_filter)
            
            docs = query.order_by("upload_date", direction="DESCENDING").get()
            
            documents = []
            for doc in docs:
                doc_data = doc.to_dict()
                documents.append(DocumentMetadata(**doc_data))
            
            return documents
            
        except Exception as e:
            logger.error(f"Failed to list teacher documents: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to list documents: {str(e)}"
            )
    
    async def list_documents_with_zip_info(
        self, 
        teacher_uid: str, 
        subject_filter: Optional[str] = None
    ) -> Dict[str, List[DocumentMetadata]]:
        """
        List teacher documents grouped by ZIP files and individual uploads.
        
        Args:
            teacher_uid: UID of the teacher
            subject_filter: Optional filter by subject
            
        Returns:
            Dictionary with 'individual' and 'zip_extractions' keys containing document lists
        """
        try:
            documents = await self.list_teacher_documents(teacher_uid, subject_filter)
            
            individual_docs = []
            zip_groups = {}
            
            for doc in documents:
                # Check if this document was extracted from a ZIP
                if hasattr(doc, 'metadata') and doc.metadata and 'parent_zip' in doc.metadata:
                    parent_zip = doc.metadata['parent_zip']
                    if parent_zip not in zip_groups:
                        zip_groups[parent_zip] = []
                    zip_groups[parent_zip].append(doc)
                else:
                    individual_docs.append(doc)
            
            return {
                'individual': individual_docs,
                'zip_extractions': zip_groups
            }
            
        except Exception as e:
            logger.error(f"Failed to list documents with ZIP info: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to list documents: {str(e)}"
            )

# Create singleton instance
document_service = DocumentService()


================================================================================
File: app/services/enhanced_assessment_service.py
Size: 14.06 kB
================================================================================

# FILE: app/services/enhanced_assessment_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.services.rag_service import RAGService
from app.agents.vertex_question_agent import VertexQuestionAgent
from app.services.simple_assessment_service import SimpleAssessmentService
from app.models.student import AssessmentConfig, Assessment, AssessmentQuestion
from app.models.rag_models import QuestionGenerationRequest
from app.core.firebase import db

logger = logging.getLogger(__name__)

class EnhancedAssessmentService:
    """Enhanced assessment service with RAG and AI question generation."""
    
    def __init__(self):
        self.rag_service = RAGService()
        self.question_agent = VertexQuestionAgent()
        self.simple_service = SimpleAssessmentService()  # Fallback for non-RAG operations
        self.assessments_collection = "assessments"
    
    async def create_rag_assessment(
        self, 
        config: AssessmentConfig,
        force_rag: bool = True
    ) -> Assessment:
        """Create an assessment using RAG and AI generation."""
        
        try:
            logger.info(f"Creating RAG assessment for config {config.config_id}")
            
            # Step 1: Retrieve relevant context using RAG
            rag_results = await self.rag_service.retrieve_context_for_assessment(
                subject=config.subject,
                grade_level=config.target_grade,
                topic=config.topic,
                teacher_uid=config.teacher_uid,
                max_chunks=5
            )
            
            # Step 2: Generate questions using AI agent (with or without RAG context)
            generation_request = QuestionGenerationRequest(
                context_chunks=rag_results if rag_results else [],  # Empty list if no context
                subject=config.subject,
                grade_level=config.target_grade,
                topic=config.topic,
                difficulty_level=config.difficulty_level,
                question_count=config.question_count,
                question_type="multiple_choice"
            )
            
            # Use AI generation even without RAG context
            if rag_results and len(rag_results) >= 2:
                logger.info(f"Using RAG-enhanced AI generation with {len(rag_results)} context chunks")
            else:
                logger.info(f"Using AI generation without RAG context for {config.topic}")
            
            enhanced_questions = await self.question_agent.generate_questions(generation_request)
            
            # Step 3: Convert to standard AssessmentQuestion format
            assessment_questions = []
            for enhanced_q in enhanced_questions:
                standard_q = AssessmentQuestion(
                    question_id=enhanced_q.question_id,
                    question_text=enhanced_q.question_text,
                    options=enhanced_q.options,
                    correct_answer=enhanced_q.correct_answer,
                    explanation=enhanced_q.explanation,
                    difficulty=enhanced_q.difficulty,
                    topic=enhanced_q.topic
                )
                assessment_questions.append(standard_q)
            
            # Step 4: Fill remaining questions with simple generation if needed
            if len(assessment_questions) < config.question_count:
                logger.info(f"Generated {len(assessment_questions)}/{config.question_count} questions, filling remainder with simple generation")
                
                remaining_count = config.question_count - len(assessment_questions)
                simple_questions = await self._generate_simple_questions(
                    config, remaining_count, len(assessment_questions)
                )
                assessment_questions.extend(simple_questions)
            
            # Step 6: Create Assessment object
            assessment_id = str(uuid.uuid4())
            assessment = Assessment(
                assessment_id=assessment_id,
                config_id=config.config_id,
                teacher_uid=config.teacher_uid,  # Add missing teacher_uid
                title=f"{config.topic} - {config.subject} Grade {config.target_grade}",
                subject=config.subject,
                grade=config.target_grade,
                difficulty=config.difficulty_level,
                topic=config.topic,
                questions=assessment_questions[:config.question_count],  # Ensure exact count
                time_limit_minutes=config.time_limit_minutes
            )
            
            # Step 7: Save to Firestore with RAG metadata
            await self._save_rag_assessment(assessment, rag_results, enhanced_questions)
            
            logger.info(f"Created RAG assessment: {assessment_id} with {len(assessment.questions)} questions")
            return assessment
            
        except Exception as e:
            logger.error(f"RAG assessment creation failed: {str(e)}")
            # Fallback to simple assessment
            logger.info("Falling back to simple assessment generation due to error")
            return await self.simple_service.create_sample_assessment(config)
    
    async def _generate_simple_questions(
        self,
        config: AssessmentConfig,
        count: int,
        start_index: int = 0
    ) -> List[AssessmentQuestion]:
        """Generate simple questions when RAG doesn't provide enough."""
        
        # Use the simple service's question generation
        simple_questions = await self.simple_service._generate_sample_questions(
            subject=config.subject,
            grade=config.target_grade,
            topic=config.topic,
            difficulty=config.difficulty_level,
            count=count
        )
        
        # Update question IDs to avoid conflicts
        for i, question in enumerate(simple_questions):
            question.question_id = f"simple_{start_index + i}_{question.question_id}"
        
        return simple_questions
    
    async def _save_rag_assessment(
        self,
        assessment: Assessment,
        rag_results: List,
        enhanced_questions: List
    ) -> None:
        """Save assessment with RAG metadata."""
        
        try:
            # Prepare assessment data
            assessment_data = assessment.dict()
            
            # Add RAG metadata
            assessment_data["generation_method"] = "rag_enhanced"
            assessment_data["rag_metadata"] = {
                "context_chunks_used": len(rag_results),
                "ai_generated_questions": len(enhanced_questions),
                "context_sources": [
                    {
                        "chunk_id": result.chunk.chunk_id,
                        "document_id": result.chunk.document_id,
                        "similarity_score": result.similarity_score,
                        "source_file": result.document_metadata.get("filename", "Unknown")
                    }
                    for result in rag_results
                ],
                "generation_timestamp": datetime.utcnow().isoformat()
            }
            
            # Save to Firestore
            doc_ref = db.collection(self.assessments_collection).document(assessment.assessment_id)
            doc_ref.set(assessment_data)
            
            logger.info(f"Saved RAG assessment with metadata: {assessment.assessment_id}")
            
        except Exception as e:
            logger.error(f"Failed to save RAG assessment: {str(e)}")
            raise e
    
    async def get_assessment_with_metadata(self, assessment_id: str) -> Optional[Dict[str, Any]]:
        """Get assessment with RAG metadata."""
        
        try:
            doc_ref = db.collection(self.assessments_collection).document(assessment_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return doc.to_dict()
            return None
            
        except Exception as e:
            logger.error(f"Failed to get assessment with metadata {assessment_id}: {str(e)}")
            return None
    
    async def get_teacher_rag_statistics(self, teacher_uid: str) -> Dict[str, Any]:
        """Get RAG usage statistics for a teacher."""
        
        try:
            # Get teacher's assessments
            assessments_ref = db.collection(self.assessments_collection)
            # Note: This would require getting configs first to find teacher's assessments
            # For now, return basic stats
            
            content_stats = await self.rag_service.get_teacher_content_stats(teacher_uid)
            
            return {
                "content_statistics": content_stats,
                "rag_enabled": True,
                "total_documents": content_stats.get("total_documents", 0),
                "total_chunks": content_stats.get("total_chunks", 0),
                "subjects_covered": content_stats.get("subjects", []),
                "grade_levels_covered": content_stats.get("grade_levels", [])
            }
            
        except Exception as e:
            logger.error(f"Failed to get RAG statistics for {teacher_uid}: {str(e)}")
            return {
                "content_statistics": {},
                "rag_enabled": False,
                "error": str(e)
            }
    
    async def search_teacher_content(
        self,
        teacher_uid: str,
        search_query: str,
        subject_filter: Optional[str] = None,
        grade_filter: Optional[int] = None
    ) -> Dict[str, Any]:
        """Search teacher's uploaded content."""
        
        try:
            results = await self.rag_service.search_documents_by_content(
                search_text=search_query,
                teacher_uid=teacher_uid,
                subject_filter=subject_filter,
                grade_filter=grade_filter,
                max_results=10
            )
            
            context_summary = await self.rag_service.get_context_summary(results)
            
            return {
                "search_query": search_query,
                "results_found": len(results),
                "summary": context_summary,
                "results": [
                    {
                        "chunk_id": r.chunk.chunk_id,
                        "content_preview": r.chunk.content[:200] + "..." if len(r.chunk.content) > 200 else r.chunk.content,
                        "similarity_score": round(r.similarity_score, 3),
                        "source_document": r.document_metadata.get("filename", "Unknown"),
                        "subject": r.chunk.metadata.get("subject", ""),
                        "grade_level": r.chunk.metadata.get("grade_level", 0)
                    }
                    for r in results
                ]
            }
            
        except Exception as e:
            logger.error(f"Content search failed for {teacher_uid}: {str(e)}")
            return {
                "search_query": search_query,
                "results_found": 0,
                "error": str(e),
                "results": []
            }
    
    # Delegate methods to simple service for backwards compatibility
    async def create_assessment_config(
        self, 
        name: str,
        subject: str,
        target_grade: int,
        difficulty_level: str,
        topic: str,
        teacher_uid: str,
        question_count: int = 10,
        time_limit_minutes: int = 30
    ) -> AssessmentConfig:
        """Create assessment config (delegates to simple service)."""
        return await self.simple_service.create_assessment_config(
            name=name,
            subject=subject,
            target_grade=target_grade,
            difficulty_level=difficulty_level,
            topic=topic,
            teacher_uid=teacher_uid,
            question_count=question_count,
            time_limit_minutes=time_limit_minutes
        )
    
    async def get_teacher_assessment_configs(
        self, 
        teacher_uid: str,
        subject_filter: Optional[str] = None
    ) -> List[AssessmentConfig]:
        """Get teacher assessment configs (delegates to simple service)."""
        return await self.simple_service.get_teacher_assessment_configs(
            teacher_uid=teacher_uid,
            subject_filter=subject_filter
        )
    
    async def get_assessment_config_by_id(
        self,
        config_id: str,
        teacher_uid: str
    ) -> Optional[AssessmentConfig]:
        """Get assessment config by ID (delegates to simple service)."""
        return await self.simple_service.get_assessment_config_by_id(
            config_id=config_id,
            teacher_uid=teacher_uid
        )
    
    async def get_assessment_by_id(self, assessment_id: str) -> Optional[Assessment]:
        """Get assessment by ID."""
        try:
            doc_ref = db.collection(self.assessments_collection).document(assessment_id)
            doc = doc_ref.get()
            
            if doc.exists:
                data = doc.to_dict()
                # Remove RAG metadata for standard response
                if "rag_metadata" in data:
                    del data["rag_metadata"]
                if "generation_method" in data:
                    del data["generation_method"]
                    
                return Assessment(**data)
            return None
            
        except Exception as e:
            logger.error(f"Failed to get assessment {assessment_id}: {str(e)}")
            return None

    async def get_available_topics(
        self, 
        subject: str, 
        grade: int, 
        teacher_uid: str
    ) -> List[str]:
        """Delegate to simple service for topics."""
        return await self.simple_service.get_available_topics(
            subject=subject,
            grade=grade,
            teacher_uid=teacher_uid
        )

# Global instance
enhanced_assessment_service = EnhancedAssessmentService()


================================================================================
File: app/services/learning_path_monitoring_service.py
Size: 12.64 kB
================================================================================

# FILE: app/services/learning_path_monitoring_service.py

import logging
import asyncio
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta

from app.core.firebase import db
from app.agents.learning_path_agent.agent import root_agent as learning_path_agent
from app.agents.tools.learning_path_tools import (
    monitor_student_assessments,
    analyze_assessment_completion,
    track_learning_progress
)

logger = logging.getLogger(__name__)

class LearningPathMonitoringService:
    """
    Service that coordinates with the Learning Path Agent to automatically 
    monitor and respond to student learning events.
    """
    
    def __init__(self):
        self.agent = learning_path_agent
        self.monitoring_active = False
        self.event_listeners = {}
        self.monitoring_tasks = {}
    
    async def start_monitoring(self, teacher_uid: str) -> Dict[str, Any]:
        """
        Start automated monitoring for a teacher's students.
        
        Args:
            teacher_uid: Teacher to monitor students for
        
        Returns:
            Dict containing monitoring status
        """
        try:
            logger.info(f"Starting automated learning path monitoring for teacher {teacher_uid}")
            
            # Activate monitoring through the agent
            monitoring_result = await monitor_student_assessments(teacher_uid, continuous=True)
            
            # Set up event listeners for real-time monitoring
            await self._setup_firestore_listeners(teacher_uid)
            
            # Schedule periodic progress checks
            await self._schedule_progress_monitoring(teacher_uid)
            
            self.monitoring_active = True
            
            result = {
                "teacher_uid": teacher_uid,
                "monitoring_started": True,
                "agent_activated": True,
                "real_time_listeners": "active",
                "progress_monitoring": "scheduled",
                "monitoring_result": monitoring_result
            }
            
            logger.info(f"Automated monitoring activated for teacher {teacher_uid}")
            return result
            
        except Exception as e:
            logger.error(f"Failed to start monitoring: {str(e)}")
            return {
                "error": f"Monitoring startup failed: {str(e)}",
                "teacher_uid": teacher_uid,
                "monitoring_started": False
            }
    
    async def handle_assessment_completion(
        self,
        student_id: str,
        assessment_id: str,
        student_answers: List[int],
        time_taken_minutes: int,
        teacher_uid: str
    ) -> Dict[str, Any]:
        """
        Handle an assessment completion event and trigger agent response.
        
        Args:
            student_id: Student who completed assessment
            assessment_id: Assessment that was completed
            student_answers: Student's answers
            time_taken_minutes: Time taken
            teacher_uid: Teacher who owns the assessment
        
        Returns:
            Dict containing agent's response and actions taken
        """
        try:
            logger.info(f"Processing assessment completion: {assessment_id} by student {student_id}")
            
            # Trigger the learning path agent to analyze and respond
            agent_response = await self._trigger_agent_analysis(
                student_id, assessment_id, student_answers, time_taken_minutes
            )
            
            # Log the event and response
            await self._log_monitoring_event(
                event_type="assessment_completion",
                student_id=student_id,
                assessment_id=assessment_id,
                teacher_uid=teacher_uid,
                agent_response=agent_response
            )
            
            return {
                "event": "assessment_completion",
                "student_id": student_id,
                "assessment_id": assessment_id,
                "processed": True,
                "agent_response": agent_response,
                "learning_path_generated": agent_response.get("learning_path_generated", False),
                "intervention_type": agent_response.get("intervention_type"),
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to handle assessment completion: {str(e)}")
            return {
                "error": f"Assessment completion handling failed: {str(e)}",
                "student_id": student_id,
                "assessment_id": assessment_id,
                "processed": False
            }
    
    async def monitor_learning_path_progress(self, path_id: str, student_id: str) -> Dict[str, Any]:
        """
        Monitor progress on a learning path and trigger adaptations if needed.
        
        Args:
            path_id: Learning path to monitor
            student_id: Student progressing through path
        
        Returns:
            Dict containing monitoring results
        """
        try:
            logger.info(f"Monitoring learning path progress: {path_id} for student {student_id}")
            
            # Use agent tools to track progress
            progress_result = await track_learning_progress(student_id, path_id)
            
            # Check if agent recommendations require action
            if progress_result.get("adaptation_triggered"):
                logger.info(f"Learning path {path_id} was automatically adapted")
            
            return {
                "path_id": path_id,
                "student_id": student_id,
                "monitoring_completed": True,
                "progress_analysis": progress_result,
                "adaptations_made": progress_result.get("adaptation_triggered", False),
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to monitor learning path progress: {str(e)}")
            return {
                "error": f"Progress monitoring failed: {str(e)}",
                "path_id": path_id,
                "student_id": student_id
            }
    
    async def process_batch_assessments(self, teacher_uid: str) -> Dict[str, Any]:
        """
        Process all pending assessments for a teacher's students.
        
        Args:
            teacher_uid: Teacher to process assessments for
        
        Returns:
            Dict containing batch processing results
        """
        try:
            logger.info(f"Processing batch assessments for teacher {teacher_uid}")
            
            # Trigger agent batch monitoring
            batch_result = await monitor_student_assessments(teacher_uid, continuous=False)
            
            return {
                "teacher_uid": teacher_uid,
                "batch_processing_completed": True,
                "agent_result": batch_result,
                "assessments_processed": batch_result.get("assessments_processed", 0),
                "learning_paths_generated": batch_result.get("learning_paths_generated", 0),
                "students_helped": batch_result.get("students_helped", []),
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to process batch assessments: {str(e)}")
            return {
                "error": f"Batch processing failed: {str(e)}",
                "teacher_uid": teacher_uid
            }
    
    async def get_monitoring_status(self, teacher_uid: str) -> Dict[str, Any]:
        """
        Get current monitoring status for a teacher.
        
        Args:
            teacher_uid: Teacher to get status for
        
        Returns:
            Dict containing monitoring status
        """
        try:
            # Get monitoring statistics from recent activity
            monitoring_stats = await self._get_monitoring_statistics(teacher_uid)
            
            return {
                "teacher_uid": teacher_uid,
                "monitoring_active": self.monitoring_active,
                "agent_status": "active" if self.monitoring_active else "inactive",
                "recent_activity": monitoring_stats,
                "event_listeners": len(self.event_listeners),
                "scheduled_tasks": len(self.monitoring_tasks),
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to get monitoring status: {str(e)}")
            return {
                "error": f"Status retrieval failed: {str(e)}",
                "teacher_uid": teacher_uid
            }
    
    # Private helper methods
    
    async def _trigger_agent_analysis(
        self,
        student_id: str,
        assessment_id: str,
        student_answers: List[int],
        time_taken_minutes: int
    ) -> Dict[str, Any]:
        """Trigger the learning path agent to analyze assessment completion."""
        
        # Use the agent's analysis tool directly
        return await analyze_assessment_completion(
            student_id, assessment_id, student_answers, time_taken_minutes
        )
    
    async def _setup_firestore_listeners(self, teacher_uid: str) -> None:
        """Set up Firestore listeners for real-time monitoring."""
        
        try:
            # This would set up real-time listeners for assessment completions
            # For now, we'll simulate the setup
            logger.info(f"Setting up Firestore listeners for teacher {teacher_uid}")
            
            # In a real implementation, this would use Firestore listeners:
            # listener = db.collection('assessments').where('teacher_uid', '==', teacher_uid).on_snapshot(callback)
            # self.event_listeners[teacher_uid] = listener
            
            self.event_listeners[teacher_uid] = "simulated_listener"
            
        except Exception as e:
            logger.warning(f"Failed to setup Firestore listeners: {str(e)}")
    
    async def _schedule_progress_monitoring(self, teacher_uid: str) -> None:
        """Schedule periodic progress monitoring tasks."""
        
        try:
            # This would schedule periodic tasks to check learning path progress
            logger.info(f"Scheduling progress monitoring for teacher {teacher_uid}")
            
            # In a real implementation, this would use a task scheduler
            self.monitoring_tasks[teacher_uid] = "scheduled_monitoring"
            
        except Exception as e:
            logger.warning(f"Failed to schedule progress monitoring: {str(e)}")
    
    async def _log_monitoring_event(
        self,
        event_type: str,
        student_id: str,
        assessment_id: str,
        teacher_uid: str,
        agent_response: Dict[str, Any]
    ) -> None:
        """Log monitoring events for analytics and debugging."""
        
        try:
            event_log = {
                "timestamp": datetime.utcnow(),
                "event_type": event_type,
                "student_id": student_id,
                "assessment_id": assessment_id,
                "teacher_uid": teacher_uid,
                "agent_response": agent_response,
                "monitoring_service": "active"
            }
            
            # Save to monitoring logs collection
            db.collection("monitoring_logs").add(event_log)
            
        except Exception as e:
            logger.warning(f"Failed to log monitoring event: {str(e)}")
    
    async def _get_monitoring_statistics(self, teacher_uid: str) -> Dict[str, Any]:
        """Get recent monitoring statistics for a teacher."""
        
        try:
            # Query recent monitoring activity
            # This would query the monitoring_logs collection in a real implementation
            
            # For now, return simulated statistics
            return {
                "assessments_processed_today": 0,
                "learning_paths_generated_today": 0,
                "students_active": 0,
                "average_response_time_seconds": 2.5,
                "last_activity": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logger.warning(f"Failed to get monitoring statistics: {str(e)}")
            return {}

# Global instance
learning_path_monitoring_service = LearningPathMonitoringService()


================================================================================
File: app/services/learning_path_service.py
Size: 23.68 kB
================================================================================

# FILE: app/services/learning_path_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

from app.models.learning_models import (
    LearningPath, LearningStep, KnowledgeGap, StudentPerformance,
    DifficultyLevel, LearningObjectiveType, LearningRecommendation
)
from app.core.firebase import db
from app.core.vertex import get_vertex_model

logger = logging.getLogger(__name__)

class LearningPathService:
    """Service for generating and managing personalized learning paths."""
    
    def __init__(self):
        self.model = get_vertex_model("gemini-2.5-pro")
        self.learning_paths_collection = "learning_paths"
        self.content_library_collection = "learning_content"
    
    async def generate_personalized_learning_path(
        self,
        student_id: str,
        teacher_uid: str,
        knowledge_gaps: List[KnowledgeGap],
        student_performances: List[StudentPerformance],
        target_subject: str,
        target_grade: int,
        learning_goals: Optional[List[str]] = None
    ) -> LearningPath:
        """Generate a personalized learning path based on assessment analysis."""
        
        try:
            logger.info(f"Generating learning path for student {student_id} in {target_subject}")
            
            # Analyze student's current state
            current_state = await self._analyze_student_current_state(
                student_id, knowledge_gaps, student_performances
            )
            
            # Generate learning steps using AI
            learning_steps = await self._generate_learning_steps_with_ai(
                current_state, knowledge_gaps, target_subject, target_grade, learning_goals
            )
            
            # Create learning path
            path_id = str(uuid.uuid4())
            learning_path = LearningPath(
                path_id=path_id,
                student_id=student_id,
                teacher_uid=teacher_uid,
                title=f"Personalized {target_subject} Learning Path",
                description=f"Customized learning journey to address knowledge gaps and achieve learning goals",
                subject=target_subject,
                target_grade=target_grade,
                learning_goals=learning_goals or self._generate_default_learning_goals(knowledge_gaps),
                addresses_gaps=[gap.gap_id for gap in knowledge_gaps],
                steps=learning_steps,
                total_estimated_duration_minutes=sum(step.estimated_duration_minutes for step in learning_steps),
                source_assessments=[gap.source_assessments[0] for gap in knowledge_gaps if gap.source_assessments]
            )
            
            # Save to Firestore
            await self._save_learning_path(learning_path)
            
            # Generate lessons for each learning step
            await self._generate_lessons_for_steps(learning_path)
            
            logger.info(f"Generated learning path {path_id} with {len(learning_steps)} steps and lessons")
            return learning_path
            
        except Exception as e:
            logger.error(f"Failed to generate learning path: {str(e)}")
            raise e
    
    async def _analyze_student_current_state(
        self,
        student_id: str,
        knowledge_gaps: List[KnowledgeGap],
        performances: List[StudentPerformance]
    ) -> Dict[str, Any]:
        """Analyze student's current learning state."""
        
        try:
            # Calculate current proficiency levels
            topic_proficiencies = {}
            difficulty_comfort = {}
            learning_style_indicators = {}
            
            if performances:
                # Aggregate topic performance
                for perf in performances:
                    for topic, score in perf.topic_scores.items():
                        if topic not in topic_proficiencies:
                            topic_proficiencies[topic] = []
                        topic_proficiencies[topic].append(score)
                
                # Average scores by topic
                topic_proficiencies = {
                    topic: sum(scores) / len(scores)
                    for topic, scores in topic_proficiencies.items()
                }
                
                # Aggregate difficulty comfort
                for perf in performances:
                    for difficulty, score in perf.difficulty_scores.items():
                        if difficulty not in difficulty_comfort:
                            difficulty_comfort[difficulty] = []
                        difficulty_comfort[difficulty].append(score)
                
                difficulty_comfort = {
                    difficulty: sum(scores) / len(scores)
                    for difficulty, scores in difficulty_comfort.items()
                }
                
                # Analyze learning patterns
                avg_time = sum(p.time_taken_minutes for p in performances) / len(performances)
                avg_score = sum(p.score_percentage for p in performances) / len(performances)
                
                learning_style_indicators = {
                    "prefers_time": "extended" if avg_time > 45 else "standard",
                    "current_level": "advanced" if avg_score > 85 else "intermediate" if avg_score > 65 else "beginner",
                    "consistency": "high" if max(p.score_percentage for p in performances) - min(p.score_percentage for p in performances) < 20 else "variable"
                }
            
            # Analyze knowledge gaps
            gap_analysis = {
                "critical_gaps": [gap for gap in knowledge_gaps if gap.severity_score > 0.8],
                "moderate_gaps": [gap for gap in knowledge_gaps if 0.5 < gap.severity_score <= 0.8],
                "minor_gaps": [gap for gap in knowledge_gaps if gap.severity_score <= 0.5],
                "primary_subjects": list(set(gap.subject for gap in knowledge_gaps)),
                "difficulty_gaps": list(set(gap.difficulty_level for gap in knowledge_gaps))
            }
            
            return {
                "student_id": student_id,
                "topic_proficiencies": topic_proficiencies,
                "difficulty_comfort": difficulty_comfort,
                "learning_style_indicators": learning_style_indicators,
                "gap_analysis": gap_analysis,
                "total_assessments": len(performances),
                "overall_performance": sum(p.score_percentage for p in performances) / len(performances) if performances else 0
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze student state: {str(e)}")
            return {"error": str(e)}
    
    async def _generate_learning_steps_with_ai(
        self,
        current_state: Dict[str, Any],
        knowledge_gaps: List[KnowledgeGap],
        subject: str,
        grade: int,
        learning_goals: Optional[List[str]]
    ) -> List[LearningStep]:
        """Use AI to generate personalized learning steps."""
        
        try:
            # Prepare prompt for AI
            prompt = f"""You are an expert educational path designer. Create a personalized learning sequence for a student based on their current state and knowledge gaps.

STUDENT CURRENT STATE:
- Grade Level: {grade}
- Subject: {subject}
- Overall Performance: {current_state.get('overall_performance', 0):.1f}%
- Learning Style: {current_state.get('learning_style_indicators', {})}
- Topic Proficiencies: {current_state.get('topic_proficiencies', {})}
- Difficulty Comfort: {current_state.get('difficulty_comfort', {})}

KNOWLEDGE GAPS TO ADDRESS:
{self._format_knowledge_gaps_for_ai(knowledge_gaps)}

LEARNING GOALS:
{learning_goals or ['Improve understanding in identified weak areas', 'Build confidence in the subject']}

Create a sequence of 8-12 learning steps that will systematically address the knowledge gaps and help the student achieve their learning goals. Each step should build on the previous ones.

Return your response in this JSON format:

{{
  "learning_steps": [
    {{
      "title": "Step title",
      "description": "What this step covers and why it's important",
      "topic": "main topic",
      "subtopic": "specific subtopic or null",
      "difficulty_level": "beginner/easy/medium/hard/advanced",
      "learning_objective": "remember/understand/apply/analyze/evaluate/create",
      "content_type": "explanation/practice/video/reading/interactive",
      "content_text": "Specific instructions or content for this step",
      "estimated_duration_minutes": 15-60,
      "addresses_gaps": ["gap_id1", "gap_id2"],
      "prerequisites": ["previous_step_titles"]
    }}
  ],
  "path_rationale": "Why this sequence was chosen",
  "key_progression": "How steps build on each other",
  "expected_outcomes": ["outcome1", "outcome2"]
}}

Generate the learning path now:"""

            # Get AI response
            response = self.model.generate_content(prompt)
            ai_plan = self._parse_ai_learning_plan(response.text)
            
            # Convert AI response to LearningStep objects
            learning_steps = []
            
            for i, step_data in enumerate(ai_plan.get("learning_steps", [])):
                step = LearningStep(
                    step_id=str(uuid.uuid4()),
                    step_number=i + 1,
                    title=step_data.get("title", f"Learning Step {i + 1}"),
                    description=step_data.get("description", ""),
                    subject=subject,
                    topic=step_data.get("topic", "General"),
                    subtopic=step_data.get("subtopic"),
                    difficulty_level=DifficultyLevel(step_data.get("difficulty_level", "medium")),
                    learning_objective=LearningObjectiveType(step_data.get("learning_objective", "understand")),
                    content_type=step_data.get("content_type", "explanation"),
                    content_text=step_data.get("content_text", ""),
                    estimated_duration_minutes=step_data.get("estimated_duration_minutes", 20),
                    addresses_gaps=step_data.get("addresses_gaps", []),
                    prerequisites=step_data.get("prerequisites", [])
                )
                learning_steps.append(step)
            
            # If AI generation fails, create basic steps
            if not learning_steps:
                learning_steps = await self._generate_fallback_learning_steps(
                    knowledge_gaps, subject, grade
                )
            
            return learning_steps
            
        except Exception as e:
            logger.warning(f"AI step generation failed, using fallback: {str(e)}")
            return await self._generate_fallback_learning_steps(knowledge_gaps, subject, grade)
    
    async def _generate_fallback_learning_steps(
        self,
        knowledge_gaps: List[KnowledgeGap],
        subject: str,
        grade: int
    ) -> List[LearningStep]:
        """Generate basic learning steps when AI generation fails."""
        
        steps = []
        
        # Create a step for each knowledge gap
        for i, gap in enumerate(knowledge_gaps[:8]):  # Limit to 8 steps
            step = LearningStep(
                step_id=str(uuid.uuid4()),
                step_number=i + 1,
                title=f"Review {gap.topic}",
                description=f"Strengthen your understanding of {gap.topic} concepts",
                subject=subject,
                topic=gap.topic,
                difficulty_level=gap.difficulty_level,
                learning_objective=gap.learning_objective,
                content_type="practice",
                content_text=f"Practice problems and review concepts related to {gap.topic}",
                estimated_duration_minutes=25,
                addresses_gaps=[gap.gap_id]
            )
            steps.append(step)
        
        return steps
    
    async def update_learning_path_progress(
        self,
        path_id: str,
        step_id: str,
        completed: bool,
        performance_score: Optional[float] = None
    ) -> LearningPath:
        """Update progress on a learning path step."""
        
        try:
            # Get current path
            path = await self.get_learning_path(path_id)
            if not path:
                raise ValueError(f"Learning path {path_id} not found")
            
            # Find and update the step
            for step in path.steps:
                if step.step_id == step_id:
                    step.is_completed = completed
                    step.completed_at = datetime.utcnow() if completed else None
                    step.performance_score = performance_score
                    break
            
            # Update path progress
            completed_steps = sum(1 for step in path.steps if step.is_completed)
            path.completion_percentage = (completed_steps / len(path.steps)) * 100 if path.steps else 0
            path.current_step = completed_steps
            path.last_updated = datetime.utcnow()
            
            if path.completion_percentage >= 100:
                path.completed_at = datetime.utcnow()
            
            # Save updated path
            await self._save_learning_path(path)
            
            logger.info(f"Updated progress for path {path_id}: {path.completion_percentage:.1f}% complete")
            return path
            
        except Exception as e:
            logger.error(f"Failed to update learning path progress: {str(e)}")
            raise e
    
    async def get_learning_path(self, path_id: str) -> Optional[LearningPath]:
        """Get a learning path by ID."""
        
        try:
            doc_ref = db.collection(self.learning_paths_collection).document(path_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return LearningPath(**doc.to_dict())
            return None
            
        except Exception as e:
            logger.error(f"Failed to get learning path {path_id}: {str(e)}")
            return None
    
    async def get_student_learning_paths(self, student_id: str) -> List[LearningPath]:
        """Get all learning paths for a student."""
        
        try:
            query = (db.collection(self.learning_paths_collection)
                    .where("student_id", "==", student_id)
                    .order_by("created_at", direction="DESCENDING"))
            
            docs = query.get()
            return [LearningPath(**doc.to_dict()) for doc in docs]
            
        except Exception as e:
            logger.error(f"Failed to get learning paths for student {student_id}: {str(e)}")
            return []
    
    async def adapt_learning_path(
        self,
        path_id: str,
        new_performance_data: StudentPerformance,
        new_gaps: List[KnowledgeGap]
    ) -> LearningPath:
        """Adapt an existing learning path based on new assessment results."""
        
        try:
            path = await self.get_learning_path(path_id)
            if not path:
                raise ValueError(f"Learning path {path_id} not found")
            
            # Analyze if adaptation is needed
            adaptation_needed = await self._assess_adaptation_need(
                path, new_performance_data, new_gaps
            )
            
            if adaptation_needed:
                # Generate new steps for unaddressed gaps
                new_steps = await self._generate_adaptive_steps(
                    path, new_performance_data, new_gaps
                )
                
                # Add new steps to path
                for step in new_steps:
                    step.step_number = len(path.steps) + 1
                    path.steps.append(step)
                
                # Update path metadata
                path.addresses_gaps.extend([gap.gap_id for gap in new_gaps])
                path.total_estimated_duration_minutes += sum(
                    step.estimated_duration_minutes for step in new_steps
                )
                path.last_updated = datetime.utcnow()
                
                # Record adaptation
                adaptation_record = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "trigger": "new_assessment_results",
                    "changes": f"Added {len(new_steps)} new steps",
                    "new_gaps_addressed": len(new_gaps)
                }
                path.adaptation_history.append(adaptation_record)
                
                # Save adapted path
                await self._save_learning_path(path)
                
                logger.info(f"Adapted learning path {path_id} with {len(new_steps)} new steps")
            
            return path
            
        except Exception as e:
            logger.error(f"Failed to adapt learning path: {str(e)}")
            raise e
    
    # Helper methods
    def _format_knowledge_gaps_for_ai(self, gaps: List[KnowledgeGap]) -> str:
        """Format knowledge gaps for AI prompt."""
        formatted = []
        for gap in gaps:
            formatted.append(
                f"- {gap.topic} ({gap.difficulty_level.value}): "
                f"Severity {gap.severity_score:.2f}, "
                f"Confidence {gap.confidence_score:.2f}, "
                f"Frequency {gap.frequency}"
            )
        return "\\n".join(formatted)
    
    def _parse_ai_learning_plan(self, response_text: str) -> Dict[str, Any]:
        """Parse AI learning plan response."""
        try:
            import json
            
            # Extract JSON from response
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_text = response_text[start_idx:end_idx]
                return json.loads(json_text)
            
            return {"learning_steps": []}
            
        except Exception as e:
            logger.warning(f"Failed to parse AI learning plan: {str(e)}")
            return {"learning_steps": []}
    
    def _generate_default_learning_goals(self, gaps: List[KnowledgeGap]) -> List[str]:
        """Generate default learning goals based on knowledge gaps."""
        goals = []
        
        if gaps:
            topics = list(set(gap.topic for gap in gaps))
            goals.extend([f"Master {topic} concepts" for topic in topics[:3]])
            
            if any(gap.severity_score > 0.8 for gap in gaps):
                goals.append("Address critical knowledge gaps")
            
            goals.append("Build confidence in problem-solving")
        
        return goals or ["Improve overall understanding", "Build foundational skills"]
    
    async def _assess_adaptation_need(
        self,
        path: LearningPath,
        new_performance: StudentPerformance,
        new_gaps: List[KnowledgeGap]
    ) -> bool:
        """Assess if learning path needs adaptation."""
        
        # Check for new gaps not addressed by current path
        current_gap_ids = set(path.addresses_gaps)
        new_gap_ids = set(gap.gap_id for gap in new_gaps)
        
        unaddressed_gaps = new_gap_ids - current_gap_ids
        
        # Check for declining performance
        performance_declining = new_performance.score_percentage < 60
        
        return len(unaddressed_gaps) > 0 or performance_declining
    
    async def _generate_adaptive_steps(
        self,
        current_path: LearningPath,
        new_performance: StudentPerformance,
        new_gaps: List[KnowledgeGap]
    ) -> List[LearningStep]:
        """Generate new steps to address newly identified gaps."""
        
        # For now, create simple adaptive steps
        # This could be enhanced with more sophisticated AI analysis
        
        adaptive_steps = []
        
        for gap in new_gaps:
            if gap.gap_id not in current_path.addresses_gaps:
                step = LearningStep(
                    step_id=str(uuid.uuid4()),
                    step_number=0,  # Will be set later
                    title=f"Address {gap.topic} Gap",
                    description=f"Additional practice for {gap.topic} based on recent assessment",
                    subject=current_path.subject,
                    topic=gap.topic,
                    difficulty_level=gap.difficulty_level,
                    learning_objective=gap.learning_objective,
                    content_type="adaptive_practice",
                    content_text=f"Targeted practice to address identified gap in {gap.topic}",
                    estimated_duration_minutes=20,
                    addresses_gaps=[gap.gap_id]
                )
                adaptive_steps.append(step)
        
        return adaptive_steps
    
    async def _save_learning_path(self, path: LearningPath) -> None:
        """Save learning path to Firestore."""
        doc_ref = db.collection(self.learning_paths_collection).document(path.path_id)
        doc_ref.set(path.dict())
    
    async def _generate_lessons_for_steps(self, learning_path: LearningPath) -> None:
        """Generate interactive lessons for each learning step in the path."""
        try:
            # Import here to avoid circular imports
            from app.services.lesson_service import lesson_service
            
            logger.info(f"Generating lessons for {len(learning_path.steps)} learning steps")
            
            for step in learning_path.steps:
                try:
                    # Generate lesson for this step
                    lesson_result = await lesson_service.create_lesson_from_step(
                        learning_step_id=step.step_id,
                        student_id=learning_path.student_id,
                        teacher_uid=learning_path.teacher_uid,
                        customizations={
                            "learning_path_context": {
                                "path_id": learning_path.path_id,
                                "step_number": step.step_number,
                                "total_steps": len(learning_path.steps),
                                "subject": learning_path.subject,
                                "target_grade": learning_path.target_grade
                            }
                        }
                    )
                    
                    if lesson_result.get("success"):
                        # Update step with lesson ID
                        step.content_url = f"/lessons/{lesson_result['lesson_id']}"
                        logger.info(f"Generated lesson {lesson_result['lesson_id']} for step {step.step_id}")
                    else:
                        logger.warning(f"Failed to generate lesson for step {step.step_id}: {lesson_result.get('error')}")
                        
                except Exception as e:
                    logger.error(f"Error generating lesson for step {step.step_id}: {str(e)}")
                    continue
            
            # Update the learning path with lesson URLs
            await self._save_learning_path(learning_path)
            
        except Exception as e:
            logger.error(f"Failed to generate lessons for learning path: {str(e)}")
            # Don't fail the entire path creation if lesson generation fails

# Global instance
learning_path_service = LearningPathService()


================================================================================
File: app/services/lesson_service.py
Size: 21.66 kB
================================================================================

# FILE: app/services/lesson_service.py

import logging
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime

from app.core.firebase import db
from app.models.lesson_models import (
    LessonContent, LessonSlide, LessonProgress, LessonChatSession
)
from app.models.learning_models import LearningStep
from app.agents.lesson_agent.agent import lesson_agent
from app.agents.tools.lesson_tools import (
    generate_lesson_content,
    get_lesson_content,
    update_lesson_progress,
    start_lesson_chat,
    send_chat_message
)

logger = logging.getLogger(__name__)

class LessonService:
    """Service for managing lesson content generation and interaction."""
    
    def __init__(self):
        self.lessons_collection = "lessons"
        self.progress_collection = "lesson_progress"
        self.chats_collection = "lesson_chats"
        
    async def create_lesson_from_step(
        self,
        learning_step_id: str,
        student_id: str,
        teacher_uid: str,
        customizations: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a new lesson from a learning step.
        
        Args:
            learning_step_id: ID of the learning step
            student_id: Student the lesson is for
            teacher_uid: Teacher who owns the learning path
            customizations: Optional lesson customizations
            
        Returns:
            Dict containing lesson creation results
        """
        try:
            logger.info(f"Creating lesson for step {learning_step_id}, student {student_id}")
            
            # Generate lesson content using the agent
            result = await generate_lesson_content(
                learning_step_id=learning_step_id,
                student_id=student_id,
                teacher_uid=teacher_uid,
                customizations=customizations
            )
            
            if not result.get("success"):
                return result
            
            # Get the complete lesson data
            lesson_data = await get_lesson_content(
                lesson_id=result["lesson_id"],
                student_id=student_id,
                include_progress=True
            )
            
            logger.info(f"Successfully created lesson {result['lesson_id']}")
            
            return {
                "success": True,
                "lesson_id": result["lesson_id"],
                "lesson": lesson_data.get("lesson"),
                "progress": lesson_data.get("progress"),
                "creation_details": {
                    "total_slides": result["total_slides"],
                    "estimated_duration_minutes": result["estimated_duration_minutes"],
                    "learning_objectives": result["learning_objectives"]
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to create lesson from step: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def get_student_lesson(
        self,
        lesson_id: str,
        student_id: str,
        include_chat_history: bool = False
    ) -> Dict[str, Any]:
        """
        Get lesson content for a student.
        
        Args:
            lesson_id: ID of the lesson
            student_id: Student requesting the lesson
            include_chat_history: Whether to include chat history
            
        Returns:
            Dict containing lesson data
        """
        try:
            # Get lesson content with progress
            result = await get_lesson_content(
                lesson_id=lesson_id,
                student_id=student_id,
                include_progress=True
            )
            
            if not result.get("success"):
                return result
            
            response = {
                "success": True,
                "lesson": result["lesson"],
                "progress": result["progress"]
            }
            
            # Add chat history if requested
            if include_chat_history:
                chat_sessions = await self._get_lesson_chat_sessions(lesson_id, student_id)
                response["chat_sessions"] = chat_sessions
            
            return response
            
        except Exception as e:
            logger.error(f"Failed to get student lesson: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def update_slide_progress(
        self,
        lesson_id: str,
        student_id: str,
        slide_id: str,
        progress_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Update student progress on a lesson slide.
        
        Args:
            lesson_id: ID of the lesson
            student_id: Student ID
            slide_id: ID of the slide
            progress_data: Progress information
            
        Returns:
            Dict containing update results
        """
        try:
            result = await update_lesson_progress(
                lesson_id=lesson_id,
                student_id=student_id,
                slide_id=slide_id,
                progress_data=progress_data
            )
            
            # If lesson is completed, trigger any follow-up actions
            if result.get("lesson_completed"):
                await self._handle_lesson_completion(lesson_id, student_id)
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to update slide progress: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def start_lesson_chatbot(
        self,
        lesson_id: str,
        student_id: str,
        initial_message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Start a chatbot session for a lesson.
        
        Args:
            lesson_id: ID of the lesson
            student_id: Student starting the chat
            initial_message: Optional initial message
            
        Returns:
            Dict containing chat session information
        """
        try:
            result = await start_lesson_chat(
                lesson_id=lesson_id,
                student_id=student_id,
                initial_message=initial_message
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to start lesson chatbot: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def send_chatbot_message(
        self,
        session_id: str,
        student_id: str,
        message: str,
        current_slide_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a message to the lesson chatbot.
        
        Args:
            session_id: Chat session ID
            student_id: Student sending the message
            message: Student's message
            current_slide_id: Current slide student is viewing
            
        Returns:
            Dict containing chatbot response
        """
        try:
            result = await send_chat_message(
                session_id=session_id,
                student_id=student_id,
                message=message,
                current_slide_id=current_slide_id
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to send chatbot message: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def get_lesson_analytics(
        self,
        lesson_id: str,
        teacher_uid: str
    ) -> Dict[str, Any]:
        """
        Get analytics for a lesson.
        
        Args:
            lesson_id: ID of the lesson
            teacher_uid: Teacher requesting analytics
            
        Returns:
            Dict containing lesson analytics
        """
        try:
            # Get lesson data
            lesson_doc = db.collection(self.lessons_collection).document(lesson_id).get()
            
            if not lesson_doc.exists:
                return {"success": False, "error": "Lesson not found"}
            
            lesson_data = lesson_doc.to_dict()
            
            # Verify teacher access
            if lesson_data.get("teacher_uid") != teacher_uid:
                return {"success": False, "error": "Access denied"}
            
            # Get progress data for all students
            progress_docs = db.collection(self.progress_collection).where(
                "lesson_id", "==", lesson_id
            ).get()
            
            progress_data = [doc.to_dict() for doc in progress_docs]
            
            # Get chat data
            chat_docs = db.collection(self.chats_collection).where(
                "lesson_id", "==", lesson_id
            ).get()
            
            chat_data = [doc.to_dict() for doc in chat_docs]
            
            # Calculate analytics
            analytics = self._calculate_lesson_analytics(lesson_data, progress_data, chat_data)
            
            return {
                "success": True,
                "lesson_analytics": analytics
            }
            
        except Exception as e:
            logger.error(f"Failed to get lesson analytics: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def get_student_lessons(
        self,
        student_id: str,
        teacher_uid: str,
        include_progress: bool = True
    ) -> Dict[str, Any]:
        """
        Get all lessons for a student.
        
        Args:
            student_id: Student ID
            teacher_uid: Teacher UID for access control
            include_progress: Whether to include progress data
            
        Returns:
            Dict containing student's lessons
        """
        try:
            # Get lessons for the student
            lessons_docs = db.collection(self.lessons_collection).where(
                "student_id", "==", student_id
            ).where(
                "teacher_uid", "==", teacher_uid
            ).order_by("created_at", direction="DESCENDING").get()
            
            lessons = []
            
            for lesson_doc in lessons_docs:
                lesson_data = lesson_doc.to_dict()
                
                lesson_info = {
                    "lesson_id": lesson_data["lesson_id"],
                    "title": lesson_data["title"],
                    "topic": lesson_data["topic"],
                    "subject": lesson_data["subject"],
                    "total_slides": lesson_data["total_slides"],
                    "created_at": lesson_data["created_at"]
                }
                
                # Add progress if requested
                if include_progress:
                    progress = await self._get_lesson_progress_summary(
                        lesson_data["lesson_id"], student_id
                    )
                    lesson_info["progress"] = progress
                
                lessons.append(lesson_info)
            
            return {
                "success": True,
                "student_id": student_id,
                "total_lessons": len(lessons),
                "lessons": lessons
            }
            
        except Exception as e:
            logger.error(f"Failed to get student lessons: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def regenerate_lesson_slide(
        self,
        lesson_id: str,
        slide_id: str,
        student_id: str,
        regeneration_reason: str
    ) -> Dict[str, Any]:
        """
        Regenerate a specific slide in a lesson.
        
        Args:
            lesson_id: ID of the lesson
            slide_id: ID of the slide to regenerate
            student_id: Student ID for personalization
            regeneration_reason: Reason for regeneration
            
        Returns:
            Dict containing regeneration results
        """
        try:
            # Get current lesson
            lesson_doc = db.collection(self.lessons_collection).document(lesson_id).get()
            
            if not lesson_doc.exists:
                return {"success": False, "error": "Lesson not found"}
            
            lesson_data = lesson_doc.to_dict()
            
            # Find the slide to regenerate
            slides = lesson_data.get("slides", [])
            slide_to_regenerate = None
            slide_index = -1
            
            for i, slide in enumerate(slides):
                if slide.get("slide_id") == slide_id:
                    slide_to_regenerate = slide
                    slide_index = i
                    break
            
            if not slide_to_regenerate:
                return {"success": False, "error": "Slide not found"}
            
            # Get student context for personalization
            student_context = await self._get_student_context_for_regeneration(
                student_id, lesson_id, regeneration_reason
            )
            
            # Regenerate slide using AI
            from app.agents.tools.lesson_tools import generate_slide_content
            
            new_slide_result = await generate_slide_content(
                slide_type=slide_to_regenerate.get("slide_type", "concept_explanation"),
                topic=lesson_data.get("topic", ""),
                learning_objective=slide_to_regenerate.get("learning_objective", ""),
                grade_level=lesson_data.get("grade_level", 5),
                student_context=student_context
            )
            
            if not new_slide_result.get("success"):
                return new_slide_result
            
            # Update the slide in the lesson
            new_slide_data = new_slide_result["slide_content"]
            new_slide_data["slide_id"] = slide_id  # Keep the same ID
            new_slide_data["slide_number"] = slide_to_regenerate["slide_number"]
            new_slide_data["regenerated_at"] = datetime.utcnow()
            new_slide_data["regeneration_reason"] = regeneration_reason
            
            slides[slide_index] = new_slide_data
            
            # Update lesson in database
            db.collection(self.lessons_collection).document(lesson_id).update({
                "slides": slides,
                "last_updated": datetime.utcnow()
            })
            
            logger.info(f"Regenerated slide {slide_id} in lesson {lesson_id}")
            
            return {
                "success": True,
                "lesson_id": lesson_id,
                "slide_id": slide_id,
                "new_slide": new_slide_data,
                "regenerated_at": new_slide_data["regenerated_at"]
            }
            
        except Exception as e:
            logger.error(f"Failed to regenerate lesson slide: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # Helper methods
    
    async def _get_lesson_chat_sessions(self, lesson_id: str, student_id: str) -> List[Dict[str, Any]]:
        """Get chat sessions for a lesson."""
        try:
            chat_docs = db.collection(self.chats_collection).where(
                "lesson_id", "==", lesson_id
            ).where(
                "student_id", "==", student_id
            ).order_by("started_at", direction="DESCENDING").get()
            
            return [doc.to_dict() for doc in chat_docs]
        except Exception as e:
            logger.error(f"Failed to get chat sessions: {str(e)}")
            return []
    
    async def _handle_lesson_completion(self, lesson_id: str, student_id: str) -> None:
        """Handle lesson completion events."""
        try:
            # Log completion
            completion_log = {
                "lesson_id": lesson_id,
                "student_id": student_id,
                "completed_at": datetime.utcnow(),
                "completion_type": "automatic"
            }
            
            db.collection("lesson_completions").add(completion_log)
            
            # Could trigger follow-up actions like:
            # - Generating next lesson
            # - Updating learning path progress
            # - Sending notifications
            
            logger.info(f"Lesson {lesson_id} completed by student {student_id}")
            
        except Exception as e:
            logger.error(f"Failed to handle lesson completion: {str(e)}")
    
    def _calculate_lesson_analytics(
        self,
        lesson_data: Dict[str, Any],
        progress_data: List[Dict[str, Any]],
        chat_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Calculate analytics for a lesson."""
        try:
            total_students = len(progress_data)
            
            if total_students == 0:
                return {
                    "total_students": 0,
                    "completion_rate": 0,
                    "average_time_spent": 0,
                    "engagement_metrics": {},
                    "chat_analytics": {}
                }
            
            # Calculate completion metrics
            completed_lessons = sum(1 for p in progress_data if p.get("completion_percentage", 0) >= 100)
            completion_rate = (completed_lessons / total_students) * 100
            
            # Calculate time metrics
            total_time = sum(p.get("time_spent_minutes", 0) for p in progress_data)
            average_time = total_time / total_students if total_students > 0 else 0
            
            # Calculate engagement metrics
            total_interactions = sum(p.get("interactions_count", 0) for p in progress_data)
            total_correct = sum(p.get("correct_responses", 0) for p in progress_data)
            total_responses = sum(p.get("total_responses", 0) for p in progress_data)
            
            success_rate = (total_correct / total_responses) * 100 if total_responses > 0 else 0
            
            # Calculate chat analytics
            total_chat_sessions = len(chat_data)
            total_messages = sum(chat.get("total_messages", 0) for chat in chat_data)
            total_questions = sum(chat.get("student_questions", 0) for chat in chat_data)
            
            return {
                "total_students": total_students,
                "completion_rate": round(completion_rate, 2),
                "average_time_spent_minutes": round(average_time, 2),
                "engagement_metrics": {
                    "total_interactions": total_interactions,
                    "success_rate": round(success_rate, 2),
                    "average_interactions_per_student": round(total_interactions / total_students, 2)
                },
                "chat_analytics": {
                    "total_sessions": total_chat_sessions,
                    "total_messages": total_messages,
                    "total_questions": total_questions,
                    "chat_usage_rate": round((total_chat_sessions / total_students) * 100, 2)
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to calculate lesson analytics: {str(e)}")
            return {}
    
    async def _get_lesson_progress_summary(self, lesson_id: str, student_id: str) -> Dict[str, Any]:
        """Get progress summary for a lesson."""
        try:
            progress_docs = db.collection(self.progress_collection).where(
                "lesson_id", "==", lesson_id
            ).where(
                "student_id", "==", student_id
            ).limit(1).get()
            
            if not progress_docs:
                return {
                    "completion_percentage": 0,
                    "slides_completed": 0,
                    "time_spent_minutes": 0,
                    "last_activity": None
                }
            
            progress_data = progress_docs[0].to_dict()
            
            return {
                "completion_percentage": progress_data.get("completion_percentage", 0),
                "slides_completed": progress_data.get("slides_completed", 0),
                "time_spent_minutes": progress_data.get("time_spent_minutes", 0),
                "last_activity": progress_data.get("last_updated"),
                "started_at": progress_data.get("started_at")
            }
            
        except Exception as e:
            logger.error(f"Failed to get lesson progress summary: {str(e)}")
            return {}
    
    async def _get_student_context_for_regeneration(
        self,
        student_id: str,
        lesson_id: str,
        regeneration_reason: str
    ) -> Dict[str, Any]:
        """Get student context for slide regeneration."""
        try:
            # Get current progress
            progress = await self._get_lesson_progress_summary(lesson_id, student_id)
            
            # Get recent performance
            # This could be enhanced with more sophisticated analysis
            
            context = {
                "student_id": student_id,
                "current_progress": progress,
                "regeneration_reason": regeneration_reason,
                "needs_simplification": "struggling" in regeneration_reason.lower(),
                "needs_enhancement": "too_easy" in regeneration_reason.lower()
            }
            
            return context
            
        except Exception as e:
            logger.error(f"Failed to get student context for regeneration: {str(e)}")
            return {}

# Global instance
lesson_service = LessonService()


================================================================================
File: app/services/rag_service.py
Size: 9.34 kB
================================================================================

# FILE: app/services/rag_service.py

import logging
from typing import List, Dict, Any, Optional
import asyncio

from app.services.vertex_rag_service import VertexAIRAGService
from app.models.rag_models import RAGQuery, RAGResult, DocumentChunk
from app.core.firebase import db

logger = logging.getLogger(__name__)

class RAGService:
    """Service for Retrieval-Augmented Generation operations."""
    
    def __init__(self):
        self.vector_service = VertexAIRAGService()
        self.processed_docs_collection = "processed_documents"
    
    async def retrieve_context_for_assessment(
        self,
        subject: str,
        grade_level: int,
        topic: str,
        teacher_uid: str,
        max_chunks: int = 5
    ) -> List[RAGResult]:
        """Retrieve relevant context for assessment generation."""
        
        try:
            # Create search queries with different strategies
            queries = self._create_search_queries(subject, grade_level, topic)
            
            all_results = []
            
            for query in queries:
                # Add teacher filter to query if needed
                results = await self.vector_service.search_similar_chunks(query)
                
                # Filter by teacher ownership
                filtered_results = await self._filter_by_teacher_ownership(results, teacher_uid)
                all_results.extend(filtered_results)
            
            # Remove duplicates and rank by relevance
            unique_results = self._deduplicate_results(all_results)
            ranked_results = self._rank_results(unique_results, subject, topic)
            
            # Return top results
            return ranked_results[:max_chunks]
            
        except Exception as e:
            logger.error(f"RAG retrieval failed for {subject} grade {grade_level}: {str(e)}")
            return []
    
    def _create_search_queries(
        self,
        subject: str,
        grade_level: int,
        topic: str
    ) -> List[RAGQuery]:
        """Create multiple search queries with different strategies."""
        
        queries = []
        
        # Primary query: exact topic match
        primary_query = RAGQuery(
            query_text=f"{topic} {subject} grade {grade_level}",
            subject=subject,
            grade_level=grade_level,
            topic=topic,
            max_results=3,
            similarity_threshold=0.7
        )
        queries.append(primary_query)
        
        # Secondary query: broader subject context
        secondary_query = RAGQuery(
            query_text=f"{subject} concepts grade {grade_level}",
            subject=subject,
            grade_level=grade_level,
            max_results=2,
            similarity_threshold=0.6
        )
        queries.append(secondary_query)
        
        # Tertiary query: topic without grade restriction (for flexibility)
        tertiary_query = RAGQuery(
            query_text=f"{topic} {subject}",
            subject=subject,
            grade_level=0,  # No grade filter
            max_results=2,
            similarity_threshold=0.65
        )
        queries.append(tertiary_query)
        
        return queries
    
    async def _filter_by_teacher_ownership(
        self,
        results: List[RAGResult],
        teacher_uid: str
    ) -> List[RAGResult]:
        """Filter results to only include documents owned by the teacher."""
        
        filtered_results = []
        
        for result in results:
            # Check if chunk belongs to teacher's documents
            chunk_teacher = result.chunk.metadata.get("teacher_uid")
            if chunk_teacher == teacher_uid:
                filtered_results.append(result)
        
        return filtered_results
    
    def _deduplicate_results(self, results: List[RAGResult]) -> List[RAGResult]:
        """Remove duplicate chunks and keep the one with highest similarity."""
        
        seen_chunks = {}
        
        for result in results:
            chunk_id = result.chunk.chunk_id
            
            if chunk_id not in seen_chunks or result.similarity_score > seen_chunks[chunk_id].similarity_score:
                seen_chunks[chunk_id] = result
        
        return list(seen_chunks.values())
    
    def _rank_results(
        self,
        results: List[RAGResult],
        subject: str,
        topic: str
    ) -> List[RAGResult]:
        """Rank results by relevance with custom scoring."""
        
        def calculate_relevance_score(result: RAGResult) -> float:
            score = result.similarity_score
            
            # Boost score for exact topic matches
            if topic.lower() in result.chunk.content.lower():
                score += 0.1
            
            # Boost score for exact subject matches in metadata
            if result.chunk.metadata.get("subject", "").lower() == subject.lower():
                score += 0.05
            
            # Penalty for very short chunks (less informative)
            if len(result.chunk.content) < 100:
                score -= 0.05
            
            return min(score, 1.0)  # Cap at 1.0
        
        # Calculate relevance scores and sort
        for result in results:
            result.similarity_score = calculate_relevance_score(result)
        
        return sorted(results, key=lambda x: x.similarity_score, reverse=True)
    
    async def get_context_summary(
        self,
        results: List[RAGResult]
    ) -> Dict[str, Any]:
        """Get a summary of the retrieved context."""
        
        if not results:
            return {
                "total_chunks": 0,
                "avg_similarity": 0.0,
                "sources": [],
                "content_length": 0
            }
        
        # Analyze sources
        sources = set()
        total_length = 0
        
        for result in results:
            filename = result.document_metadata.get("filename", "Unknown")
            sources.add(filename)
            total_length += len(result.chunk.content)
        
        avg_similarity = sum(r.similarity_score for r in results) / len(results)
        
        return {
            "total_chunks": len(results),
            "avg_similarity": round(avg_similarity, 3),
            "sources": list(sources),
            "content_length": total_length,
            "chunks_preview": [
                {
                    "chunk_id": r.chunk.chunk_id,
                    "content_preview": r.chunk.content[:100] + "..." if len(r.chunk.content) > 100 else r.chunk.content,
                    "similarity": round(r.similarity_score, 3),
                    "source": r.document_metadata.get("filename", "Unknown")
                }
                for r in results[:3]  # Show first 3 chunks
            ]
        }
    
    async def search_documents_by_content(
        self,
        search_text: str,
        teacher_uid: str,
        subject_filter: Optional[str] = None,
        grade_filter: Optional[int] = None,
        max_results: int = 10
    ) -> List[RAGResult]:
        """Search documents by content for teacher's content management."""
        
        try:
            query = RAGQuery(
                query_text=search_text,
                subject=subject_filter or "",
                grade_level=grade_filter or 0,
                max_results=max_results,
                similarity_threshold=0.5
            )
            
            results = await self.vector_service.search_similar_chunks(query)
            filtered_results = await self._filter_by_teacher_ownership(results, teacher_uid)
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"Content search failed: {str(e)}")
            return []
    
    async def get_teacher_content_stats(self, teacher_uid: str) -> Dict[str, Any]:
        """Get statistics about teacher's uploaded content."""
        
        try:
            # Get processed documents
            docs_ref = db.collection(self.processed_docs_collection)
            teacher_docs = docs_ref.where("teacher_uid", "==", teacher_uid).stream()
            
            doc_count = 0
            subjects = set()
            grades = set()
            total_chunks = 0
            
            for doc in teacher_docs:
                doc_data = doc.to_dict()
                doc_count += 1
                total_chunks += doc_data.get("total_chunks", 0)
                
                if doc_data.get("subject"):
                    subjects.add(doc_data["subject"])
                if doc_data.get("grade_level"):
                    grades.add(doc_data["grade_level"])
            
            return {
                "total_documents": doc_count,
                "total_chunks": total_chunks,
                "subjects": list(subjects),
                "grade_levels": sorted(list(grades)),
                "teacher_uid": teacher_uid
            }
            
        except Exception as e:
            logger.error(f"Failed to get teacher content stats: {str(e)}")
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "subjects": [],
                "grade_levels": [],
                "teacher_uid": teacher_uid
            }


================================================================================
File: app/services/rag_stub_service.py
Size: 2.07 kB
================================================================================

# FILE: app/services/rag_stub_service.py

import logging
from typing import List, Dict, Any, Optional

from app.models.rag_models import RAGQuery, RAGResult, DocumentChunk
from app.models.student import AssessmentConfig, Assessment, AssessmentQuestion

logger = logging.getLogger(__name__)

class RAGStubService:
    """Stub RAG service for when full RAG dependencies aren't available."""
    
    def __init__(self):
        self.is_rag_enabled = False
        logger.warning("Using RAG stub service - full RAG functionality not available")
    
    async def retrieve_context_for_assessment(
        self,
        subject: str,
        grade_level: int,
        topic: str,
        teacher_uid: str,
        max_chunks: int = 5
    ) -> List[RAGResult]:
        """Stub method - returns empty results."""
        logger.info(f"RAG stub: would retrieve context for {subject} grade {grade_level} topic {topic}")
        return []
    
    async def get_context_summary(self, results: List[RAGResult]) -> Dict[str, Any]:
        """Stub method for context summary."""
        return {
            "total_chunks": 0,
            "avg_similarity": 0.0,
            "sources": [],
            "content_length": 0,
            "message": "RAG not fully configured"
        }
    
    async def search_documents_by_content(
        self,
        search_text: str,
        teacher_uid: str,
        subject_filter: Optional[str] = None,
        grade_filter: Optional[int] = None,
        max_results: int = 10
    ) -> List[RAGResult]:
        """Stub method for content search."""
        logger.info(f"RAG stub: would search for '{search_text}' for teacher {teacher_uid}")
        return []
    
    async def get_teacher_content_stats(self, teacher_uid: str) -> Dict[str, Any]:
        """Stub method for teacher content stats."""
        return {
            "total_documents": 0,
            "total_chunks": 0,
            "subjects": [],
            "grade_levels": [],
            "teacher_uid": teacher_uid,
            "message": "RAG not fully configured"
        }


================================================================================
File: app/services/simple_assessment_service.py
Size: 10.73 kB
================================================================================

# FILE: app/services/simple_assessment_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.firebase import db
from app.models.student import (
    AssessmentConfig, Assessment, AssessmentQuestion, 
    StudentAssessmentResult, LearningPath
)

logger = logging.getLogger(__name__)

class SimpleAssessmentService:
    """Simplified assessment service without AI generation for Phase 2 start."""
    
    def __init__(self):
        self.assessment_configs_collection = "assessment_configs"
        self.assessments_collection = "assessments"
        self.assessment_results_collection = "assessment_results"
        self.learning_paths_collection = "learning_paths"
    
    async def create_assessment_config(
        self, 
        name: str,
        subject: str,
        target_grade: int,
        difficulty_level: str,
        topic: str,
        teacher_uid: str,
        question_count: int = 10,
        time_limit_minutes: int = 30
    ) -> AssessmentConfig:
        """Create a new assessment configuration."""
        
        config_id = str(uuid.uuid4())
        
        config = AssessmentConfig(
            config_id=config_id,
            teacher_uid=teacher_uid,
            name=name,
            subject=subject,
            target_grade=target_grade,
            difficulty_level=difficulty_level,
            topic=topic,
            question_count=question_count,
            time_limit_minutes=time_limit_minutes
        )
        
        # Save to Firestore
        doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
        doc_ref.set(config.dict())
        
        logger.info(f"Created assessment config: {config_id}")
        return config
    
    async def get_assessment_config_by_id(
        self,
        config_id: str,
        teacher_uid: str
    ) -> Optional[AssessmentConfig]:
        """Get a single assessment config by ID and verify teacher ownership."""
        try:
            doc_ref = db.collection(self.assessment_configs_collection).document(config_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                return None
                
            config_data = doc.to_dict()
            if config_data.get("teacher_uid") != teacher_uid:
                return None  # Teacher doesn't own this config
                
            return AssessmentConfig(**config_data)
            
        except Exception as e:
            logger.error(f"Error getting assessment config {config_id}: {str(e)}")
            raise e
    
    async def get_teacher_assessment_configs(
        self, 
        teacher_uid: str,
        subject_filter: Optional[str] = None
    ) -> List[AssessmentConfig]:
        """Get all assessment configurations for a teacher."""
        
        try:
            # Simplified query to avoid composite index requirement
            query = (db.collection(self.assessment_configs_collection)
                    .where("teacher_uid", "==", teacher_uid))
            
            docs = query.get()
            
            configs = []
            for doc in docs:
                config_data = doc.to_dict()
                config = AssessmentConfig(**config_data)
                
                # Filter in memory to avoid index requirements
                if not config.is_active:
                    continue
                    
                if subject_filter and config.subject != subject_filter:
                    continue
                
                configs.append(config)
            
            # Sort by created_at in memory
            configs.sort(key=lambda x: x.created_at, reverse=True)
            
            return configs
            
        except Exception as e:
            logger.error(f"Failed to get assessment configs: {e}")
            return []
    
    async def create_sample_assessment(
        self, 
        config: AssessmentConfig
    ) -> Assessment:
        """Create a sample assessment with placeholder questions."""
        
        assessment_id = str(uuid.uuid4())
        
        # Create sample questions based on the configuration
        sample_questions = self._generate_sample_questions(
            config.subject, 
            config.target_grade, 
            config.topic,
            config.difficulty_level,
            config.question_count
        )
        
        assessment = Assessment(
            assessment_id=assessment_id,
            config_id=config.config_id,
            teacher_uid=config.teacher_uid,
            title=f"{config.topic} - {config.subject} Grade {config.target_grade}",
            subject=config.subject,
            grade=config.target_grade,
            difficulty=config.difficulty_level,
            topic=config.topic,
            questions=sample_questions,
            time_limit_minutes=config.time_limit_minutes
        )
        
        # Save to Firestore
        doc_ref = db.collection(self.assessments_collection).document(assessment_id)
        doc_ref.set(assessment.dict())
        
        logger.info(f"Created sample assessment: {assessment_id}")
        return assessment
    
    def _generate_sample_questions(
        self, 
        subject: str, 
        grade: int, 
        topic: str,
        difficulty: str,
        count: int
    ) -> List[AssessmentQuestion]:
        """Generate sample questions for demonstration."""
        
        questions = []
        
        # Sample question templates by subject
        print("Using Simple Question file still")
        templates = {
            "Mathematics": [
                {
                    "question": f"What is 2 + 3?",
                    "options": ["4", "5", "6", "7"],
                    "correct": 1,
                    "explanation": "2 + 3 = 5"
                },
                {
                    "question": f"What is 10 - 4?",
                    "options": ["5", "6", "7", "8"],
                    "correct": 1,
                    "explanation": "10 - 4 = 6"
                }
            ],
            "Science": [
                {
                    "question": f"What planet is closest to the Sun?",
                    "options": ["Venus", "Mercury", "Earth", "Mars"],
                    "correct": 1,
                    "explanation": "Mercury is the closest planet to the Sun"
                },
                {
                    "question": f"What gas do plants use for photosynthesis?",
                    "options": ["Oxygen", "Nitrogen", "Carbon Dioxide", "Hydrogen"],
                    "correct": 2,
                    "explanation": "Plants use carbon dioxide for photosynthesis"
                }
            ],
            "English": [
                {
                    "question": f"What is a noun?",
                    "options": ["Action word", "Describing word", "Person, place, or thing", "Connecting word"],
                    "correct": 2,
                    "explanation": "A noun is a person, place, or thing"
                },
                {
                    "question": f"What is the past tense of 'run'?",
                    "options": ["Runs", "Running", "Ran", "Runned"],
                    "correct": 2,
                    "explanation": "The past tense of 'run' is 'ran'"
                }
            ]
        }
        
        # Get templates for the subject, or use generic ones
        subject_templates = templates.get(subject, templates["Mathematics"])
        
        # Generate the requested number of questions
        for i in range(min(count, len(subject_templates) * 3)):  # Repeat templates if needed
            template = subject_templates[i % len(subject_templates)]
            
            question = AssessmentQuestion(
                question_id=str(uuid.uuid4()),
                question_text=template["question"],
                options=template["options"],
                correct_answer=template["correct"],
                explanation=template["explanation"],
                difficulty=difficulty,
                topic=topic
            )
            questions.append(question)
        
        return questions[:count]  # Return exactly the requested count
    
    async def get_assessment_by_id(self, assessment_id: str) -> Optional[Assessment]:
        """Get an assessment by ID."""
        
        try:
            doc_ref = db.collection(self.assessments_collection).document(assessment_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                return None
            
            return Assessment(**doc.to_dict())
            
        except Exception as e:
            logger.error(f"Failed to get assessment {assessment_id}: {e}")
            return None
    
    async def get_available_topics(
        self, 
        subject: str, 
        grade: int, 
        teacher_uid: str
    ) -> List[str]:
        """Get available topics based on uploaded documents (placeholder for now)."""
        
        # For now, return some sample topics based on subject and grade
        topic_templates = {
            "Mathematics": {
                "elementary": ["Addition", "Subtraction", "Multiplication", "Division", "Fractions"],
                "middle": ["Algebra", "Geometry", "Statistics", "Probability", "Equations"],
                "high": ["Calculus", "Trigonometry", "Advanced Algebra", "Statistics", "Geometry"]
            },
            "Science": {
                "elementary": ["Animals", "Plants", "Weather", "Solar System", "Matter"],
                "middle": ["Physics", "Chemistry", "Biology", "Earth Science", "Energy"],
                "high": ["Advanced Physics", "Organic Chemistry", "Biology", "Environmental Science", "Astronomy"]
            },
            "English": {
                "elementary": ["Reading", "Writing", "Grammar", "Vocabulary", "Spelling"],
                "middle": ["Literature", "Writing Skills", "Grammar", "Poetry", "Essays"],
                "high": ["Literature Analysis", "Creative Writing", "Research", "Critical Thinking", "Communication"]
            }
        }
        
        # Determine grade category
        if grade <= 5:
            category = "elementary"
        elif grade <= 8:
            category = "middle"
        else:
            category = "high"
        
        # Get topics for the subject and grade category
        subject_topics = topic_templates.get(subject, topic_templates["Mathematics"])
        topics = subject_topics.get(category, ["General Topics"])
        
        return topics

# Create singleton instance
simple_assessment_service = SimpleAssessmentService()


================================================================================
File: app/services/student_service.py
Size: 11.03 kB
================================================================================

# FILE: app/services/student_service.py

import logging
import uuid
import csv
import io
from typing import List, Dict, Any, Optional
from datetime import datetime

from fastapi import UploadFile, HTTPException
from app.core.firebase import db
from app.models.student import (
    StudentProfile, StudentCSVRow, StudentBatchUploadResponse,
    AssessmentConfig, Assessment, StudentAssessmentResult, LearningPath
)

logger = logging.getLogger(__name__)

class StudentService:
    """Service for managing student profiles and related operations."""
    
    def __init__(self):
        self.students_collection = "students"
        self.assessments_collection = "assessments"
        self.assessment_configs_collection = "assessment_configs"
        self.assessment_results_collection = "assessment_results"
        self.learning_paths_collection = "learning_paths"
    
    async def upload_students_csv(
        self, 
        file: UploadFile, 
        teacher_uid: str,
        teacher_subjects: List[str]
    ) -> StudentBatchUploadResponse:
        """
        Upload students from CSV file and create profiles.
        
        Args:
            file: CSV file with student data
            teacher_uid: UID of the teacher uploading students
            teacher_subjects: List of subjects the teacher handles
            
        Returns:
            StudentBatchUploadResponse with upload results
        """
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="File must be a CSV file")
        
        try:
            # Read CSV content
            content = await file.read()
            csv_content = content.decode('utf-8')
            
            # Parse CSV
            csv_reader = csv.DictReader(io.StringIO(csv_content))
            
            # Validate CSV headers
            required_headers = {'first_name', 'last_name', 'grade', 'password'}
            if not required_headers.issubset(set(csv_reader.fieldnames)):
                raise HTTPException(
                    status_code=400, 
                    detail=f"CSV must contain headers: {required_headers}. Found: {csv_reader.fieldnames}"
                )
            
            students_created = 0
            students_updated = 0
            students_failed = 0
            failed_students = []
            created_student_ids = []
            total_students = 0
            
            # Process each row
            for row_num, row in enumerate(csv_reader, start=2):  # Start at 2 for header row
                total_students += 1
                
                try:
                    # Validate row data
                    student_data = StudentCSVRow(**row)
                    
                    # Check if student already exists (by name and grade)
                    existing_student = await self._find_existing_student(
                        teacher_uid, 
                        student_data.first_name, 
                        student_data.last_name, 
                        student_data.grade
                    )
                    
                    if existing_student:
                        # Update existing student
                        await self._update_student_profile(
                            existing_student['student_id'],
                            student_data,
                            teacher_subjects
                        )
                        students_updated += 1
                        logger.info(f"Updated existing student: {student_data.first_name} {student_data.last_name}")
                    else:
                        # Create new student profile
                        student_id = await self._create_student_profile(
                            student_data, 
                            teacher_uid, 
                            teacher_subjects
                        )
                        students_created += 1
                        created_student_ids.append(student_id)
                        logger.info(f"Created new student: {student_data.first_name} {student_data.last_name}")
                
                except Exception as e:
                    students_failed += 1
                    failed_students.append({
                        "row": row_num,
                        "data": row,
                        "error": str(e)
                    })
                    logger.error(f"Failed to process row {row_num}: {e}")
            
            # Generate summary message
            summary = f"Processed {total_students} students: {students_created} created, {students_updated} updated, {students_failed} failed"
            
            return StudentBatchUploadResponse(
                total_students=total_students,
                students_created=students_created,
                students_updated=students_updated,
                students_failed=students_failed,
                failed_students=failed_students,
                created_student_ids=created_student_ids,
                upload_summary=summary
            )
            
        except Exception as e:
            logger.error(f"Failed to process student CSV: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to process CSV file: {str(e)}"
            )
    
    async def _find_existing_student(
        self, 
        teacher_uid: str, 
        first_name: str, 
        last_name: str, 
        grade: int
    ) -> Optional[Dict[str, Any]]:
        """Find existing student by name and grade."""
        try:
            query = (db.collection(self.students_collection)
                    .where("teacher_uid", "==", teacher_uid)
                    .where("first_name", "==", first_name)
                    .where("last_name", "==", last_name)
                    .where("grade", "==", grade))
            
            docs = query.get()
            if docs:
                return docs[0].to_dict()
            return None
            
        except Exception as e:
            logger.error(f"Error finding existing student: {e}")
            return None
    
    async def _create_student_profile(
        self, 
        student_data: StudentCSVRow, 
        teacher_uid: str,
        teacher_subjects: List[str]
    ) -> str:
        """Create a new student profile."""
        student_id = str(uuid.uuid4())
        
        profile = StudentProfile(
            student_id=student_id,
            teacher_uid=teacher_uid,
            first_name=student_data.first_name,
            last_name=student_data.last_name,
            grade=student_data.grade,
            default_password=student_data.password,
            subjects=teacher_subjects.copy()  # Assign all teacher's subjects
        )
        
        # Save to Firestore
        doc_ref = db.collection(self.students_collection).document(student_id)
        doc_ref.set(profile.dict())
        
        return student_id
    
    async def _update_student_profile(
        self, 
        student_id: str, 
        student_data: StudentCSVRow,
        teacher_subjects: List[str]
    ) -> None:
        """Update existing student profile."""
        update_data = {
            "first_name": student_data.first_name,
            "last_name": student_data.last_name,
            "grade": student_data.grade,
            "default_password": student_data.password,
            "subjects": teacher_subjects,
            "updated_at": datetime.utcnow()
        }
        
        doc_ref = db.collection(self.students_collection).document(student_id)
        doc_ref.update(update_data)
    
    async def get_teacher_students(
        self, 
        teacher_uid: str,
        grade_filter: Optional[int] = None,
        subject_filter: Optional[str] = None
    ) -> List[StudentProfile]:
        """Get all students for a teacher with optional filters."""
        try:
            query = db.collection(self.students_collection).where("teacher_uid", "==", teacher_uid)
            
            if grade_filter:
                query = query.where("grade", "==", grade_filter)
            
            docs = query.get()
            students = []
            
            for doc in docs:
                student_data = doc.to_dict()
                
                # Filter by subject if specified
                if subject_filter and subject_filter not in student_data.get('subjects', []):
                    continue
                
                students.append(StudentProfile(**student_data))
            
            # Sort by grade, then by last name
            students.sort(key=lambda s: (s.grade, s.last_name, s.first_name))
            
            return students
            
        except Exception as e:
            logger.error(f"Failed to get teacher students: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve students: {str(e)}"
            )
    
    async def get_student_by_id(self, student_id: str) -> StudentProfile:
        """Get a student profile by ID."""
        try:
            doc_ref = db.collection(self.students_collection).document(student_id)
            doc = doc_ref.get()
            
            if not doc.exists:
                raise HTTPException(status_code=404, detail="Student not found")
            
            return StudentProfile(**doc.to_dict())
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to get student: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve student: {str(e)}"
            )
    
    async def update_student_subjects(
        self, 
        student_id: str, 
        subjects: List[str]
    ) -> None:
        """Update a student's enrolled subjects."""
        try:
            doc_ref = db.collection(self.students_collection).document(student_id)
            doc_ref.update({
                "subjects": subjects,
                "updated_at": datetime.utcnow()
            })
            
        except Exception as e:
            logger.error(f"Failed to update student subjects: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to update student: {str(e)}"
            )
    
    async def deactivate_student(self, student_id: str) -> None:
        """Deactivate a student profile."""
        try:
            doc_ref = db.collection(self.students_collection).document(student_id)
            doc_ref.update({
                "is_active": False,
                "updated_at": datetime.utcnow()
            })
            
        except Exception as e:
            logger.error(f"Failed to deactivate student: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to deactivate student: {str(e)}"
            )

# Create singleton instance
student_service = StudentService()


================================================================================
File: app/services/vector_service.py
Size: 12.18 kB
================================================================================

# FILE: app/services/vector_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime

try:
    import chromadb
    from chromadb.config import Settings
    from chromadb.utils import embedding_functions
    CHROMADB_AVAILABLE = True
except ImportError:
    chromadb = None
    CHROMADB_AVAILABLE = False

from google.cloud import aiplatform
from app.core.config import settings
from app.models.rag_models import DocumentChunk, RAGQuery, RAGResult, VectorSearchMetrics

logger = logging.getLogger(__name__)

class VectorService:
    """Service for vector database operations using ChromaDB."""
    
    def __init__(self):
        self.settings = settings
        self.collection_name = "document_embeddings"
        self.client = None
        self.collection = None
        self.embedding_function = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize ChromaDB client and collection."""
        try:
            if not CHROMADB_AVAILABLE:
                raise ImportError("ChromaDB not installed. Run: pip install chromadb")
            
            # Initialize ChromaDB client
            self.client = chromadb.PersistentClient(
                path="./chroma_db",
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # Initialize Vertex AI embedding function
            self.embedding_function = self._get_vertex_embedding_function()
            
            # Get or create collection
            try:
                self.collection = self.client.get_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function
                )
                logger.info(f"Connected to existing collection: {self.collection_name}")
            except:
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"description": "Document chunks for RAG"}
                )
                logger.info(f"Created new collection: {self.collection_name}")
                
        except Exception as e:
            logger.error(f"Failed to initialize vector service: {str(e)}")
            raise e
    
    def _get_vertex_embedding_function(self):
        """Get Vertex AI embedding function."""
        try:
            # Use Google Vertex AI for embeddings
            return embedding_functions.GoogleVertexEmbeddingFunction(
                api_key=None,  # Will use service account
                model_name="text-embedding-005",
                project_id=self.settings.google_cloud_project or self.settings.firebase_project_id
            )
        except Exception as e:
            logger.warning(f"Failed to initialize Vertex AI embeddings: {str(e)}")
            # Fallback to sentence transformers
            return embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name="all-MiniLM-L6-v2"
            )
    
    async def add_chunks(self, chunks: List[DocumentChunk]) -> bool:
        """Add document chunks to vector database."""
        try:
            if not chunks:
                return True
            
            # Prepare data for ChromaDB
            documents = []
            metadatas = []
            ids = []
            
            for chunk in chunks:
                documents.append(chunk.content)
                ids.append(chunk.chunk_id)
                
                # Prepare metadata (ChromaDB requires flat dict)
                metadata = {
                    "document_id": chunk.document_id,
                    "chunk_index": chunk.chunk_index,
                    "teacher_uid": chunk.metadata.get("teacher_uid", ""),
                    "subject": chunk.metadata.get("subject", ""),
                    "grade_level": chunk.metadata.get("grade_level", 0),
                    "filename": chunk.metadata.get("filename", ""),
                    "created_at": chunk.created_at.isoformat()
                }
                metadatas.append(metadata)
            
            # Add to collection
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"Added {len(chunks)} chunks to vector database")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add chunks to vector database: {str(e)}")
            return False
    
    async def search_similar_chunks(
        self,
        query: RAGQuery
    ) -> List[RAGResult]:
        """Search for similar document chunks."""
        try:
            start_time = datetime.utcnow()
            
            # Build where filter
            where_filter = {}
            if query.subject:
                where_filter["subject"] = query.subject
            if query.grade_level:
                where_filter["grade_level"] = query.grade_level
            
            # Perform vector search
            results = self.collection.query(
                query_texts=[query.query_text],
                n_results=query.max_results,
                where=where_filter if where_filter else None,
                include=["documents", "metadatas", "distances"]
            )
            
            # Process results
            rag_results = []
            if results and results["documents"] and results["documents"][0]:
                documents = results["documents"][0]
                metadatas = results["metadatas"][0]
                distances = results["distances"][0]
                
                for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
                    # Convert distance to similarity score (ChromaDB uses cosine distance)
                    similarity_score = 1.0 - distance
                    
                    # Skip results below threshold
                    if similarity_score < query.similarity_threshold:
                        continue
                    
                    # Create DocumentChunk from search result
                    chunk = DocumentChunk(
                        chunk_id=results["ids"][0][i],
                        document_id=metadata["document_id"],
                        content=doc,
                        chunk_index=metadata["chunk_index"],
                        metadata={
                            "teacher_uid": metadata.get("teacher_uid", ""),
                            "subject": metadata.get("subject", ""),
                            "grade_level": metadata.get("grade_level", 0),
                            "filename": metadata.get("filename", "")
                        }
                    )
                    
                    rag_result = RAGResult(
                        chunk=chunk,
                        similarity_score=similarity_score,
                        document_metadata={
                            "filename": metadata.get("filename", ""),
                            "document_id": metadata["document_id"]
                        }
                    )
                    rag_results.append(rag_result)
            
            # Calculate metrics
            end_time = datetime.utcnow()
            query_time = (end_time - start_time).total_seconds() * 1000
            
            metrics = VectorSearchMetrics(
                query_time_ms=query_time,
                total_documents=self.collection.count(),
                results_returned=len(rag_results),
                average_similarity=sum(r.similarity_score for r in rag_results) / len(rag_results) if rag_results else 0.0
            )
            
            logger.info(f"Vector search completed: {len(rag_results)} results, {query_time:.2f}ms")
            return rag_results
            
        except Exception as e:
            logger.error(f"Vector search failed: {str(e)}")
            return []
    
    async def get_chunk_by_id(self, chunk_id: str) -> Optional[DocumentChunk]:
        """Get a specific chunk by ID."""
        try:
            results = self.collection.get(
                ids=[chunk_id],
                include=["documents", "metadatas"]
            )
            
            if results and results["documents"]:
                doc = results["documents"][0]
                metadata = results["metadatas"][0]
                
                return DocumentChunk(
                    chunk_id=chunk_id,
                    document_id=metadata["document_id"],
                    content=doc,
                    chunk_index=metadata["chunk_index"],
                    metadata={
                        "teacher_uid": metadata.get("teacher_uid", ""),
                        "subject": metadata.get("subject", ""),
                        "grade_level": metadata.get("grade_level", 0),
                        "filename": metadata.get("filename", "")
                    }
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get chunk {chunk_id}: {str(e)}")
            return None
    
    async def delete_document_chunks(self, document_id: str) -> bool:
        """Delete all chunks for a document."""
        try:
            # Get all chunks for the document
            results = self.collection.get(
                where={"document_id": document_id},
                include=["documents"]
            )
            
            if results and results["ids"]:
                chunk_ids = results["ids"]
                self.collection.delete(ids=chunk_ids)
                logger.info(f"Deleted {len(chunk_ids)} chunks for document {document_id}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete chunks for document {document_id}: {str(e)}")
            return False
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about the vector collection."""
        try:
            count = self.collection.count()
            
            # Get sample of metadata to analyze subjects/grades
            sample_results = self.collection.get(
                limit=100,
                include=["metadatas"]
            )
            
            subjects = set()
            grades = set()
            
            if sample_results and sample_results["metadatas"]:
                for metadata in sample_results["metadatas"]:
                    if metadata.get("subject"):
                        subjects.add(metadata["subject"])
                    if metadata.get("grade_level"):
                        grades.add(metadata["grade_level"])
            
            return {
                "total_chunks": count,
                "unique_subjects": list(subjects),
                "grade_levels": sorted(list(grades)),
                "collection_name": self.collection_name
            }
            
        except Exception as e:
            logger.error(f"Failed to get collection stats: {str(e)}")
            return {
                "total_chunks": 0,
                "unique_subjects": [],
                "grade_levels": [],
                "collection_name": self.collection_name
            }
    
    def reset_collection(self) -> bool:
        """Reset the vector collection (for testing/development)."""
        try:
            self.client.delete_collection(self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": "Document chunks for RAG"}
            )
            logger.info("Vector collection reset successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to reset collection: {str(e)}")
            return False


================================================================================
File: app/services/vertex_rag_service.py
Size: 13.37 kB
================================================================================

# FILE: app/services/vertex_rag_service.py

import logging
import uuid
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime

from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel
import vertexai

from app.core.config import settings
from app.core.firebase import db
from app.models.rag_models import DocumentChunk, RAGQuery, RAGResult, VectorSearchMetrics

logger = logging.getLogger(__name__)

class VertexAIRAGService:
    """RAG service using Vertex AI Vector Search and Embeddings."""
    
    def __init__(self):
        self.project_id = settings.google_cloud_project
        self.location = settings.google_cloud_location
        self.collection_name = "document_embeddings"
        self.embedding_model = None
        self._initialize_vertex_ai()
        
    def _initialize_vertex_ai(self):
        """Initialize Vertex AI."""
        try:
            vertexai.init(
                project=self.project_id,
                location=self.location
            )
            
            # Initialize embedding model
            self.embedding_model = TextEmbeddingModel.from_pretrained("text-embedding-005")
            
            logger.info("Vertex AI RAG service initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Vertex AI RAG: {str(e)}")
            # Don't raise, allow graceful fallback
            self.embedding_model = None
    
    async def add_chunks(self, chunks: List[DocumentChunk]) -> bool:
        """Add document chunks to Firestore with embeddings."""
        try:
            if not chunks:
                return True
                
            if not self.embedding_model:
                logger.warning("Embedding model not available, saving chunks without embeddings")
                return await self._save_chunks_without_embeddings(chunks)
            
            # Generate embeddings for all chunks
            texts = [chunk.content for chunk in chunks]
            embeddings = await self._generate_embeddings(texts)
            
            # Store chunks with embeddings in Firestore
            batch = db.batch()
            
            for chunk, embedding in zip(chunks, embeddings):
                chunk.embedding_vector = embedding
                
                doc_ref = db.collection(self.collection_name).document(chunk.chunk_id)
                chunk_data = chunk.dict()
                batch.set(doc_ref, chunk_data)
            
            batch.commit()
            
            logger.info(f"Added {len(chunks)} chunks with embeddings to Firestore")
            return True
            
        except Exception as e:
            logger.error(f"Failed to add chunks: {str(e)}")
            return False
    
    async def _save_chunks_without_embeddings(self, chunks: List[DocumentChunk]) -> bool:
        """Fallback to save chunks without embeddings."""
        try:
            batch = db.batch()
            
            for chunk in chunks:
                doc_ref = db.collection(self.collection_name).document(chunk.chunk_id)
                chunk_data = chunk.dict()
                batch.set(doc_ref, chunk_data)
            
            batch.commit()
            logger.info(f"Saved {len(chunks)} chunks without embeddings")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save chunks without embeddings: {str(e)}")
            return False
    
    async def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using Vertex AI."""
        try:
            if not self.embedding_model:
                return [[0.0] * 768 for _ in texts]  # Return dummy embeddings
            
            # Batch embeddings for efficiency
            embeddings = []
            batch_size = 5  # Process 5 texts at a time
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.embedding_model.get_embeddings(batch_texts)
                
                for embedding in batch_embeddings:
                    embeddings.append(embedding.values)
                
                # Small delay to avoid rate limits
                if i + batch_size < len(texts):
                    await asyncio.sleep(0.1)
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {str(e)}")
            return [[0.0] * 768 for _ in texts]  # Return dummy embeddings as fallback
    
    async def search_similar_chunks(self, query: RAGQuery) -> List[RAGResult]:
        """Search for similar chunks using vector similarity."""
        try:
            start_time = datetime.utcnow()
            
            if not self.embedding_model:
                logger.warning("Embedding model not available, using text-based search")
                return await self._text_based_search(query)
            
            # Generate embedding for query
            query_embeddings = await self._generate_embeddings([query.query_text])
            query_embedding = query_embeddings[0]
            
            # Build Firestore query with filters
            collection_ref = db.collection(self.collection_name)
            firestore_query = collection_ref
            
            # Apply filters
            if query.subject:
                firestore_query = firestore_query.where("metadata.subject", "==", query.subject)
            if query.grade_level:
                firestore_query = firestore_query.where("metadata.grade_level", "==", query.grade_level)
            
            # Get all matching documents
            docs = firestore_query.stream()
            
            # Calculate similarity scores
            candidates = []
            for doc in docs:
                doc_data = doc.to_dict()
                
                if not doc_data.get("embedding_vector"):
                    continue
                
                # Calculate cosine similarity
                similarity = self._calculate_cosine_similarity(
                    query_embedding, 
                    doc_data["embedding_vector"]
                )
                
                if similarity >= query.similarity_threshold:
                    chunk = DocumentChunk(**doc_data)
                    result = RAGResult(
                        chunk=chunk,
                        similarity_score=similarity,
                        document_metadata={
                            "filename": chunk.metadata.get("filename", ""),
                            "document_id": chunk.document_id
                        }
                    )
                    candidates.append(result)
            
            # Sort by similarity and return top results
            candidates.sort(key=lambda x: x.similarity_score, reverse=True)
            results = candidates[:query.max_results]
            
            # Calculate metrics
            end_time = datetime.utcnow()
            query_time = (end_time - start_time).total_seconds() * 1000
            
            logger.info(f"Vector search completed: {len(results)} results, {query_time:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {str(e)}")
            return []
    
    async def _text_based_search(self, query: RAGQuery) -> List[RAGResult]:
        """Fallback text-based search when embeddings aren't available."""
        try:
            collection_ref = db.collection(self.collection_name)
            firestore_query = collection_ref
            
            # Apply filters
            if query.subject:
                firestore_query = firestore_query.where("metadata.subject", "==", query.subject)
            if query.grade_level:
                firestore_query = firestore_query.where("metadata.grade_level", "==", query.grade_level)
            
            docs = firestore_query.stream()
            
            # Simple text matching
            candidates = []
            query_words = set(query.query_text.lower().split())
            
            for doc in docs:
                doc_data = doc.to_dict()
                content = doc_data.get("content", "").lower()
                content_words = set(content.split())
                
                # Calculate word overlap as similarity
                overlap = len(query_words.intersection(content_words))
                similarity = overlap / max(len(query_words), 1)
                
                if similarity >= query.similarity_threshold:
                    chunk = DocumentChunk(**doc_data)
                    result = RAGResult(
                        chunk=chunk,
                        similarity_score=similarity,
                        document_metadata={
                            "filename": chunk.metadata.get("filename", ""),
                            "document_id": chunk.document_id
                        }
                    )
                    candidates.append(result)
            
            # Sort and return top results
            candidates.sort(key=lambda x: x.similarity_score, reverse=True)
            return candidates[:query.max_results]
            
        except Exception as e:
            logger.error(f"Text-based search failed: {str(e)}")
            return []
    
    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors."""
        try:
            import numpy as np
            
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            
            dot_product = np.dot(vec1, vec2)
            magnitude1 = np.linalg.norm(vec1)
            magnitude2 = np.linalg.norm(vec2)
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            return float(dot_product / (magnitude1 * magnitude2))
            
        except Exception as e:
            logger.error(f"Failed to calculate similarity: {str(e)}")
            return 0.0
    
    async def get_chunk_by_id(self, chunk_id: str) -> Optional[DocumentChunk]:
        """Get a specific chunk by ID."""
        try:
            doc_ref = db.collection(self.collection_name).document(chunk_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return DocumentChunk(**doc.to_dict())
            return None
            
        except Exception as e:
            logger.error(f"Failed to get chunk {chunk_id}: {str(e)}")
            return None
    
    async def delete_document_chunks(self, document_id: str) -> bool:
        """Delete all chunks for a document."""
        try:
            query = db.collection(self.collection_name).where("document_id", "==", document_id)
            docs = query.stream()
            
            batch = db.batch()
            chunk_count = 0
            
            for doc in docs:
                batch.delete(doc.reference)
                chunk_count += 1
            
            if chunk_count > 0:
                batch.commit()
                logger.info(f"Deleted {chunk_count} chunks for document {document_id}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete chunks for document {document_id}: {str(e)}")
            return False
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """Get statistics about the vector collection."""
        try:
            # Count total documents
            collection_ref = db.collection(self.collection_name)
            docs = collection_ref.stream()
            
            total_count = 0
            subjects = set()
            grades = set()
            with_embeddings = 0
            
            for doc in docs:
                total_count += 1
                doc_data = doc.to_dict()
                metadata = doc_data.get("metadata", {})
                
                if doc_data.get("embedding_vector"):
                    with_embeddings += 1
                
                if metadata.get("subject"):
                    subjects.add(metadata["subject"])
                if metadata.get("grade_level"):
                    grades.add(metadata["grade_level"])
            
            return {
                "total_chunks": total_count,
                "chunks_with_embeddings": with_embeddings,
                "unique_subjects": list(subjects),
                "grade_levels": sorted(list(grades)),
                "collection_name": self.collection_name,
                "embedding_model": "text-embedding-005" if self.embedding_model else "none"
            }
            
        except Exception as e:
            logger.error(f"Failed to get collection stats: {str(e)}")
            return {
                "total_chunks": 0,
                "chunks_with_embeddings": 0,
                "unique_subjects": [],
                "grade_levels": [],
                "collection_name": self.collection_name,
                "error": str(e)
            }

# Global instance for easy import
vertex_rag_service = VertexAIRAGService()


================================================================================
File: requirements.txt
Size: 757 B
================================================================================

# FILE: requirements.txt

# FastAPI Framework
fastapi
uvicorn[standard]

# Pydantic for data models and settings management
pydantic
pydantic-settings

# Google Cloud and Firebase
firebase-admin
google-cloud-aiplatform
google-cloud-speech
google-cloud-texttospeech
google-cloud-storage
google-cloud-documentai
google-cloud-discoveryengine

# Vertex AI RAG and Embeddings
google-cloud-aiplatform
vertexai

# For handling multipart/form-data (file uploads)
python-multipart

# Background task processing
celery[redis]
redis

# File processing
PyPDF2
pillow
python-magic
pytesseract
python-docx

# Data processing
numpy
pandas

# AI and Embeddings
openai
tiktoken
sentence-transformers

# Data processing
numpy
pandas

# Google Agent Development Kit
google-adk

================================================================================
File: svc-acc-key.json
Size: 2.39 kB
================================================================================

{
  "type": "service_account",
  "project_id": "skilful-berm-466510-h9",
  "private_key_id": "d4aafc8444ec3df58fb493fdd2ed4d34cc636c99",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCQuWXjsvLJvmbL\nhjazZoOG9Ul0fb/Db6HvP6DtBXKq95EhM+zyU2KDH4GnKH4AqupA/Ok4DzMeK2ci\nIs0so+vekKrc0Hk/UTtGsteL4Ec3FlDoRqfbbu5rCVEUO1PK8AiRGxdwDelRDKIY\nZxL4CcaEk1tB2kMQdBxM8zrkS99ql2I/vtb7+JfwrKLxDA5IJCckXVK9YXEvwUPv\nNN2xfMemS43YZS+aPA/Xd3UU4npUMlTIQT56nbdf+ilZlhjCdeO4X6NeTLVg/Mmh\nEYhBmR7nkf2Vy7lIH9vwpPPftLbllBlh3cbbCknzq9vlspXgLlwNZjYCPVDRPO69\nwE42SBaZAgMBAAECggEACwkeHj2rIovaqA6XDXlF0oTR+fhsRt10cEtObMB3eNa5\nqfndCbHmz0eX4fyWFhwyUmwOpxwOmnB2E+3qLgseXWlz4/RgKH472kVhJwz+NTLG\nDt4zZsnH06DhXjzzY+RHjPQPRpfC+K+RTTHP6hrJuNqJMQVxC3hFH6+E6DtqYQTm\nh9zSoUJvkdrwz22rU/b0ddYOQHZPjoZbfEhf0g+mZg/CSPP5d10Yc825mb+6qh+H\n8b6Ed+SS9EuTvjLWa0bo4kX2BA/x5f61yjDRjSgzhsfvxtGnDrmuq4lrgNkbmsfR\nvNduAQzbSuPIq8jq/lU3TcboRpgGlR57UKlwypUQoQKBgQDE2Y90IHaYlM56DGqg\nikA/bbc9J39nvg2BSwSZKwKGQGr8CcQP5xn+mqvpJrgU4LEKj39LS3Zbk0JJ35lM\nnB0g7zfaISuvQzNU8cDSGyRqbC/fNi1gZd+54H6oXGlJQco4O1UvFkht4yUYYBVC\nVKj+End5nldmduqp/8bnH8/sIQKBgQC8NiKoGwg9awsSIsa4tjyOnxPBsA6DgCpW\nVYlt4AezjXr4CEjnBoVZG4ot+aLn1MmmnvUM+aHc3BdTbwVTvdO3ZcoiifO/a3+9\nD4oZTrn64AHxj//juKlE0NJ3bRuSbSpYX3UQaLDCBbNpSeRqPrBo78ySPkTx8jmb\nLHhoy9obeQKBgAIlBKUVPXWIGX/G5tp85xpJzbdC2OZ+cZM5CAHLMCnaRdAdIx+x\nOt1FPh46J8NCd2tQCOj/F3kD77LyVq84/DBBqvAbcGhmaQbB44Hmfc0TBKp/4pqs\nQXW69mZLK3J63wMh0lnC+mMIGH1dfp3MYlLyTI7spLMfFItpl/cop5FBAoGAZZC5\nJwpfafVn5gOKRYdiKnzBxpXhXyk8r5Y9MWPiBAznPQ4LeVBViX6JFMJQHNcBcr74\n8mWL6YmDtOyGbWhU5fUyHW1gcf3q0vWdyoZnanCT2+2fYx31ikM+MGBarqvmXAY3\nQAGe0TUOWlUbxBHMVPSC6rBbuEzEtW5MFBFdw9ECgYEAmSpZ97Nuwk7b9UmTOT67\numwQkNQ45S8ezDfkgnQsTI8Js92rTOCQE95kBuQZmYO2pFCEOixtb0YJC7Tf6QQ0\nMUO7kEy5HHu4cOWMteGF02raVFk0MMtXV3ZkvdQlCQIWtEwIZMlMxCDUYFnzna5O\nse6HhHP6rjF+Ln2RXuiZwCU=\n-----END PRIVATE KEY-----\n",
  "client_email": "edvance-48771@skilful-berm-466510-h9.iam.gserviceaccount.com",
  "client_id": "112373176482423548292",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/edvance-48771%40skilful-berm-466510-h9.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}

